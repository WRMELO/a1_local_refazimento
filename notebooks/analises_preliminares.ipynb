{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Objetivo: carregar somente os cabeçalhos (nomes das colunas) dos quatro arquivos CSV informados e mostrar em tela, de forma transposta, um quadro onde cada arquivo aparece como uma coluna e os nomes de campo ficam verticalmente.\n",
    "Nada além disso será feito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e483ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz transposta de cabeçalhos (cada coluna é um arquivo; linhas = nomes de campos na ordem original):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "A1_SECONDARIES_FOR_PEDRA_MODEL.csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "A1_SECONDARIES_REFS.csv",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "DELTA_PROXY_DIAGNOSTICS.csv",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e4d4af4b-51e0-4637-b3a1-0488af3c2808",
       "rows": [
        [
         "0",
         "Timestamp",
         "timestamp",
         "delta_proxy_ref",
         "col"
        ],
        [
         "1",
         "coal_flow_furnace_t_h",
         "coal_flow_furnace_t_h",
         "tau_densa_ref",
         "group"
        ],
        [
         "2",
         "total_air_flow_knm3_h",
         "vazao_ar_total_knm3_h",
         "tau_diluida_ref",
         "nan_rate"
        ],
        [
         "3",
         "total_paf_air_flow_knm3_h",
         "vazao_ar_prim_total_knm3_h",
         "tau_backpass_ref",
         "spearman_tau"
        ],
        [
         "4",
         "te_of_hot_pri_air_in_aph_outl_adegc",
         "temp_hot_pri_air_in_preaq_ar_saida",
         "v_proxy_total_ref",
         "spearman_vtotal"
        ],
        [
         "5",
         "tau_densa",
         "tau_densa",
         "v_proxy_primary_ref",
         "spearman_vprimary"
        ],
        [
         "6",
         "tau_diluida",
         "tau_diluida",
         null,
         "trend_tau"
        ],
        [
         "7",
         "tau_backpass",
         "tau_backpass",
         null,
         "trend_vtotal"
        ],
        [
         "8",
         "o2_excess_pct",
         "o2_excess_pct",
         null,
         "trend_vprimary"
        ],
        [
         "9",
         "delta_proxy",
         "delta_proxy",
         null,
         "score"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv</th>\n",
       "      <th>A1_SECONDARIES_FOR_PEDRA_MODEL.csv</th>\n",
       "      <th>A1_SECONDARIES_REFS.csv</th>\n",
       "      <th>DELTA_PROXY_DIAGNOSTICS.csv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Timestamp</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>delta_proxy_ref</td>\n",
       "      <td>col</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coal_flow_furnace_t_h</td>\n",
       "      <td>coal_flow_furnace_t_h</td>\n",
       "      <td>tau_densa_ref</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_air_flow_knm3_h</td>\n",
       "      <td>vazao_ar_total_knm3_h</td>\n",
       "      <td>tau_diluida_ref</td>\n",
       "      <td>nan_rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>total_paf_air_flow_knm3_h</td>\n",
       "      <td>vazao_ar_prim_total_knm3_h</td>\n",
       "      <td>tau_backpass_ref</td>\n",
       "      <td>spearman_tau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>te_of_hot_pri_air_in_aph_outl_adegc</td>\n",
       "      <td>temp_hot_pri_air_in_preaq_ar_saida</td>\n",
       "      <td>v_proxy_total_ref</td>\n",
       "      <td>spearman_vtotal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tau_densa</td>\n",
       "      <td>tau_densa</td>\n",
       "      <td>v_proxy_primary_ref</td>\n",
       "      <td>spearman_vprimary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tau_diluida</td>\n",
       "      <td>tau_diluida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trend_tau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tau_backpass</td>\n",
       "      <td>tau_backpass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trend_vtotal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>o2_excess_pct</td>\n",
       "      <td>o2_excess_pct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trend_vprimary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>delta_proxy</td>\n",
       "      <td>delta_proxy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>score</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv  A1_SECONDARIES_FOR_PEDRA_MODEL.csv  \\\n",
       "0                              Timestamp                           timestamp   \n",
       "1                  coal_flow_furnace_t_h               coal_flow_furnace_t_h   \n",
       "2                  total_air_flow_knm3_h               vazao_ar_total_knm3_h   \n",
       "3              total_paf_air_flow_knm3_h          vazao_ar_prim_total_knm3_h   \n",
       "4    te_of_hot_pri_air_in_aph_outl_adegc  temp_hot_pri_air_in_preaq_ar_saida   \n",
       "5                              tau_densa                           tau_densa   \n",
       "6                            tau_diluida                         tau_diluida   \n",
       "7                           tau_backpass                        tau_backpass   \n",
       "8                          o2_excess_pct                       o2_excess_pct   \n",
       "9                            delta_proxy                         delta_proxy   \n",
       "\n",
       "  A1_SECONDARIES_REFS.csv DELTA_PROXY_DIAGNOSTICS.csv  \n",
       "0         delta_proxy_ref                         col  \n",
       "1           tau_densa_ref                       group  \n",
       "2         tau_diluida_ref                    nan_rate  \n",
       "3        tau_backpass_ref                spearman_tau  \n",
       "4       v_proxy_total_ref             spearman_vtotal  \n",
       "5     v_proxy_primary_ref           spearman_vprimary  \n",
       "6                     NaN                   trend_tau  \n",
       "7                     NaN                trend_vtotal  \n",
       "8                     NaN              trend_vprimary  \n",
       "9                     NaN                       score  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===============================================\n",
    "# ETAPA: LEITURA DE CABEÇALHOS E EXIBIÇÃO TRANSPOSTA\n",
    "# ===============================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Arquivos informados (use exatamente estes caminhos)\n",
    "csv_paths = [\n",
    "    r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\outputs\\pedra\\A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv\",\n",
    "    r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\outputs\\pedra\\A1_SECONDARIES_FOR_PEDRA_MODEL.csv\",\n",
    "    r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\outputs\\pedra\\A1_SECONDARIES_REFS.csv\",\n",
    "    r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\outputs\\pedra\\DELTA_PROXY_DIAGNOSTICS.csv\",\n",
    "]\n",
    "\n",
    "def read_headers_only(path: str):\n",
    "    \"\"\"\n",
    "    Lê apenas os cabeçalhos do CSV (sem carregar dados).\n",
    "    Tenta primeiro utf-8-sig, depois latin1.\n",
    "    Retorna lista de nomes de colunas.\n",
    "    \"\"\"\n",
    "    # Verificação de existência\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado: {path}\")\n",
    "\n",
    "    encodings = [\"utf-8-sig\", \"latin1\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            # nrows=0 garante leitura apenas do header\n",
    "            df = pd.read_csv(path, nrows=0, encoding=enc)\n",
    "            return list(df.columns)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    # Se todas as tentativas falharem, reapresenta o último erro\n",
    "    raise last_err\n",
    "\n",
    "# Monta um DataFrame onde cada coluna representa um arquivo\n",
    "series_by_file = {}\n",
    "for p in csv_paths:\n",
    "    headers = read_headers_only(p)\n",
    "    name = os.path.basename(p)\n",
    "    # Series indexadas por posição, para fácil visualização vertical\n",
    "    series_by_file[name] = pd.Series(headers, dtype=\"object\")\n",
    "\n",
    "# Concatena lado a lado, alinhando pelo índice (posição dos cabeçalhos)\n",
    "headers_matrix = pd.concat(series_by_file, axis=1)\n",
    "\n",
    "# Exibição: nomes de arquivos como cabeçalhos de coluna\n",
    "pd.set_option(\"display.max_rows\", None)      # mostra todos os cabeçalhos\n",
    "pd.set_option(\"display.max_columns\", None)   # mostra todas as colunas (arquivos)\n",
    "pd.set_option(\"display.width\", 0)\n",
    "\n",
    "print(\"Matriz transposta de cabeçalhos (cada coluna é um arquivo; linhas = nomes de campos na ordem original):\")\n",
    "display(headers_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2745db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\analises_preliminares\\matriz_cabecalhos.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# ETAPA: SALVAMENTO DO DATAFRAME DE CABEÇALHOS\n",
    "# ===============================================\n",
    "\n",
    "import os\n",
    "\n",
    "# Caminho de saída\n",
    "output_dir = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\analises_preliminares\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Arquivo final\n",
    "output_path = os.path.join(output_dir, \"matriz_cabecalhos.csv\")\n",
    "\n",
    "# Salvar DataFrame\n",
    "headers_matrix.to_csv(output_path, index=True, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Arquivo salvo em: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77759fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos com colunas-alvo encontrados: 0\n",
      "Inventário salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\inventario_wr_wm.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# ETAPA: VARREDURA DE CSVs (APENAS CABEÇALHOS)\n",
    "# ===============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Diretório base do projeto\n",
    "base_dir = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "# Pastas a inspecionar (relativas a base_dir)\n",
    "folders_to_scan = [\"data\", \"outputs\"]\n",
    "\n",
    "# Colunas-alvo\n",
    "target_cols = [\"Wr\", \"Wm\", \"Wr_ref\", \"Wm_ref\", \"Wr_idx\", \"Wm_idx\"]\n",
    "\n",
    "# Onde salvar o inventário\n",
    "output_dir = os.path.join(base_dir, \"outputs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"inventario_wr_wm.csv\")\n",
    "\n",
    "# Coletar resultados\n",
    "results = []\n",
    "\n",
    "for folder in folders_to_scan:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if not os.path.exists(folder_path):\n",
    "        continue\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for f in files:\n",
    "            if not f.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            full_path = os.path.join(root, f)\n",
    "\n",
    "            # Tenta ler só o header com duas codificações comuns\n",
    "            df = None\n",
    "            for enc in (\"utf-8-sig\", \"latin1\"):\n",
    "                try:\n",
    "                    df = pd.read_csv(full_path, nrows=0, encoding=enc)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    df = None\n",
    "\n",
    "            if df is None:\n",
    "                # Não conseguiu ler o cabeçalho — ignora\n",
    "                continue\n",
    "\n",
    "            found = [c for c in target_cols if c in df.columns]\n",
    "            if found:\n",
    "                results.append({\n",
    "                    \"arquivo\": full_path,\n",
    "                    \"colunas_encontradas\": \", \".join(found)\n",
    "                })\n",
    "\n",
    "# Monta DataFrame do inventário\n",
    "df_inventory = pd.DataFrame(results, columns=[\"arquivo\", \"colunas_encontradas\"])\n",
    "\n",
    "# Salva inventário\n",
    "df_inventory.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Exibe um resumo em tela (não lê dados, só informa)\n",
    "print(f\"Arquivos com colunas-alvo encontrados: {len(df_inventory)}\")\n",
    "print(f\"Inventário salvo em: {output_path}\")\n",
    "if not df_inventory.empty:\n",
    "    print(\"\\nPrévia (até 10 linhas):\")\n",
    "    print(df_inventory.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e922e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Tabela consolidada (sem cálculos) salva em:\n",
      "C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_padrao.csv\n",
      "Linhas: 35,271 | Colunas: 18\n",
      "Colunas com algum preenchimento detectado:\n",
      "['timestamp', 'zona', 'componente', 'total_air_flow_knm3_h', 'total_paf_air_flow_knm3_h', 'te_of_hot_pri_air_in_aph_outl_adegc', 'delta_proxy', 'o2_excess_pct']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONSOLIDAÇÃO MÍNIMA – AJUSTE DE CAMINHOS (MVP, SEM CÁLCULOS)\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Bases de caminho\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"                    # leitura (já existente)\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"         # escrita (criar se não existir)\n",
    "\n",
    "# Garante que o diretório de destino exista\n",
    "os.makedirs(DST_BASE, exist_ok=True)\n",
    "\n",
    "# Fontes preferenciais (em A1_LOCAL)\n",
    "SECUNDARIOS_CANDIDATOS = [\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_FOR_PEDRA_MODEL.csv\"),\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv\"),\n",
    "]\n",
    "REFS_CANDIDATOS = [\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_REFS.csv\"),\n",
    "]\n",
    "\n",
    "# Saída padrão (em A1_LOCAL_REFAZIMENTO)\n",
    "OUT_DIR  = os.path.join(DST_BASE, \"data\", \"derivadas\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PATH = os.path.join(OUT_DIR, \"tabela_wr_wm_padrao.csv\")\n",
    "\n",
    "# Colunas alvo do padrão (sem cálculos; apenas preencher se existirem nas fontes)\n",
    "PADRAO_COLS = [\n",
    "    \"timestamp\",\"zona\",\"componente\",\n",
    "    \"flw_total_c_t_h\",\n",
    "    \"total_air_flow_knm3_h\",\"total_paf_air_flow_knm3_h\",\n",
    "    \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "    \"delta_proxy\",\"tau_densa\",\"tau_diluida\",\"tau_backpass\",\"o2_excess_pct\",\n",
    "    \"Wr\",\"Wm\",\"Wr_ref\",\"Wm_ref\",\"Wr_idx\",\"Wm_idx\",\n",
    "]\n",
    "\n",
    "# Mapeamentos de nomes observados (não inferimos nada novo)\n",
    "NAME_MAP = {\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"Timestamp\": \"timestamp\",\n",
    "    \"vazao_ar_total_knm3_h\": \"total_air_flow_knm3_h\",\n",
    "    \"vazao_ar_prim_total_knm3_h\": \"total_paf_air_flow_knm3_h\",\n",
    "    \"temp_hot_pri_air_in_preaq_ar_saida\": \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "    \"delta_proxy\": \"delta_proxy\",\n",
    "    \"tau_densa\": \"tau_densa\",\n",
    "    \"tau_diluida\": \"tau_diluida\",\n",
    "    \"tau_backpass\": \"tau_backpass\",\n",
    "    \"o2_excess_pct\": \"o2_excess_pct\",\n",
    "}\n",
    "\n",
    "def read_csv_best_effort(path):\n",
    "    for enc in (\"utf-8-sig\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise FileNotFoundError(f\"Falha ao ler: {path}\")\n",
    "\n",
    "def pick_existing(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# 1) Seleciona fontes existentes em A1_LOCAL\n",
    "src_sec = pick_existing(SECUNDARIOS_CANDIDATOS)\n",
    "src_ref = pick_existing(REFS_CANDIDATOS)\n",
    "\n",
    "if src_sec is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Não encontrei nenhum dos arquivos de secundárias esperados em A1_LOCAL\\\\outputs\\\\pedra\\\\ \"\n",
    "        \"(A1_SECONDARIES_FOR_PEDRA_MODEL*.csv).\"\n",
    "    )\n",
    "\n",
    "# 2) Carrega secundárias e aplica renomeações conhecidas\n",
    "df_sec = read_csv_best_effort(src_sec)\n",
    "rename_map = {c: NAME_MAP[c] for c in df_sec.columns if c in NAME_MAP}\n",
    "df_sec = df_sec.rename(columns=rename_map)\n",
    "\n",
    "if \"timestamp\" not in df_sec.columns:\n",
    "    raise RuntimeError(\"A coluna 'timestamp' não foi encontrada após renomeação.\")\n",
    "\n",
    "# 3) Carrega referências (se existir), sem cálculos\n",
    "df_ref = None\n",
    "if src_ref is not None:\n",
    "    try:\n",
    "        df_ref = read_csv_best_effort(src_ref)\n",
    "        ref_rename = {c: NAME_MAP[c] for c in df_ref.columns if c in NAME_MAP}\n",
    "        if ref_rename:\n",
    "            df_ref = df_ref.rename(columns=ref_rename)\n",
    "    except Exception:\n",
    "        df_ref = None\n",
    "\n",
    "# 4) Expandir por 'zona' a partir de tau_* (apenas reshape; sem cálculos)\n",
    "zonas_cols = [c for c in [\"tau_densa\",\"tau_diluida\",\"tau_backpass\"] if c in df_sec.columns]\n",
    "if zonas_cols:\n",
    "    id_cols = [c for c in df_sec.columns if c not in zonas_cols]\n",
    "    df_long = df_sec.melt(\n",
    "        id_vars=id_cols,\n",
    "        value_vars=zonas_cols,\n",
    "        var_name=\"zona\",\n",
    "        value_name=\"tau_val\"\n",
    "    )\n",
    "else:\n",
    "    df_long = df_sec.copy()\n",
    "    df_long[\"zona\"] = \"[INFORMAÇÃO AUSENTE – PRECISAR PREENCHER]\"\n",
    "\n",
    "# 5) Componente ausente nas fontes → marcar como ausente (sem inferir)\n",
    "df_long[\"componente\"] = \"[INFORMAÇÃO AUSENTE – PRECISAR PREENCHER]\"\n",
    "\n",
    "# 6) Merge leve com referências por timestamp (somente novas colunas)\n",
    "if df_ref is not None and \"timestamp\" in df_ref.columns:\n",
    "    cols_to_merge = [c for c in df_ref.columns if c != \"timestamp\" and c not in df_long.columns]\n",
    "    if cols_to_merge:\n",
    "        df_long = df_long.merge(df_ref[[\"timestamp\"] + cols_to_merge], on=\"timestamp\", how=\"left\")\n",
    "\n",
    "# 7) Garantir colunas do padrão (criar vazias quando ausentes)\n",
    "for col in PADRAO_COLS:\n",
    "    if col not in df_long.columns:\n",
    "        df_long[col] = pd.NA\n",
    "\n",
    "# 8) Reordenar e salvar em A1_LOCAL_REFAZIMENTO\n",
    "df_final = df_long[PADRAO_COLS].copy()\n",
    "df_final.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"[OK] Tabela consolidada (sem cálculos) salva em:\\n{OUT_PATH}\")\n",
    "print(f\"Linhas: {len(df_final):,} | Colunas: {len(df_final.columns)}\")\n",
    "print(\"Colunas com algum preenchimento detectado:\")\n",
    "print([c for c in PADRAO_COLS if df_final[c].notna().any()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be77c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Tabela corrigida salva em:\n",
      "C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_padrao.csv\n",
      "Linhas: 35,271 | Colunas: 18\n",
      "Colunas com algum preenchimento detectado:\n",
      "['timestamp', 'zona', 'componente', 'flw_total_c_t_h', 'total_air_flow_knm3_h', 'total_paf_air_flow_knm3_h', 'te_of_hot_pri_air_in_aph_outl_adegc', 'delta_proxy', 'tau_densa', 'tau_diluida', 'tau_backpass', 'o2_excess_pct']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONSOLIDAÇÃO (CORREÇÃO): PRESERVAR tau_* E REPLICAR POR ZONA\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Bases de caminho\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"                    # leitura (já existente)\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"         # escrita (criar se não existir)\n",
    "os.makedirs(DST_BASE, exist_ok=True)\n",
    "\n",
    "# Fontes em A1_LOCAL\n",
    "SECUNDARIOS_CANDIDATOS = [\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_FOR_PEDRA_MODEL.csv\"),\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv\"),\n",
    "]\n",
    "REFS_CANDIDATOS = [\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_REFS.csv\"),\n",
    "]\n",
    "\n",
    "# Saída em A1_LOCAL_REFAZIMENTO\n",
    "OUT_DIR  = os.path.join(DST_BASE, \"data\", \"derivadas\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PATH = os.path.join(OUT_DIR, \"tabela_wr_wm_padrao.csv\")\n",
    "\n",
    "# Padrão de colunas (sem cálculos)\n",
    "PADRAO_COLS = [\n",
    "    \"timestamp\",\"zona\",\"componente\",\n",
    "    \"flw_total_c_t_h\",\n",
    "    \"total_air_flow_knm3_h\",\"total_paf_air_flow_knm3_h\",\n",
    "    \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "    \"delta_proxy\",\"tau_densa\",\"tau_diluida\",\"tau_backpass\",\"o2_excess_pct\",\n",
    "    \"Wr\",\"Wm\",\"Wr_ref\",\"Wm_ref\",\"Wr_idx\",\"Wm_idx\",\n",
    "]\n",
    "\n",
    "# Mapeamentos de nomes observados\n",
    "NAME_MAP = {\n",
    "    # timestamp\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"Timestamp\": \"timestamp\",\n",
    "\n",
    "    # vazões de ar\n",
    "    \"vazao_ar_total_knm3_h\": \"total_air_flow_knm3_h\",\n",
    "    \"total_air_flow_knm3_h\": \"total_air_flow_knm3_h\",\n",
    "    \"vazao_ar_prim_total_knm3_h\": \"total_paf_air_flow_knm3_h\",\n",
    "    \"total_paf_air_flow_knm3_h\": \"total_paf_air_flow_knm3_h\",\n",
    "\n",
    "    # temperatura ar primário (APH)\n",
    "    \"temp_hot_pri_air_in_preaq_ar_saida\": \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "    \"te_of_hot_pri_air_in_aph_outl_adegc\": \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "\n",
    "    # proxies / temps / O2\n",
    "    \"delta_proxy\": \"delta_proxy\",\n",
    "    \"tau_densa\": \"tau_densa\",\n",
    "    \"tau_diluida\": \"tau_diluida\",\n",
    "    \"tau_backpass\": \"tau_backpass\",\n",
    "    \"o2_excess_pct\": \"o2_excess_pct\",\n",
    "\n",
    "    # carvão\n",
    "    \"coal_flow_furnace_t_h\": \"flw_total_c_t_h\",\n",
    "    \"flw_total_c_t_h\": \"flw_total_c_t_h\",\n",
    "}\n",
    "\n",
    "def read_csv_best_effort(path):\n",
    "    for enc in (\"utf-8-sig\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise FileNotFoundError(f\"Falha ao ler: {path}\")\n",
    "\n",
    "def pick_existing(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# 1) Selecionar fontes\n",
    "src_sec = pick_existing(SECUNDARIOS_CANDIDATOS)\n",
    "src_ref = pick_existing(REFS_CANDIDATOS)\n",
    "\n",
    "if src_sec is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Não encontrei arquivos de secundárias em A1_LOCAL\\\\outputs\\\\pedra\\\\ \"\n",
    "        \"(A1_SECONDARIES_FOR_PEDRA_MODEL*.csv).\"\n",
    "    )\n",
    "\n",
    "# 2) Carregar e renomear colunas conhecidas\n",
    "df_sec = read_csv_best_effort(src_sec)\n",
    "rename_map = {c: NAME_MAP[c] for c in df_sec.columns if c in NAME_MAP}\n",
    "df_sec = df_sec.rename(columns=rename_map)\n",
    "\n",
    "if \"timestamp\" not in df_sec.columns:\n",
    "    raise RuntimeError(\"A coluna 'timestamp' não foi encontrada após renomeação.\")\n",
    "\n",
    "# 3) Carregar referências (merge leve por timestamp, sem cálculos)\n",
    "df_ref = None\n",
    "if src_ref is not None:\n",
    "    try:\n",
    "        df_ref = read_csv_best_effort(src_ref)\n",
    "        ref_rename = {c: NAME_MAP[c] for c in df_ref.columns if c in NAME_MAP}\n",
    "        if ref_rename:\n",
    "            df_ref = df_ref.rename(columns=ref_rename)\n",
    "    except Exception:\n",
    "        df_ref = None\n",
    "\n",
    "# 4) Replicar por zonas disponíveis, PRESERVANDO tau_* como colunas\n",
    "zonas_presentes = []\n",
    "if \"tau_densa\" in df_sec.columns:\n",
    "    zonas_presentes.append(\"densa\")\n",
    "if \"tau_diluida\" in df_sec.columns:\n",
    "    zonas_presentes.append(\"diluida\")\n",
    "if \"tau_backpass\" in df_sec.columns:\n",
    "    zonas_presentes.append(\"backpass\")\n",
    "\n",
    "if zonas_presentes:\n",
    "    partes = []\n",
    "    for z in zonas_presentes:\n",
    "        parte = df_sec.copy()\n",
    "        parte[\"zona\"] = z\n",
    "        partes.append(parte)\n",
    "    df_z = pd.concat(partes, axis=0, ignore_index=True)\n",
    "else:\n",
    "    df_z = df_sec.copy()\n",
    "    df_z[\"zona\"] = \"[INFORMAÇÃO AUSENTE – PRECISAR PREENCHER]\"\n",
    "\n",
    "# 5) Componente ausente → marcador\n",
    "df_z[\"componente\"] = \"[INFORMAÇÃO AUSENTE – PRECISAR PREENCHER]\"\n",
    "\n",
    "# 6) Merge leve com referências (adiciona colunas que não existirem ainda)\n",
    "if df_ref is not None and \"timestamp\" in df_ref.columns:\n",
    "    cols_to_merge = [c for c in df_ref.columns if c != \"timestamp\" and c not in df_z.columns]\n",
    "    if cols_to_merge:\n",
    "        df_z = df_z.merge(df_ref[[\"timestamp\"] + cols_to_merge], on=\"timestamp\", how=\"left\")\n",
    "\n",
    "# 7) Garantir todas as colunas do padrão (criando vazias quando necessário)\n",
    "for col in PADRAO_COLS:\n",
    "    if col not in df_z.columns:\n",
    "        df_z[col] = pd.NA\n",
    "\n",
    "# 8) Reordenar e salvar\n",
    "df_final = df_z[PADRAO_COLS].copy()\n",
    "df_final.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"[OK] Tabela corrigida salva em:\\n{OUT_PATH}\")\n",
    "print(f\"Linhas: {len(df_final):,} | Colunas: {len(df_final.columns)}\")\n",
    "print(\"Colunas com algum preenchimento detectado:\")\n",
    "print([c for c in PADRAO_COLS if df_final[c].notna().any()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ebb91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Parquet gerado:\n",
      " caminho: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_padrao.parquet\n",
      " shape : (35271, 18)\n",
      " colunas: ['timestamp', 'zona', 'componente', 'flw_total_c_t_h', 'total_air_flow_knm3_h', 'total_paf_air_flow_knm3_h', 'te_of_hot_pri_air_in_aph_outl_adegc', 'delta_proxy', 'tau_densa', 'tau_diluida', 'tau_backpass', 'o2_excess_pct', 'Wr', 'Wm', 'Wr_ref', 'Wm_ref', 'Wr_idx', 'Wm_idx']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EXPORTAR TABELA PADRÃO PARA PARQUET (SEM CÁLCULOS)\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "IN_CSV   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_padrao.csv\")\n",
    "OUT_DIR  = os.path.join(DST_BASE, r\"data\\derivadas\")\n",
    "OUT_PQ   = os.path.join(OUT_DIR, \"tabela_wr_wm_padrao.parquet\")\n",
    "\n",
    "# Garantir diretório de saída\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Ler CSV (conteúdo já consolidado anteriormente)\n",
    "df = pd.read_csv(IN_CSV, low_memory=False)\n",
    "\n",
    "# Salvar em Parquet (engine pyarrow, se disponível)\n",
    "df.to_parquet(OUT_PQ, index=False)\n",
    "\n",
    "# Validação rápida\n",
    "df_check = pd.read_parquet(OUT_PQ)\n",
    "print(\"[OK] Parquet gerado:\")\n",
    "print(\" caminho:\", OUT_PQ)\n",
    "print(\" shape :\", df_check.shape)\n",
    "print(\" colunas:\", list(df_check.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7299ea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] shape: (35271, 18) (linhas=35271, colunas=18)\n",
      "\n",
      "[EXATAS]\n",
      " - Linhas pertencentes a grupos duplicados (inclui primeira ocorrência): 0\n",
      " - Linhas duplicadas além da primeira (contagem de 'extras'):          0\n",
      " - Linhas únicas após remover exatas:                                 35271\n",
      " - Nenhuma duplicata exata encontrada.\n",
      "\n",
      "[POR CHAVE: timestamp, zona, componente]\n",
      " - Quantidade de chaves duplicadas: 0\n",
      " - Total de linhas cobertas por essas chaves: 0\n",
      "\n",
      "[CONTAGEM POR ZONA]\n",
      "zona\n",
      "densa       11757\n",
      "diluida     11757\n",
      "backpass    11757\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DIAGNÓSTICO DE DUPLICIDADES NO PARQUET\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "PQ_PATH  = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_padrao.parquet\")\n",
    "\n",
    "OUT_DIR  = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_DUP_ALL = os.path.join(OUT_DIR, \"duplicatas_exatas.csv\")\n",
    "OUT_DUP_KEY = os.path.join(OUT_DIR, \"duplicatas_por_chave_timestamp_zona_componente.csv\")\n",
    "\n",
    "# 1) Ler parquet\n",
    "df = pd.read_parquet(PQ_PATH)\n",
    "total = len(df)\n",
    "print(f\"[INFO] shape: {df.shape} (linhas={total}, colunas={df.shape[1]})\")\n",
    "\n",
    "# 2) Duplicatas EXATAS (todas as colunas iguais)\n",
    "mask_dups_all = df.duplicated(keep=False)  # marca todos os membros de cada grupo duplicado\n",
    "qtd_linhas_em_grupos_duplicados = int(mask_dups_all.sum())\n",
    "qtd_linhas_duplicadas_alem_da_primeira = int(df.duplicated().sum())\n",
    "qtd_unicas = int(len(df.drop_duplicates()))\n",
    "\n",
    "print(\"\\n[EXATAS]\")\n",
    "print(f\" - Linhas pertencentes a grupos duplicados (inclui primeira ocorrência): {qtd_linhas_em_grupos_duplicados}\")\n",
    "print(f\" - Linhas duplicadas além da primeira (contagem de 'extras'):          {qtd_linhas_duplicadas_alem_da_primeira}\")\n",
    "print(f\" - Linhas únicas após remover exatas:                                 {qtd_unicas}\")\n",
    "\n",
    "# Exporta amostra das duplicatas exatas (se existir)\n",
    "if qtd_linhas_em_grupos_duplicados > 0:\n",
    "    df_dups_all = df[mask_dups_all].copy()\n",
    "    # Para reduzir tamanho do arquivo, limitamos a 50k linhas na exportação (ajuste se quiser)\n",
    "    if len(df_dups_all) > 50000:\n",
    "        df_dups_all = df_dups_all.head(50000)\n",
    "    df_dups_all.to_csv(OUT_DUP_ALL, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\" - Inventário (amostra) salvo em: {OUT_DUP_ALL}\")\n",
    "else:\n",
    "    print(\" - Nenhuma duplicata exata encontrada.\")\n",
    "\n",
    "# 3) Duplicatas por CHAVE ['timestamp','zona','componente']\n",
    "key_cols = [c for c in [\"timestamp\",\"zona\",\"componente\"] if c in df.columns]\n",
    "if len(key_cols) == 3:\n",
    "    grp = df.groupby(key_cols, dropna=False).size().reset_index(name=\"count\")\n",
    "    dup_keys = grp[grp[\"count\"] > 1].sort_values(\"count\", ascending=False)\n",
    "    qtd_chaves_duplicadas = int(len(dup_keys))\n",
    "    total_linhas_em_chaves_duplicadas = int(dup_keys[\"count\"].sum())\n",
    "\n",
    "    print(\"\\n[POR CHAVE: timestamp, zona, componente]\")\n",
    "    print(f\" - Quantidade de chaves duplicadas: {qtd_chaves_duplicadas}\")\n",
    "    print(f\" - Total de linhas cobertas por essas chaves: {total_linhas_em_chaves_duplicadas}\")\n",
    "\n",
    "    if qtd_chaves_duplicadas > 0:\n",
    "        # Exportar todas as chaves duplicadas\n",
    "        dup_keys.to_csv(OUT_DUP_KEY, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\" - Inventário de chaves duplicadas salvo em: {OUT_DUP_KEY}\")\n",
    "else:\n",
    "    print(\"\\n[POR CHAVE]\")\n",
    "    print(\" - A checagem por chave foi pulada porque faltam colunas em ['timestamp','zona','componente'].\")\n",
    "\n",
    "# 4) Contagem por zona (útil para entender multiplicação esperada)\n",
    "if \"zona\" in df.columns:\n",
    "    print(\"\\n[CONTAGEM POR ZONA]\")\n",
    "    print(df[\"zona\"].value_counts(dropna=False).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a3590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] shape original: (35271, 18)\n",
      "[INFO] Nenhum conflito detectado entre as 3 linhas por timestamp (além de 'zona').\n",
      "\n",
      "[OK] Tabela deduplicada gerada:\n",
      " - Parquet: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_padrao_dedup.parquet\n",
      " - CSV    : C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_padrao_dedup.csv\n",
      "Shape final: (11757, 17)\n",
      "Total de timestamps únicos: 11757\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COLAPSAR ZONAS -> 1 LINHA POR TIMESTAMP (SEM CÁLCULOS)\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "IN_PQ    = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_padrao.parquet\")\n",
    "\n",
    "OUT_DIR  = os.path.join(DST_BASE, r\"data\\derivadas\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PQ   = os.path.join(OUT_DIR, \"tabela_wr_wm_padrao_dedup.parquet\")\n",
    "OUT_CSV  = os.path.join(OUT_DIR, \"tabela_wr_wm_padrao_dedup.csv\")\n",
    "\n",
    "DIAG_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(DIAG_DIR, exist_ok=True)\n",
    "OUT_CONFLITOS = os.path.join(DIAG_DIR, \"conflitos_por_timestamp.csv\")\n",
    "\n",
    "# 1) Ler parquet atual\n",
    "df = pd.read_parquet(IN_PQ)\n",
    "print(\"[INFO] shape original:\", df.shape)\n",
    "\n",
    "# 2) Verificar conflitos entre as 3 linhas por timestamp (excluindo 'zona')\n",
    "cols_check = [c for c in df.columns if c != \"zona\"]\n",
    "conf_linhas = []\n",
    "\n",
    "# (Opcional) primeiro, confirmar a multiplicidade por timestamp\n",
    "# (não bloqueia se não for sempre 3)\n",
    "mult_por_ts = df.groupby(\"timestamp\", dropna=False).size()\n",
    "\n",
    "# Varredura de conflitos (se colunas variam dentro do mesmo timestamp)\n",
    "for ts, grp in df.groupby(\"timestamp\", dropna=False):\n",
    "    diffs = [c for c in cols_check if grp[c].nunique(dropna=False) > 1]\n",
    "    if diffs:\n",
    "        conf_linhas.append({\n",
    "            \"timestamp\": ts,\n",
    "            \"n_linhas_grupo\": len(grp),\n",
    "            \"colunas_com_diferencas\": \", \".join(diffs)\n",
    "        })\n",
    "\n",
    "# Exporta conflitos, se existirem\n",
    "if conf_linhas:\n",
    "    pd.DataFrame(conf_linhas).to_csv(OUT_CONFLITOS, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[ATENÇÃO] Conflitos detectados por timestamp. Inventário salvo em:\\n{OUT_CONFLITOS}\")\n",
    "else:\n",
    "    print(\"[INFO] Nenhum conflito detectado entre as 3 linhas por timestamp (além de 'zona').\")\n",
    "\n",
    "# 3) Colapsar: remover 'zona' e deduplicar por 'timestamp' (mantendo a primeira ocorrência)\n",
    "df_nz = df.drop(columns=[\"zona\"], errors=\"ignore\").copy()\n",
    "\n",
    "# Ordena por timestamp para garantir determinismo na escolha da primeira\n",
    "df_nz = df_nz.sort_values(by=[\"timestamp\"]).copy()\n",
    "\n",
    "# Remove duplicatas por timestamp\n",
    "df_dedup = df_nz.drop_duplicates(subset=[\"timestamp\"], keep=\"first\").copy()\n",
    "\n",
    "# 4) Salvar resultados\n",
    "df_dedup.to_parquet(OUT_PQ, index=False)\n",
    "df_dedup.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[OK] Tabela deduplicada gerada:\")\n",
    "print(\" - Parquet:\", OUT_PQ)\n",
    "print(\" - CSV    :\", OUT_CSV)\n",
    "print(\"Shape final:\", df_dedup.shape)\n",
    "\n",
    "# 5) Validações auxiliares\n",
    "if \"timestamp\" in df_dedup.columns:\n",
    "    n_ts_unique = df_dedup[\"timestamp\"].nunique(dropna=False)\n",
    "    print(\"Total de timestamps únicos:\", n_ts_unique)\n",
    "else:\n",
    "    print(\"[ALERTA] Coluna 'timestamp' ausente após deduplicação (não esperado).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cfb9cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Derivados salvos:\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\anomalias_isoforest.parquet\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\anomalias_isoforest.csv\n",
      "[OK] Modelo salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\modelos\\isoforest_baseline.pkl\n",
      "\n",
      "Resumo:\n",
      "Registros: 11,757\n",
      "Features usadas: ['flw_total_c_t_h', 'total_air_flow_knm3_h', 'total_paf_air_flow_knm3_h', 'te_of_hot_pri_air_in_aph_outl_adegc', 'delta_proxy', 'tau_densa', 'tau_diluida', 'tau_backpass', 'o2_excess_pct']\n",
      "% anomalias (contamination alvo=3%): 3.00%\n",
      "\n",
      "Top 5 timestamps mais anômalos (maior score):\n",
      "          timestamp  anomalia_score\n",
      "2023-07-04 04:00:00        1.000000\n",
      "2023-05-31 11:00:00        0.996667\n",
      "2023-11-16 19:00:00        0.990161\n",
      "2023-05-31 09:00:00        0.954710\n",
      "2023-10-11 21:00:00        0.926770\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MVP DE MODELO: ISOLATION FOREST (CORREÇÃO NumPy 2.0)\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "IN_PQ    = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_padrao_dedup.parquet\")\n",
    "DERIV_DIR = os.path.join(DST_BASE, r\"data\\derivadas\")\n",
    "MODEL_DIR = os.path.join(DST_BASE, r\"outputs\\modelos\")\n",
    "DIAG_DIR  = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "\n",
    "os.makedirs(DERIV_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DIAG_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Ler base deduplicada\n",
    "df = pd.read_parquet(IN_PQ)\n",
    "\n",
    "# 2) Seleção de features já preenchidas\n",
    "features = [\n",
    "    \"flw_total_c_t_h\",\n",
    "    \"total_air_flow_knm3_h\",\n",
    "    \"total_paf_air_flow_knm3_h\",\n",
    "    \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "    \"delta_proxy\",\n",
    "    \"tau_densa\",\n",
    "    \"tau_diluida\",\n",
    "    \"tau_backpass\",\n",
    "    \"o2_excess_pct\",\n",
    "]\n",
    "features = [c for c in features if c in df.columns]\n",
    "\n",
    "# 3) Subconjunto de dados (timestamp + features)\n",
    "df_model = df[[\"timestamp\"] + features].copy()\n",
    "for c in features:\n",
    "    df_model[c] = pd.to_numeric(df_model[c], errors=\"coerce\")\n",
    "\n",
    "# 4) Pipeline: Imputer (mediana) + StandardScaler + IsolationForest\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"iso\", IsolationForest(\n",
    "        n_estimators=300,\n",
    "        max_samples=\"auto\",\n",
    "        contamination=0.03,   # ~3% anomalias\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "X = df_model[features].values\n",
    "pipeline.fit(X)\n",
    "\n",
    "# 5) Scores e rótulos (usando o pipeline para aplicar transformações)\n",
    "# decision_function: maior = mais normal. Vamos inverter para anomalia (maior = mais anômalo).\n",
    "raw_scores = pipeline.decision_function(X)  # array shape (n,)\n",
    "anom_base = -raw_scores\n",
    "rng = np.ptp(anom_base)  # max - min (NumPy 2.0+)\n",
    "if not np.isfinite(rng) or rng == 0:\n",
    "    rng = 1e-12\n",
    "anomalia_score = (anom_base - np.min(anom_base)) / rng  # 0..1\n",
    "\n",
    "pred_labels = pipeline.predict(X)  # -1 anomalia, 1 normal\n",
    "is_anomalia = (pred_labels == -1).astype(int)\n",
    "\n",
    "# 6) Montar saída\n",
    "out = df_model.copy()\n",
    "out[\"anomalia_score\"] = anomalia_score\n",
    "out[\"is_anomalia\"] = is_anomalia\n",
    "\n",
    "# 7) Salvar derivados\n",
    "out_pq  = os.path.join(DERIV_DIR, \"anomalias_isoforest.parquet\")\n",
    "out_csv = os.path.join(DERIV_DIR, \"anomalias_isoforest.csv\")\n",
    "out.to_parquet(out_pq, index=False)\n",
    "out.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 8) Salvar modelo\n",
    "model_path = os.path.join(MODEL_DIR, \"isoforest_baseline.pkl\")\n",
    "joblib.dump(pipeline, model_path)\n",
    "\n",
    "# 9) Estatísticas simples\n",
    "pct_anom = 100.0 * out[\"is_anomalia\"].mean()\n",
    "sumario = []\n",
    "sumario.append(f\"Registros: {len(out):,}\")\n",
    "sumario.append(f\"Features usadas: {features}\")\n",
    "sumario.append(f\"% anomalias (contamination alvo=3%): {pct_anom:.2f}%\")\n",
    "top5 = out.sort_values(\"anomalia_score\", ascending=False).head(5)[[\"timestamp\",\"anomalia_score\"]]\n",
    "sumario.append(\"\\nTop 5 timestamps mais anômalos (maior score):\")\n",
    "sumario.append(top5.to_string(index=False))\n",
    "\n",
    "diag_path = os.path.join(DIAG_DIR, \"anomalia_stats.txt\")\n",
    "with open(diag_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sumario))\n",
    "\n",
    "print(\"[OK] Derivados salvos:\")\n",
    "print(\" -\", out_pq)\n",
    "print(\" -\", out_csv)\n",
    "print(\"[OK] Modelo salvo em:\", model_path)\n",
    "print(\"\\nResumo:\")\n",
    "print(\"\\n\".join(sumario))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "491b0807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Tabela com desgaste salva:\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_com_desgaste.parquet\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_com_desgaste.csv\n",
      "\n",
      "Resumo rápido:\n",
      "  Wr_ref: 166056.27285204153  | Wm_ref: 248106045.49944544\n",
      "  % NaN Wr_idx: 36.437866802755806 % | % NaN Wm_idx: 81.20268776048312 %\n",
      "\n",
      "Top 5 maiores Wr_idx:\n",
      "          timestamp    Wr_idx\n",
      "2024-09-29 14:00:00 12.490335\n",
      "2024-03-29 05:00:00 11.579226\n",
      "2024-03-29 16:00:00 11.470146\n",
      "2024-03-29 02:00:00 11.241728\n",
      "2024-03-29 01:00:00 10.761733\n",
      "\n",
      "Top 5 maiores Wm_idx:\n",
      "          timestamp   Wm_idx\n",
      "2024-09-29 14:00:00 3.428456\n",
      "2024-03-29 05:00:00 3.291170\n",
      "2024-03-29 16:00:00 3.263653\n",
      "2024-03-29 02:00:00 3.181821\n",
      "2024-03-29 01:00:00 3.140810\n",
      "\n",
      "[INFO] Parâmetros e fallbacks registrados em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\physics_params_mvp.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_26704\\1751241561.py:124: RuntimeWarning: invalid value encountered in power\n",
      "  Wm = params[\"beta\"]  * np.power(v_curr_v, params[\"n_m\"]) * np.power(delta_v, params[\"m_m\"]) * np.exp(-tauK.values / params[\"Topt\"])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHYSICS-BASED (MVP): Wr/Wm + refs + índices\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "IN_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_padrao_dedup.parquet\")\n",
    "REFS_CSV= os.path.join(SRC_BASE, r\"outputs\\pedra\\A1_SECONDARIES_REFS.csv\")\n",
    "\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"data\\derivadas\")\n",
    "LOG_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "OUT_PQ  = os.path.join(OUT_DIR, \"tabela_wr_wm_com_desgaste.parquet\")\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"tabela_wr_wm_com_desgaste.csv\")\n",
    "OUT_LOG = os.path.join(LOG_DIR, \"physics_params_mvp.json\")\n",
    "\n",
    "# --- 1) Ler base deduplicada ---\n",
    "df = pd.read_parquet(IN_PQ)\n",
    "\n",
    "# Garantir numérico nas colunas usadas\n",
    "def to_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "for c in [\"delta_proxy\",\"tau_densa\",\"tau_diluida\",\"tau_backpass\",\n",
    "          \"total_air_flow_knm3_h\",\"total_paf_air_flow_knm3_h\",\"flw_total_c_t_h\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = to_num(df[c])\n",
    "\n",
    "# --- 2) Ler referências (se houver) ---\n",
    "def read_csv_best_effort(path):\n",
    "    for enc in (\"utf-8-sig\",\"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "refs = read_csv_best_effort(REFS_CSV)\n",
    "\n",
    "# Extrair refs como únicos valores (esperado 1 linha). Se houver várias, pegar a 1ª não nula.\n",
    "def pick_ref(series):\n",
    "    if series is None:\n",
    "        return np.nan\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    s = s.dropna()\n",
    "    return s.iloc[0] if len(s) else np.nan\n",
    "\n",
    "ref_vals = {}\n",
    "if refs is not None:\n",
    "    for col in [\"delta_proxy_ref\",\"tau_densa_ref\",\"tau_diluida_ref\",\"tau_backpass_ref\",\n",
    "                \"v_proxy_total_ref\",\"v_proxy_primary_ref\"]:\n",
    "        if col in refs.columns:\n",
    "            ref_vals[col] = pick_ref(refs[col])\n",
    "        else:\n",
    "            ref_vals[col] = np.nan\n",
    "else:\n",
    "    # sem arquivo de refs\n",
    "    ref_vals = {k: np.nan for k in\n",
    "        [\"delta_proxy_ref\",\"tau_densa_ref\",\"tau_diluida_ref\",\"tau_backpass_ref\",\"v_proxy_total_ref\",\"v_proxy_primary_ref\"]}\n",
    "\n",
    "# --- 3) Definir variáveis de trabalho (MVP) ---\n",
    "# τ (°C) -> τK (K). Usaremos média das três zonas disponíveis.\n",
    "taus = [c for c in [\"tau_densa\",\"tau_diluida\",\"tau_backpass\"] if c in df.columns]\n",
    "df[\"tau_mean_C\"] = np.nan\n",
    "if taus:\n",
    "    df[\"tau_mean_C\"] = pd.concat([df[t] for t in taus], axis=1).mean(axis=1, skipna=True)\n",
    "\n",
    "# v (proxy de velocidade): preferimos ar total; se não tiver, ar primário; fallback: mediana.\n",
    "if \"total_air_flow_knm3_h\" in df.columns:\n",
    "    v_curr = df[\"total_air_flow_knm3_h\"].copy()\n",
    "elif \"total_paf_air_flow_knm3_h\" in df.columns:\n",
    "    v_curr = df[\"total_paf_air_flow_knm3_h\"].copy()\n",
    "else:\n",
    "    v_curr = pd.Series(np.nan, index=df.index)\n",
    "\n",
    "# delta_proxy atual\n",
    "delta_curr = df[\"delta_proxy\"] if \"delta_proxy\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "# Refs: se existirem no CSV, usamos; senão, fallback pela mediana dos próprios dados\n",
    "v_ref = ref_vals.get(\"v_proxy_total_ref\", np.nan)\n",
    "if not np.isfinite(v_ref):\n",
    "    v_ref = np.nanmedian(v_curr.values)\n",
    "\n",
    "delta_ref = ref_vals.get(\"delta_proxy_ref\", np.nan)\n",
    "if not np.isfinite(delta_ref):\n",
    "    delta_ref = np.nanmedian(delta_curr.values)\n",
    "\n",
    "tau_ref_C_vals = [ref_vals.get(\"tau_densa_ref\", np.nan),\n",
    "                  ref_vals.get(\"tau_diluida_ref\", np.nan),\n",
    "                  ref_vals.get(\"tau_backpass_ref\", np.nan)]\n",
    "tau_ref_C = np.nanmean([x for x in tau_ref_C_vals if np.isfinite(x)]) if np.any(np.isfinite(tau_ref_C_vals)) else np.nan\n",
    "if not np.isfinite(tau_ref_C):\n",
    "    tau_ref_C = np.nanmedian(df[\"tau_mean_C\"].values)\n",
    "\n",
    "# --- 4) Constantes do MVP (ajustáveis) ---\n",
    "params = {\n",
    "    \"alpha\": 1.0,        # fatores escala (cancelam nos índices)\n",
    "    \"beta\":  1.0,\n",
    "    \"n_r\":   2.0,        # expoente de v para Wr\n",
    "    \"m_r\":   1.0,        # expoente de delta para Wr\n",
    "    \"n_m\":   3.0,        # expoente de v para Wm\n",
    "    \"m_m\":   0.5,        # expoente de delta para Wm\n",
    "    \"Tcrit\": 1200.0,     # K (refratário)\n",
    "    \"Topt\":  1250.0      # K (metálico)\n",
    "}\n",
    "\n",
    "# --- 5) Cálculo (Wr/Wm e refs) ---\n",
    "# Preparos\n",
    "tauK     = to_num(df[\"tau_mean_C\"]) + 273.15\n",
    "tau_refK = float(tau_ref_C) + 273.15\n",
    "\n",
    "v_curr_v = to_num(v_curr).values\n",
    "delta_v  = to_num(delta_curr).values\n",
    "\n",
    "# Wr e Wm atuais (proxy adimensional de desgaste)\n",
    "Wr = params[\"alpha\"] * np.power(v_curr_v, params[\"n_r\"]) * np.power(delta_v, params[\"m_r\"]) * np.exp(-tauK.values / params[\"Tcrit\"])\n",
    "Wm = params[\"beta\"]  * np.power(v_curr_v, params[\"n_m\"]) * np.power(delta_v, params[\"m_m\"]) * np.exp(-tauK.values / params[\"Topt\"])\n",
    "\n",
    "# Wr_ref e Wm_ref (constantes, usando refs)\n",
    "Wr_ref = params[\"alpha\"] * (v_ref ** params[\"n_r\"]) * (delta_ref ** params[\"m_r\"]) * np.exp(-tau_refK / params[\"Tcrit\"])\n",
    "Wm_ref = params[\"beta\"]  * (v_ref ** params[\"n_m\"]) * (delta_ref ** params[\"m_m\"]) * np.exp(-tau_refK / params[\"Topt\"])\n",
    "\n",
    "# Índices (cuidados para divisão por zero/NaN)\n",
    "def safe_div(a, b):\n",
    "    out = np.full_like(a, np.nan, dtype=\"float64\")\n",
    "    if b is not None and np.isfinite(b) and b != 0:\n",
    "        out = a / b\n",
    "    return out\n",
    "\n",
    "Wr_idx = safe_div(Wr, Wr_ref)\n",
    "Wm_idx = safe_div(Wm, Wm_ref)\n",
    "\n",
    "# --- 6) Gravar na tabela final (não sobrescreve a original) ---\n",
    "df_out = df.copy()\n",
    "df_out[\"Wr\"] = Wr\n",
    "df_out[\"Wm\"] = Wm\n",
    "df_out[\"Wr_ref\"] = Wr_ref if np.isfinite(Wr_ref) else np.nan\n",
    "df_out[\"Wm_ref\"] = Wm_ref if np.isfinite(Wm_ref) else np.nan\n",
    "df_out[\"Wr_idx\"] = Wr_idx\n",
    "df_out[\"Wm_idx\"] = Wm_idx\n",
    "\n",
    "df_out.to_parquet(OUT_PQ, index=False)\n",
    "df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# --- 7) Log de parâmetros e referências usadas ---\n",
    "log = {\n",
    "    \"refs_arquivo_encontrado\": os.path.exists(REFS_CSV),\n",
    "    \"refs_utilizadas\": ref_vals,\n",
    "    \"fallbacks\": {\n",
    "        \"v_ref_usou_median?\"     : not np.isfinite(ref_vals.get(\"v_proxy_total_ref\", np.nan)),\n",
    "        \"delta_ref_usou_median?\" : not np.isfinite(ref_vals.get(\"delta_proxy_ref\", np.nan)),\n",
    "        \"tau_ref_usou_median?\"   : not np.any(np.isfinite(tau_ref_C_vals)),\n",
    "    },\n",
    "    \"params\": params,\n",
    "    \"coluna_v_atual\": \"total_air_flow_knm3_h\" if \"total_air_flow_knm3_h\" in df.columns\n",
    "                      else (\"total_paf_air_flow_knm3_h\" if \"total_paf_air_flow_knm3_h\" in df.columns else None),\n",
    "    \"linhas\": len(df_out),\n",
    "    \"colunas\": list(df_out.columns)\n",
    "}\n",
    "with open(OUT_LOG, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(log, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- 8) Resumo em tela ---\n",
    "print(\"[OK] Tabela com desgaste salva:\")\n",
    "print(\" -\", OUT_PQ)\n",
    "print(\" -\", OUT_CSV)\n",
    "print(\"\\nResumo rápido:\")\n",
    "print(\"  Wr_ref:\", Wr_ref, \" | Wm_ref:\", Wm_ref)\n",
    "print(\"  % NaN Wr_idx:\", np.mean(~np.isfinite(Wr_idx)) * 100, \"% | % NaN Wm_idx:\", np.mean(~np.isfinite(Wm_idx)) * 100, \"%\")\n",
    "print(\"\\nTop 5 maiores Wr_idx:\")\n",
    "print(pd.DataFrame({\"timestamp\": df_out[\"timestamp\"], \"Wr_idx\": Wr_idx}).sort_values(\"Wr_idx\", ascending=False).head(5).to_string(index=False))\n",
    "print(\"\\nTop 5 maiores Wm_idx:\")\n",
    "print(pd.DataFrame({\"timestamp\": df_out[\"timestamp\"], \"Wm_idx\": Wm_idx}).sort_values(\"Wm_idx\", ascending=False).head(5).to_string(index=False))\n",
    "print(\"\\n[INFO] Parâmetros e fallbacks registrados em:\", OUT_LOG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fdb8b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Gráfico salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_idx_timeline.png\n",
      "[OK] Gráfico salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wm_idx_timeline.png\n",
      "[INFO] Período de baseline usado para sombreamento: 2024-03-04 20:00:00 → 2024-03-12 15:00:00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GRÁFICOS TEMPORAIS DE Wr_idx / Wm_idx + MARCAÇÃO DE BASELINE\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "IN_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste.parquet\")\n",
    "LOG_JSON= os.path.join(DST_BASE, r\"outputs\\diagnosticos\\physics_params_mvp.json\")\n",
    "\n",
    "# possíveis fontes de período-baseline (opcionais)\n",
    "CANDIDATES_BASELINE = [\n",
    "    os.path.join(SRC_BASE, r\"outputs\\baseline_datasets\\physics_baseline_proxies.csv\"),\n",
    "    os.path.join(SRC_BASE, r\"outputs\\baseline_mask.csv\"),\n",
    "]\n",
    "\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Ler tabela com desgaste\n",
    "df = pd.read_parquet(IN_PQ)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"timestamp\")\n",
    "\n",
    "# 2) Carregar log para informar se refs vieram de medianas\n",
    "fallback_info = {}\n",
    "if os.path.exists(LOG_JSON):\n",
    "    with open(LOG_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        log_data = json.load(f)\n",
    "    fallback_info = log_data.get(\"fallbacks\", {})\n",
    "\n",
    "# 3) Descobrir período de baseline (se existir)\n",
    "baseline_range = None\n",
    "baseline_source = None\n",
    "\n",
    "# tenta arquivo de proxies de baseline (min..max do timestamp)\n",
    "bp = CANDIDATES_BASELINE[0]\n",
    "if os.path.exists(bp):\n",
    "    try:\n",
    "        bdf = pd.read_csv(bp, encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        bdf = pd.read_csv(bp, encoding=\"latin1\")\n",
    "    if \"timestamp\" in bdf.columns:\n",
    "        bdf[\"timestamp\"] = pd.to_datetime(bdf[\"timestamp\"], errors=\"coerce\")\n",
    "        bdf = bdf.dropna(subset=[\"timestamp\"])\n",
    "        if not bdf.empty:\n",
    "            baseline_range = (bdf[\"timestamp\"].min(), bdf[\"timestamp\"].max())\n",
    "            baseline_source = os.path.relpath(bp, SRC_BASE)\n",
    "\n",
    "# tenta máscara booleana\n",
    "if baseline_range is None:\n",
    "    bm = CANDIDATES_BASELINE[1]\n",
    "    if os.path.exists(bm):\n",
    "        try:\n",
    "            mdf = pd.read_csv(bm, encoding=\"utf-8-sig\")\n",
    "        except Exception:\n",
    "            mdf = pd.read_csv(bm, encoding=\"latin1\")\n",
    "        if {\"timestamp\",\"is_baseline\"}.issubset(mdf.columns):\n",
    "            mdf[\"timestamp\"] = pd.to_datetime(mdf[\"timestamp\"], errors=\"coerce\")\n",
    "            mdf = mdf[mdf[\"is_baseline\"]==1].dropna(subset=[\"timestamp\"])\n",
    "            if not mdf.empty:\n",
    "                baseline_range = (mdf[\"timestamp\"].min(), mdf[\"timestamp\"].max())\n",
    "                baseline_source = os.path.relpath(bm, SRC_BASE)\n",
    "\n",
    "# 4) Função helper para plotar série e sombrear baseline\n",
    "def plot_idx(df, col_idx, out_png, title):\n",
    "    s = df[[\"timestamp\", col_idx]].dropna().copy()\n",
    "    if s.empty:\n",
    "        print(f\"[INFO] Não há dados válidos para {col_idx}. Gráfico não gerado.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(s[\"timestamp\"], s[col_idx])\n",
    "    plt.axhline(1.0, linestyle=\"--\")  # linha do índice 1.0\n",
    "\n",
    "    if baseline_range is not None:\n",
    "        plt.axvspan(baseline_range[0], baseline_range[1], alpha=0.15)\n",
    "        subtitle = f\"Baseline sombreado (fonte: {baseline_source})\"\n",
    "    else:\n",
    "        # informar na figura que não há período explícito\n",
    "        # (refs podem ter vindo de constantes/medianas)\n",
    "        fb_txt = []\n",
    "        for k,v in fallback_info.items():\n",
    "            if isinstance(v, bool):\n",
    "                fb_txt.append(f\"{k}={'sim' if v else 'não'}\")\n",
    "        subtitle = \"Sem período de baseline localizado. \" + (\", \".join(fb_txt) if fb_txt else \"\")\n",
    "\n",
    "    plt.title(f\"{title}\\n{subtitle}\")\n",
    "    plt.xlabel(\"timestamp\")\n",
    "    plt.ylabel(col_idx)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=120)\n",
    "    plt.close()\n",
    "    print(\"[OK] Gráfico salvo em:\", out_png)\n",
    "\n",
    "# 5) Gerar gráficos\n",
    "plot_idx(df, \"Wr_idx\", os.path.join(OUT_DIR, \"wr_idx_timeline.png\"), \"Wr_idx ao longo do tempo\")\n",
    "plot_idx(df, \"Wm_idx\", os.path.join(OUT_DIR, \"wm_idx_timeline.png\"), \"Wm_idx ao longo do tempo\")\n",
    "\n",
    "# 6) Feedback em texto (terminal)\n",
    "if baseline_range is not None:\n",
    "    print(f\"[INFO] Período de baseline usado para sombreamento: {baseline_range[0]} → {baseline_range[1]}\")\n",
    "else:\n",
    "    print(\"[INFO] Não foi encontrado período explícito de baseline nos artefatos padrão.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
