{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7b8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Objetivo: carregar somente os cabeçalhos (nomes das colunas) dos quatro arquivos CSV informados e mostrar em tela, de forma transposta, um quadro onde cada arquivo aparece como uma coluna e os nomes de campo ficam verticalmente.\n",
    "Nada além disso será feito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e483ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz transposta de cabeçalhos (cada coluna é um arquivo; linhas = nomes de campos na ordem original):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "A1_SECONDARIES_FOR_PEDRA_MODEL.csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "A1_SECONDARIES_REFS.csv",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "DELTA_PROXY_DIAGNOSTICS.csv",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e4d4af4b-51e0-4637-b3a1-0488af3c2808",
       "rows": [
        [
         "0",
         "Timestamp",
         "timestamp",
         "delta_proxy_ref",
         "col"
        ],
        [
         "1",
         "coal_flow_furnace_t_h",
         "coal_flow_furnace_t_h",
         "tau_densa_ref",
         "group"
        ],
        [
         "2",
         "total_air_flow_knm3_h",
         "vazao_ar_total_knm3_h",
         "tau_diluida_ref",
         "nan_rate"
        ],
        [
         "3",
         "total_paf_air_flow_knm3_h",
         "vazao_ar_prim_total_knm3_h",
         "tau_backpass_ref",
         "spearman_tau"
        ],
        [
         "4",
         "te_of_hot_pri_air_in_aph_outl_adegc",
         "temp_hot_pri_air_in_preaq_ar_saida",
         "v_proxy_total_ref",
         "spearman_vtotal"
        ],
        [
         "5",
         "tau_densa",
         "tau_densa",
         "v_proxy_primary_ref",
         "spearman_vprimary"
        ],
        [
         "6",
         "tau_diluida",
         "tau_diluida",
         null,
         "trend_tau"
        ],
        [
         "7",
         "tau_backpass",
         "tau_backpass",
         null,
         "trend_vtotal"
        ],
        [
         "8",
         "o2_excess_pct",
         "o2_excess_pct",
         null,
         "trend_vprimary"
        ],
        [
         "9",
         "delta_proxy",
         "delta_proxy",
         null,
         "score"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv</th>\n",
       "      <th>A1_SECONDARIES_FOR_PEDRA_MODEL.csv</th>\n",
       "      <th>A1_SECONDARIES_REFS.csv</th>\n",
       "      <th>DELTA_PROXY_DIAGNOSTICS.csv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Timestamp</td>\n",
       "      <td>timestamp</td>\n",
       "      <td>delta_proxy_ref</td>\n",
       "      <td>col</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coal_flow_furnace_t_h</td>\n",
       "      <td>coal_flow_furnace_t_h</td>\n",
       "      <td>tau_densa_ref</td>\n",
       "      <td>group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_air_flow_knm3_h</td>\n",
       "      <td>vazao_ar_total_knm3_h</td>\n",
       "      <td>tau_diluida_ref</td>\n",
       "      <td>nan_rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>total_paf_air_flow_knm3_h</td>\n",
       "      <td>vazao_ar_prim_total_knm3_h</td>\n",
       "      <td>tau_backpass_ref</td>\n",
       "      <td>spearman_tau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>te_of_hot_pri_air_in_aph_outl_adegc</td>\n",
       "      <td>temp_hot_pri_air_in_preaq_ar_saida</td>\n",
       "      <td>v_proxy_total_ref</td>\n",
       "      <td>spearman_vtotal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tau_densa</td>\n",
       "      <td>tau_densa</td>\n",
       "      <td>v_proxy_primary_ref</td>\n",
       "      <td>spearman_vprimary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tau_diluida</td>\n",
       "      <td>tau_diluida</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trend_tau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tau_backpass</td>\n",
       "      <td>tau_backpass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trend_vtotal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>o2_excess_pct</td>\n",
       "      <td>o2_excess_pct</td>\n",
       "      <td>NaN</td>\n",
       "      <td>trend_vprimary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>delta_proxy</td>\n",
       "      <td>delta_proxy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>score</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv  A1_SECONDARIES_FOR_PEDRA_MODEL.csv  \\\n",
       "0                              Timestamp                           timestamp   \n",
       "1                  coal_flow_furnace_t_h               coal_flow_furnace_t_h   \n",
       "2                  total_air_flow_knm3_h               vazao_ar_total_knm3_h   \n",
       "3              total_paf_air_flow_knm3_h          vazao_ar_prim_total_knm3_h   \n",
       "4    te_of_hot_pri_air_in_aph_outl_adegc  temp_hot_pri_air_in_preaq_ar_saida   \n",
       "5                              tau_densa                           tau_densa   \n",
       "6                            tau_diluida                         tau_diluida   \n",
       "7                           tau_backpass                        tau_backpass   \n",
       "8                          o2_excess_pct                       o2_excess_pct   \n",
       "9                            delta_proxy                         delta_proxy   \n",
       "\n",
       "  A1_SECONDARIES_REFS.csv DELTA_PROXY_DIAGNOSTICS.csv  \n",
       "0         delta_proxy_ref                         col  \n",
       "1           tau_densa_ref                       group  \n",
       "2         tau_diluida_ref                    nan_rate  \n",
       "3        tau_backpass_ref                spearman_tau  \n",
       "4       v_proxy_total_ref             spearman_vtotal  \n",
       "5     v_proxy_primary_ref           spearman_vprimary  \n",
       "6                     NaN                   trend_tau  \n",
       "7                     NaN                trend_vtotal  \n",
       "8                     NaN              trend_vprimary  \n",
       "9                     NaN                       score  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===============================================\n",
    "# ETAPA: LEITURA DE CABEÇALHOS E EXIBIÇÃO TRANSPOSTA\n",
    "# ===============================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Arquivos informados (use exatamente estes caminhos)\n",
    "csv_paths = [\n",
    "    r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\outputs\\pedra\\A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv\",\n",
    "    r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\outputs\\pedra\\A1_SECONDARIES_FOR_PEDRA_MODEL.csv\",\n",
    "    r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\outputs\\pedra\\A1_SECONDARIES_REFS.csv\",\n",
    "    r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\outputs\\pedra\\DELTA_PROXY_DIAGNOSTICS.csv\",\n",
    "]\n",
    "\n",
    "def read_headers_only(path: str):\n",
    "    \"\"\"\n",
    "    Lê apenas os cabeçalhos do CSV (sem carregar dados).\n",
    "    Tenta primeiro utf-8-sig, depois latin1.\n",
    "    Retorna lista de nomes de colunas.\n",
    "    \"\"\"\n",
    "    # Verificação de existência\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Arquivo não encontrado: {path}\")\n",
    "\n",
    "    encodings = [\"utf-8-sig\", \"latin1\"]\n",
    "    last_err = None\n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            # nrows=0 garante leitura apenas do header\n",
    "            df = pd.read_csv(path, nrows=0, encoding=enc)\n",
    "            return list(df.columns)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    # Se todas as tentativas falharem, reapresenta o último erro\n",
    "    raise last_err\n",
    "\n",
    "# Monta um DataFrame onde cada coluna representa um arquivo\n",
    "series_by_file = {}\n",
    "for p in csv_paths:\n",
    "    headers = read_headers_only(p)\n",
    "    name = os.path.basename(p)\n",
    "    # Series indexadas por posição, para fácil visualização vertical\n",
    "    series_by_file[name] = pd.Series(headers, dtype=\"object\")\n",
    "\n",
    "# Concatena lado a lado, alinhando pelo índice (posição dos cabeçalhos)\n",
    "headers_matrix = pd.concat(series_by_file, axis=1)\n",
    "\n",
    "# Exibição: nomes de arquivos como cabeçalhos de coluna\n",
    "pd.set_option(\"display.max_rows\", None)      # mostra todos os cabeçalhos\n",
    "pd.set_option(\"display.max_columns\", None)   # mostra todas as colunas (arquivos)\n",
    "pd.set_option(\"display.width\", 0)\n",
    "\n",
    "print(\"Matriz transposta de cabeçalhos (cada coluna é um arquivo; linhas = nomes de campos na ordem original):\")\n",
    "display(headers_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2745db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\analises_preliminares\\matriz_cabecalhos.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# ETAPA: SALVAMENTO DO DATAFRAME DE CABEÇALHOS\n",
    "# ===============================================\n",
    "\n",
    "import os\n",
    "\n",
    "# Caminho de saída\n",
    "output_dir = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\analises_preliminares\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Arquivo final\n",
    "output_path = os.path.join(output_dir, \"matriz_cabecalhos.csv\")\n",
    "\n",
    "# Salvar DataFrame\n",
    "headers_matrix.to_csv(output_path, index=True, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Arquivo salvo em: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77759fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos com colunas-alvo encontrados: 0\n",
      "Inventário salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\inventario_wr_wm.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# ETAPA: VARREDURA DE CSVs (APENAS CABEÇALHOS)\n",
    "# ===============================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Diretório base do projeto\n",
    "base_dir = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "# Pastas a inspecionar (relativas a base_dir)\n",
    "folders_to_scan = [\"data\", \"outputs\"]\n",
    "\n",
    "# Colunas-alvo\n",
    "target_cols = [\"Wr\", \"Wm\", \"Wr_ref\", \"Wm_ref\", \"Wr_idx\", \"Wm_idx\"]\n",
    "\n",
    "# Onde salvar o inventário\n",
    "output_dir = os.path.join(base_dir, \"outputs\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"inventario_wr_wm.csv\")\n",
    "\n",
    "# Coletar resultados\n",
    "results = []\n",
    "\n",
    "for folder in folders_to_scan:\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if not os.path.exists(folder_path):\n",
    "        continue\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for f in files:\n",
    "            if not f.lower().endswith(\".csv\"):\n",
    "                continue\n",
    "            full_path = os.path.join(root, f)\n",
    "\n",
    "            # Tenta ler só o header com duas codificações comuns\n",
    "            df = None\n",
    "            for enc in (\"utf-8-sig\", \"latin1\"):\n",
    "                try:\n",
    "                    df = pd.read_csv(full_path, nrows=0, encoding=enc)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    df = None\n",
    "\n",
    "            if df is None:\n",
    "                # Não conseguiu ler o cabeçalho — ignora\n",
    "                continue\n",
    "\n",
    "            found = [c for c in target_cols if c in df.columns]\n",
    "            if found:\n",
    "                results.append({\n",
    "                    \"arquivo\": full_path,\n",
    "                    \"colunas_encontradas\": \", \".join(found)\n",
    "                })\n",
    "\n",
    "# Monta DataFrame do inventário\n",
    "df_inventory = pd.DataFrame(results, columns=[\"arquivo\", \"colunas_encontradas\"])\n",
    "\n",
    "# Salva inventário\n",
    "df_inventory.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# Exibe um resumo em tela (não lê dados, só informa)\n",
    "print(f\"Arquivos com colunas-alvo encontrados: {len(df_inventory)}\")\n",
    "print(f\"Inventário salvo em: {output_path}\")\n",
    "if not df_inventory.empty:\n",
    "    print(\"\\nPrévia (até 10 linhas):\")\n",
    "    print(df_inventory.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e922e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Tabela consolidada (sem cálculos) salva em:\n",
      "C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_padrao.csv\n",
      "Linhas: 35,271 | Colunas: 18\n",
      "Colunas com algum preenchimento detectado:\n",
      "['timestamp', 'zona', 'componente', 'total_air_flow_knm3_h', 'total_paf_air_flow_knm3_h', 'te_of_hot_pri_air_in_aph_outl_adegc', 'delta_proxy', 'o2_excess_pct']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONSOLIDAÇÃO MÍNIMA – AJUSTE DE CAMINHOS (MVP, SEM CÁLCULOS)\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Bases de caminho\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"                    # leitura (já existente)\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"         # escrita (criar se não existir)\n",
    "\n",
    "# Garante que o diretório de destino exista\n",
    "os.makedirs(DST_BASE, exist_ok=True)\n",
    "\n",
    "# Fontes preferenciais (em A1_LOCAL)\n",
    "SECUNDARIOS_CANDIDATOS = [\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_FOR_PEDRA_MODEL.csv\"),\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv\"),\n",
    "]\n",
    "REFS_CANDIDATOS = [\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_REFS.csv\"),\n",
    "]\n",
    "\n",
    "# Saída padrão (em A1_LOCAL_REFAZIMENTO)\n",
    "OUT_DIR  = os.path.join(DST_BASE, \"data\", \"derivadas\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PATH = os.path.join(OUT_DIR, \"tabela_wr_wm_padrao.csv\")\n",
    "\n",
    "# Colunas alvo do padrão (sem cálculos; apenas preencher se existirem nas fontes)\n",
    "PADRAO_COLS = [\n",
    "    \"timestamp\",\"zona\",\"componente\",\n",
    "    \"flw_total_c_t_h\",\n",
    "    \"total_air_flow_knm3_h\",\"total_paf_air_flow_knm3_h\",\n",
    "    \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "    \"delta_proxy\",\"tau_densa\",\"tau_diluida\",\"tau_backpass\",\"o2_excess_pct\",\n",
    "    \"Wr\",\"Wm\",\"Wr_ref\",\"Wm_ref\",\"Wr_idx\",\"Wm_idx\",\n",
    "]\n",
    "\n",
    "# Mapeamentos de nomes observados (não inferimos nada novo)\n",
    "NAME_MAP = {\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"Timestamp\": \"timestamp\",\n",
    "    \"vazao_ar_total_knm3_h\": \"total_air_flow_knm3_h\",\n",
    "    \"vazao_ar_prim_total_knm3_h\": \"total_paf_air_flow_knm3_h\",\n",
    "    \"temp_hot_pri_air_in_preaq_ar_saida\": \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "    \"delta_proxy\": \"delta_proxy\",\n",
    "    \"tau_densa\": \"tau_densa\",\n",
    "    \"tau_diluida\": \"tau_diluida\",\n",
    "    \"tau_backpass\": \"tau_backpass\",\n",
    "    \"o2_excess_pct\": \"o2_excess_pct\",\n",
    "}\n",
    "\n",
    "def read_csv_best_effort(path):\n",
    "    for enc in (\"utf-8-sig\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise FileNotFoundError(f\"Falha ao ler: {path}\")\n",
    "\n",
    "def pick_existing(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# 1) Seleciona fontes existentes em A1_LOCAL\n",
    "src_sec = pick_existing(SECUNDARIOS_CANDIDATOS)\n",
    "src_ref = pick_existing(REFS_CANDIDATOS)\n",
    "\n",
    "if src_sec is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Não encontrei nenhum dos arquivos de secundárias esperados em A1_LOCAL\\\\outputs\\\\pedra\\\\ \"\n",
    "        \"(A1_SECONDARIES_FOR_PEDRA_MODEL*.csv).\"\n",
    "    )\n",
    "\n",
    "# 2) Carrega secundárias e aplica renomeações conhecidas\n",
    "df_sec = read_csv_best_effort(src_sec)\n",
    "rename_map = {c: NAME_MAP[c] for c in df_sec.columns if c in NAME_MAP}\n",
    "df_sec = df_sec.rename(columns=rename_map)\n",
    "\n",
    "if \"timestamp\" not in df_sec.columns:\n",
    "    raise RuntimeError(\"A coluna 'timestamp' não foi encontrada após renomeação.\")\n",
    "\n",
    "# 3) Carrega referências (se existir), sem cálculos\n",
    "df_ref = None\n",
    "if src_ref is not None:\n",
    "    try:\n",
    "        df_ref = read_csv_best_effort(src_ref)\n",
    "        ref_rename = {c: NAME_MAP[c] for c in df_ref.columns if c in NAME_MAP}\n",
    "        if ref_rename:\n",
    "            df_ref = df_ref.rename(columns=ref_rename)\n",
    "    except Exception:\n",
    "        df_ref = None\n",
    "\n",
    "# 4) Expandir por 'zona' a partir de tau_* (apenas reshape; sem cálculos)\n",
    "zonas_cols = [c for c in [\"tau_densa\",\"tau_diluida\",\"tau_backpass\"] if c in df_sec.columns]\n",
    "if zonas_cols:\n",
    "    id_cols = [c for c in df_sec.columns if c not in zonas_cols]\n",
    "    df_long = df_sec.melt(\n",
    "        id_vars=id_cols,\n",
    "        value_vars=zonas_cols,\n",
    "        var_name=\"zona\",\n",
    "        value_name=\"tau_val\"\n",
    "    )\n",
    "else:\n",
    "    df_long = df_sec.copy()\n",
    "    df_long[\"zona\"] = \"[INFORMAÇÃO AUSENTE – PRECISAR PREENCHER]\"\n",
    "\n",
    "# 5) Componente ausente nas fontes → marcar como ausente (sem inferir)\n",
    "df_long[\"componente\"] = \"[INFORMAÇÃO AUSENTE – PRECISAR PREENCHER]\"\n",
    "\n",
    "# 6) Merge leve com referências por timestamp (somente novas colunas)\n",
    "if df_ref is not None and \"timestamp\" in df_ref.columns:\n",
    "    cols_to_merge = [c for c in df_ref.columns if c != \"timestamp\" and c not in df_long.columns]\n",
    "    if cols_to_merge:\n",
    "        df_long = df_long.merge(df_ref[[\"timestamp\"] + cols_to_merge], on=\"timestamp\", how=\"left\")\n",
    "\n",
    "# 7) Garantir colunas do padrão (criar vazias quando ausentes)\n",
    "for col in PADRAO_COLS:\n",
    "    if col not in df_long.columns:\n",
    "        df_long[col] = pd.NA\n",
    "\n",
    "# 8) Reordenar e salvar em A1_LOCAL_REFAZIMENTO\n",
    "df_final = df_long[PADRAO_COLS].copy()\n",
    "df_final.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"[OK] Tabela consolidada (sem cálculos) salva em:\\n{OUT_PATH}\")\n",
    "print(f\"Linhas: {len(df_final):,} | Colunas: {len(df_final.columns)}\")\n",
    "print(\"Colunas com algum preenchimento detectado:\")\n",
    "print([c for c in PADRAO_COLS if df_final[c].notna().any()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1be77c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Tabela corrigida salva em:\n",
      "C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_padrao.csv\n",
      "Linhas: 35,271 | Colunas: 18\n",
      "Colunas com algum preenchimento detectado:\n",
      "['timestamp', 'zona', 'componente', 'flw_total_c_t_h', 'total_air_flow_knm3_h', 'total_paf_air_flow_knm3_h', 'te_of_hot_pri_air_in_aph_outl_adegc', 'delta_proxy', 'tau_densa', 'tau_diluida', 'tau_backpass', 'o2_excess_pct']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CONSOLIDAÇÃO (CORREÇÃO): PRESERVAR tau_* E REPLICAR POR ZONA\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Bases de caminho\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"                    # leitura (já existente)\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"         # escrita (criar se não existir)\n",
    "os.makedirs(DST_BASE, exist_ok=True)\n",
    "\n",
    "# Fontes em A1_LOCAL\n",
    "SECUNDARIOS_CANDIDATOS = [\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_FOR_PEDRA_MODEL.csv\"),\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_FOR_PEDRA_MODEL_old.csv\"),\n",
    "]\n",
    "REFS_CANDIDATOS = [\n",
    "    os.path.join(SRC_BASE, \"outputs\", \"pedra\", \"A1_SECONDARIES_REFS.csv\"),\n",
    "]\n",
    "\n",
    "# Saída em A1_LOCAL_REFAZIMENTO\n",
    "OUT_DIR  = os.path.join(DST_BASE, \"data\", \"derivadas\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PATH = os.path.join(OUT_DIR, \"tabela_wr_wm_padrao.csv\")\n",
    "\n",
    "# Padrão de colunas (sem cálculos)\n",
    "PADRAO_COLS = [\n",
    "    \"timestamp\",\"zona\",\"componente\",\n",
    "    \"flw_total_c_t_h\",\n",
    "    \"total_air_flow_knm3_h\",\"total_paf_air_flow_knm3_h\",\n",
    "    \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "    \"delta_proxy\",\"tau_densa\",\"tau_diluida\",\"tau_backpass\",\"o2_excess_pct\",\n",
    "    \"Wr\",\"Wm\",\"Wr_ref\",\"Wm_ref\",\"Wr_idx\",\"Wm_idx\",\n",
    "]\n",
    "\n",
    "# Mapeamentos de nomes observados\n",
    "NAME_MAP = {\n",
    "    # timestamp\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"Timestamp\": \"timestamp\",\n",
    "\n",
    "    # vazões de ar\n",
    "    \"vazao_ar_total_knm3_h\": \"total_air_flow_knm3_h\",\n",
    "    \"total_air_flow_knm3_h\": \"total_air_flow_knm3_h\",\n",
    "    \"vazao_ar_prim_total_knm3_h\": \"total_paf_air_flow_knm3_h\",\n",
    "    \"total_paf_air_flow_knm3_h\": \"total_paf_air_flow_knm3_h\",\n",
    "\n",
    "    # temperatura ar primário (APH)\n",
    "    \"temp_hot_pri_air_in_preaq_ar_saida\": \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "    \"te_of_hot_pri_air_in_aph_outl_adegc\": \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "\n",
    "    # proxies / temps / O2\n",
    "    \"delta_proxy\": \"delta_proxy\",\n",
    "    \"tau_densa\": \"tau_densa\",\n",
    "    \"tau_diluida\": \"tau_diluida\",\n",
    "    \"tau_backpass\": \"tau_backpass\",\n",
    "    \"o2_excess_pct\": \"o2_excess_pct\",\n",
    "\n",
    "    # carvão\n",
    "    \"coal_flow_furnace_t_h\": \"flw_total_c_t_h\",\n",
    "    \"flw_total_c_t_h\": \"flw_total_c_t_h\",\n",
    "}\n",
    "\n",
    "def read_csv_best_effort(path):\n",
    "    for enc in (\"utf-8-sig\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, low_memory=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise FileNotFoundError(f\"Falha ao ler: {path}\")\n",
    "\n",
    "def pick_existing(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# 1) Selecionar fontes\n",
    "src_sec = pick_existing(SECUNDARIOS_CANDIDATOS)\n",
    "src_ref = pick_existing(REFS_CANDIDATOS)\n",
    "\n",
    "if src_sec is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Não encontrei arquivos de secundárias em A1_LOCAL\\\\outputs\\\\pedra\\\\ \"\n",
    "        \"(A1_SECONDARIES_FOR_PEDRA_MODEL*.csv).\"\n",
    "    )\n",
    "\n",
    "# 2) Carregar e renomear colunas conhecidas\n",
    "df_sec = read_csv_best_effort(src_sec)\n",
    "rename_map = {c: NAME_MAP[c] for c in df_sec.columns if c in NAME_MAP}\n",
    "df_sec = df_sec.rename(columns=rename_map)\n",
    "\n",
    "if \"timestamp\" not in df_sec.columns:\n",
    "    raise RuntimeError(\"A coluna 'timestamp' não foi encontrada após renomeação.\")\n",
    "\n",
    "# 3) Carregar referências (merge leve por timestamp, sem cálculos)\n",
    "df_ref = None\n",
    "if src_ref is not None:\n",
    "    try:\n",
    "        df_ref = read_csv_best_effort(src_ref)\n",
    "        ref_rename = {c: NAME_MAP[c] for c in df_ref.columns if c in NAME_MAP}\n",
    "        if ref_rename:\n",
    "            df_ref = df_ref.rename(columns=ref_rename)\n",
    "    except Exception:\n",
    "        df_ref = None\n",
    "\n",
    "# 4) Replicar por zonas disponíveis, PRESERVANDO tau_* como colunas\n",
    "zonas_presentes = []\n",
    "if \"tau_densa\" in df_sec.columns:\n",
    "    zonas_presentes.append(\"densa\")\n",
    "if \"tau_diluida\" in df_sec.columns:\n",
    "    zonas_presentes.append(\"diluida\")\n",
    "if \"tau_backpass\" in df_sec.columns:\n",
    "    zonas_presentes.append(\"backpass\")\n",
    "\n",
    "if zonas_presentes:\n",
    "    partes = []\n",
    "    for z in zonas_presentes:\n",
    "        parte = df_sec.copy()\n",
    "        parte[\"zona\"] = z\n",
    "        partes.append(parte)\n",
    "    df_z = pd.concat(partes, axis=0, ignore_index=True)\n",
    "else:\n",
    "    df_z = df_sec.copy()\n",
    "    df_z[\"zona\"] = \"[INFORMAÇÃO AUSENTE – PRECISAR PREENCHER]\"\n",
    "\n",
    "# 5) Componente ausente → marcador\n",
    "df_z[\"componente\"] = \"[INFORMAÇÃO AUSENTE – PRECISAR PREENCHER]\"\n",
    "\n",
    "# 6) Merge leve com referências (adiciona colunas que não existirem ainda)\n",
    "if df_ref is not None and \"timestamp\" in df_ref.columns:\n",
    "    cols_to_merge = [c for c in df_ref.columns if c != \"timestamp\" and c not in df_z.columns]\n",
    "    if cols_to_merge:\n",
    "        df_z = df_z.merge(df_ref[[\"timestamp\"] + cols_to_merge], on=\"timestamp\", how=\"left\")\n",
    "\n",
    "# 7) Garantir todas as colunas do padrão (criando vazias quando necessário)\n",
    "for col in PADRAO_COLS:\n",
    "    if col not in df_z.columns:\n",
    "        df_z[col] = pd.NA\n",
    "\n",
    "# 8) Reordenar e salvar\n",
    "df_final = df_z[PADRAO_COLS].copy()\n",
    "df_final.to_csv(OUT_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"[OK] Tabela corrigida salva em:\\n{OUT_PATH}\")\n",
    "print(f\"Linhas: {len(df_final):,} | Colunas: {len(df_final.columns)}\")\n",
    "print(\"Colunas com algum preenchimento detectado:\")\n",
    "print([c for c in PADRAO_COLS if df_final[c].notna().any()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39ebb91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Parquet gerado:\n",
      " caminho: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_padrao.parquet\n",
      " shape : (35271, 18)\n",
      " colunas: ['timestamp', 'zona', 'componente', 'flw_total_c_t_h', 'total_air_flow_knm3_h', 'total_paf_air_flow_knm3_h', 'te_of_hot_pri_air_in_aph_outl_adegc', 'delta_proxy', 'tau_densa', 'tau_diluida', 'tau_backpass', 'o2_excess_pct', 'Wr', 'Wm', 'Wr_ref', 'Wm_ref', 'Wr_idx', 'Wm_idx']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EXPORTAR TABELA PADRÃO PARA PARQUET (SEM CÁLCULOS)\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "IN_CSV   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_padrao.csv\")\n",
    "OUT_DIR  = os.path.join(DST_BASE, r\"data\\derivadas\")\n",
    "OUT_PQ   = os.path.join(OUT_DIR, \"tabela_wr_wm_padrao.parquet\")\n",
    "\n",
    "# Garantir diretório de saída\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Ler CSV (conteúdo já consolidado anteriormente)\n",
    "df = pd.read_csv(IN_CSV, low_memory=False)\n",
    "\n",
    "# Salvar em Parquet (engine pyarrow, se disponível)\n",
    "df.to_parquet(OUT_PQ, index=False)\n",
    "\n",
    "# Validação rápida\n",
    "df_check = pd.read_parquet(OUT_PQ)\n",
    "print(\"[OK] Parquet gerado:\")\n",
    "print(\" caminho:\", OUT_PQ)\n",
    "print(\" shape :\", df_check.shape)\n",
    "print(\" colunas:\", list(df_check.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7299ea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] shape: (35271, 18) (linhas=35271, colunas=18)\n",
      "\n",
      "[EXATAS]\n",
      " - Linhas pertencentes a grupos duplicados (inclui primeira ocorrência): 0\n",
      " - Linhas duplicadas além da primeira (contagem de 'extras'):          0\n",
      " - Linhas únicas após remover exatas:                                 35271\n",
      " - Nenhuma duplicata exata encontrada.\n",
      "\n",
      "[POR CHAVE: timestamp, zona, componente]\n",
      " - Quantidade de chaves duplicadas: 0\n",
      " - Total de linhas cobertas por essas chaves: 0\n",
      "\n",
      "[CONTAGEM POR ZONA]\n",
      "zona\n",
      "densa       11757\n",
      "diluida     11757\n",
      "backpass    11757\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# DIAGNÓSTICO DE DUPLICIDADES NO PARQUET\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "PQ_PATH  = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_padrao.parquet\")\n",
    "\n",
    "OUT_DIR  = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_DUP_ALL = os.path.join(OUT_DIR, \"duplicatas_exatas.csv\")\n",
    "OUT_DUP_KEY = os.path.join(OUT_DIR, \"duplicatas_por_chave_timestamp_zona_componente.csv\")\n",
    "\n",
    "# 1) Ler parquet\n",
    "df = pd.read_parquet(PQ_PATH)\n",
    "total = len(df)\n",
    "print(f\"[INFO] shape: {df.shape} (linhas={total}, colunas={df.shape[1]})\")\n",
    "\n",
    "# 2) Duplicatas EXATAS (todas as colunas iguais)\n",
    "mask_dups_all = df.duplicated(keep=False)  # marca todos os membros de cada grupo duplicado\n",
    "qtd_linhas_em_grupos_duplicados = int(mask_dups_all.sum())\n",
    "qtd_linhas_duplicadas_alem_da_primeira = int(df.duplicated().sum())\n",
    "qtd_unicas = int(len(df.drop_duplicates()))\n",
    "\n",
    "print(\"\\n[EXATAS]\")\n",
    "print(f\" - Linhas pertencentes a grupos duplicados (inclui primeira ocorrência): {qtd_linhas_em_grupos_duplicados}\")\n",
    "print(f\" - Linhas duplicadas além da primeira (contagem de 'extras'):          {qtd_linhas_duplicadas_alem_da_primeira}\")\n",
    "print(f\" - Linhas únicas após remover exatas:                                 {qtd_unicas}\")\n",
    "\n",
    "# Exporta amostra das duplicatas exatas (se existir)\n",
    "if qtd_linhas_em_grupos_duplicados > 0:\n",
    "    df_dups_all = df[mask_dups_all].copy()\n",
    "    # Para reduzir tamanho do arquivo, limitamos a 50k linhas na exportação (ajuste se quiser)\n",
    "    if len(df_dups_all) > 50000:\n",
    "        df_dups_all = df_dups_all.head(50000)\n",
    "    df_dups_all.to_csv(OUT_DUP_ALL, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\" - Inventário (amostra) salvo em: {OUT_DUP_ALL}\")\n",
    "else:\n",
    "    print(\" - Nenhuma duplicata exata encontrada.\")\n",
    "\n",
    "# 3) Duplicatas por CHAVE ['timestamp','zona','componente']\n",
    "key_cols = [c for c in [\"timestamp\",\"zona\",\"componente\"] if c in df.columns]\n",
    "if len(key_cols) == 3:\n",
    "    grp = df.groupby(key_cols, dropna=False).size().reset_index(name=\"count\")\n",
    "    dup_keys = grp[grp[\"count\"] > 1].sort_values(\"count\", ascending=False)\n",
    "    qtd_chaves_duplicadas = int(len(dup_keys))\n",
    "    total_linhas_em_chaves_duplicadas = int(dup_keys[\"count\"].sum())\n",
    "\n",
    "    print(\"\\n[POR CHAVE: timestamp, zona, componente]\")\n",
    "    print(f\" - Quantidade de chaves duplicadas: {qtd_chaves_duplicadas}\")\n",
    "    print(f\" - Total de linhas cobertas por essas chaves: {total_linhas_em_chaves_duplicadas}\")\n",
    "\n",
    "    if qtd_chaves_duplicadas > 0:\n",
    "        # Exportar todas as chaves duplicadas\n",
    "        dup_keys.to_csv(OUT_DUP_KEY, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\" - Inventário de chaves duplicadas salvo em: {OUT_DUP_KEY}\")\n",
    "else:\n",
    "    print(\"\\n[POR CHAVE]\")\n",
    "    print(\" - A checagem por chave foi pulada porque faltam colunas em ['timestamp','zona','componente'].\")\n",
    "\n",
    "# 4) Contagem por zona (útil para entender multiplicação esperada)\n",
    "if \"zona\" in df.columns:\n",
    "    print(\"\\n[CONTAGEM POR ZONA]\")\n",
    "    print(df[\"zona\"].value_counts(dropna=False).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6a3590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] shape original: (35271, 18)\n",
      "[INFO] Nenhum conflito detectado entre as 3 linhas por timestamp (além de 'zona').\n",
      "\n",
      "[OK] Tabela deduplicada gerada:\n",
      " - Parquet: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_padrao_dedup.parquet\n",
      " - CSV    : C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_padrao_dedup.csv\n",
      "Shape final: (11757, 17)\n",
      "Total de timestamps únicos: 11757\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# COLAPSAR ZONAS -> 1 LINHA POR TIMESTAMP (SEM CÁLCULOS)\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "IN_PQ    = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_padrao.parquet\")\n",
    "\n",
    "OUT_DIR  = os.path.join(DST_BASE, r\"data\\derivadas\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PQ   = os.path.join(OUT_DIR, \"tabela_wr_wm_padrao_dedup.parquet\")\n",
    "OUT_CSV  = os.path.join(OUT_DIR, \"tabela_wr_wm_padrao_dedup.csv\")\n",
    "\n",
    "DIAG_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(DIAG_DIR, exist_ok=True)\n",
    "OUT_CONFLITOS = os.path.join(DIAG_DIR, \"conflitos_por_timestamp.csv\")\n",
    "\n",
    "# 1) Ler parquet atual\n",
    "df = pd.read_parquet(IN_PQ)\n",
    "print(\"[INFO] shape original:\", df.shape)\n",
    "\n",
    "# 2) Verificar conflitos entre as 3 linhas por timestamp (excluindo 'zona')\n",
    "cols_check = [c for c in df.columns if c != \"zona\"]\n",
    "conf_linhas = []\n",
    "\n",
    "# (Opcional) primeiro, confirmar a multiplicidade por timestamp\n",
    "# (não bloqueia se não for sempre 3)\n",
    "mult_por_ts = df.groupby(\"timestamp\", dropna=False).size()\n",
    "\n",
    "# Varredura de conflitos (se colunas variam dentro do mesmo timestamp)\n",
    "for ts, grp in df.groupby(\"timestamp\", dropna=False):\n",
    "    diffs = [c for c in cols_check if grp[c].nunique(dropna=False) > 1]\n",
    "    if diffs:\n",
    "        conf_linhas.append({\n",
    "            \"timestamp\": ts,\n",
    "            \"n_linhas_grupo\": len(grp),\n",
    "            \"colunas_com_diferencas\": \", \".join(diffs)\n",
    "        })\n",
    "\n",
    "# Exporta conflitos, se existirem\n",
    "if conf_linhas:\n",
    "    pd.DataFrame(conf_linhas).to_csv(OUT_CONFLITOS, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[ATENÇÃO] Conflitos detectados por timestamp. Inventário salvo em:\\n{OUT_CONFLITOS}\")\n",
    "else:\n",
    "    print(\"[INFO] Nenhum conflito detectado entre as 3 linhas por timestamp (além de 'zona').\")\n",
    "\n",
    "# 3) Colapsar: remover 'zona' e deduplicar por 'timestamp' (mantendo a primeira ocorrência)\n",
    "df_nz = df.drop(columns=[\"zona\"], errors=\"ignore\").copy()\n",
    "\n",
    "# Ordena por timestamp para garantir determinismo na escolha da primeira\n",
    "df_nz = df_nz.sort_values(by=[\"timestamp\"]).copy()\n",
    "\n",
    "# Remove duplicatas por timestamp\n",
    "df_dedup = df_nz.drop_duplicates(subset=[\"timestamp\"], keep=\"first\").copy()\n",
    "\n",
    "# 4) Salvar resultados\n",
    "df_dedup.to_parquet(OUT_PQ, index=False)\n",
    "df_dedup.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\n[OK] Tabela deduplicada gerada:\")\n",
    "print(\" - Parquet:\", OUT_PQ)\n",
    "print(\" - CSV    :\", OUT_CSV)\n",
    "print(\"Shape final:\", df_dedup.shape)\n",
    "\n",
    "# 5) Validações auxiliares\n",
    "if \"timestamp\" in df_dedup.columns:\n",
    "    n_ts_unique = df_dedup[\"timestamp\"].nunique(dropna=False)\n",
    "    print(\"Total de timestamps únicos:\", n_ts_unique)\n",
    "else:\n",
    "    print(\"[ALERTA] Coluna 'timestamp' ausente após deduplicação (não esperado).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cfb9cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Derivados salvos:\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\anomalias_isoforest.parquet\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\anomalias_isoforest.csv\n",
      "[OK] Modelo salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\modelos\\isoforest_baseline.pkl\n",
      "\n",
      "Resumo:\n",
      "Registros: 11,757\n",
      "Features usadas: ['flw_total_c_t_h', 'total_air_flow_knm3_h', 'total_paf_air_flow_knm3_h', 'te_of_hot_pri_air_in_aph_outl_adegc', 'delta_proxy', 'tau_densa', 'tau_diluida', 'tau_backpass', 'o2_excess_pct']\n",
      "% anomalias (contamination alvo=3%): 3.00%\n",
      "\n",
      "Top 5 timestamps mais anômalos (maior score):\n",
      "          timestamp  anomalia_score\n",
      "2023-07-04 04:00:00        1.000000\n",
      "2023-05-31 11:00:00        0.996667\n",
      "2023-11-16 19:00:00        0.990161\n",
      "2023-05-31 09:00:00        0.954710\n",
      "2023-10-11 21:00:00        0.926770\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# MVP DE MODELO: ISOLATION FOREST (CORREÇÃO NumPy 2.0)\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "IN_PQ    = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_padrao_dedup.parquet\")\n",
    "DERIV_DIR = os.path.join(DST_BASE, r\"data\\derivadas\")\n",
    "MODEL_DIR = os.path.join(DST_BASE, r\"outputs\\modelos\")\n",
    "DIAG_DIR  = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "\n",
    "os.makedirs(DERIV_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DIAG_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Ler base deduplicada\n",
    "df = pd.read_parquet(IN_PQ)\n",
    "\n",
    "# 2) Seleção de features já preenchidas\n",
    "features = [\n",
    "    \"flw_total_c_t_h\",\n",
    "    \"total_air_flow_knm3_h\",\n",
    "    \"total_paf_air_flow_knm3_h\",\n",
    "    \"te_of_hot_pri_air_in_aph_outl_adegc\",\n",
    "    \"delta_proxy\",\n",
    "    \"tau_densa\",\n",
    "    \"tau_diluida\",\n",
    "    \"tau_backpass\",\n",
    "    \"o2_excess_pct\",\n",
    "]\n",
    "features = [c for c in features if c in df.columns]\n",
    "\n",
    "# 3) Subconjunto de dados (timestamp + features)\n",
    "df_model = df[[\"timestamp\"] + features].copy()\n",
    "for c in features:\n",
    "    df_model[c] = pd.to_numeric(df_model[c], errors=\"coerce\")\n",
    "\n",
    "# 4) Pipeline: Imputer (mediana) + StandardScaler + IsolationForest\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"iso\", IsolationForest(\n",
    "        n_estimators=300,\n",
    "        max_samples=\"auto\",\n",
    "        contamination=0.03,   # ~3% anomalias\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "X = df_model[features].values\n",
    "pipeline.fit(X)\n",
    "\n",
    "# 5) Scores e rótulos (usando o pipeline para aplicar transformações)\n",
    "# decision_function: maior = mais normal. Vamos inverter para anomalia (maior = mais anômalo).\n",
    "raw_scores = pipeline.decision_function(X)  # array shape (n,)\n",
    "anom_base = -raw_scores\n",
    "rng = np.ptp(anom_base)  # max - min (NumPy 2.0+)\n",
    "if not np.isfinite(rng) or rng == 0:\n",
    "    rng = 1e-12\n",
    "anomalia_score = (anom_base - np.min(anom_base)) / rng  # 0..1\n",
    "\n",
    "pred_labels = pipeline.predict(X)  # -1 anomalia, 1 normal\n",
    "is_anomalia = (pred_labels == -1).astype(int)\n",
    "\n",
    "# 6) Montar saída\n",
    "out = df_model.copy()\n",
    "out[\"anomalia_score\"] = anomalia_score\n",
    "out[\"is_anomalia\"] = is_anomalia\n",
    "\n",
    "# 7) Salvar derivados\n",
    "out_pq  = os.path.join(DERIV_DIR, \"anomalias_isoforest.parquet\")\n",
    "out_csv = os.path.join(DERIV_DIR, \"anomalias_isoforest.csv\")\n",
    "out.to_parquet(out_pq, index=False)\n",
    "out.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 8) Salvar modelo\n",
    "model_path = os.path.join(MODEL_DIR, \"isoforest_baseline.pkl\")\n",
    "joblib.dump(pipeline, model_path)\n",
    "\n",
    "# 9) Estatísticas simples\n",
    "pct_anom = 100.0 * out[\"is_anomalia\"].mean()\n",
    "sumario = []\n",
    "sumario.append(f\"Registros: {len(out):,}\")\n",
    "sumario.append(f\"Features usadas: {features}\")\n",
    "sumario.append(f\"% anomalias (contamination alvo=3%): {pct_anom:.2f}%\")\n",
    "top5 = out.sort_values(\"anomalia_score\", ascending=False).head(5)[[\"timestamp\",\"anomalia_score\"]]\n",
    "sumario.append(\"\\nTop 5 timestamps mais anômalos (maior score):\")\n",
    "sumario.append(top5.to_string(index=False))\n",
    "\n",
    "diag_path = os.path.join(DIAG_DIR, \"anomalia_stats.txt\")\n",
    "with open(diag_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(sumario))\n",
    "\n",
    "print(\"[OK] Derivados salvos:\")\n",
    "print(\" -\", out_pq)\n",
    "print(\" -\", out_csv)\n",
    "print(\"[OK] Modelo salvo em:\", model_path)\n",
    "print(\"\\nResumo:\")\n",
    "print(\"\\n\".join(sumario))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "491b0807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Tabela com desgaste salva:\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_com_desgaste.parquet\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_com_desgaste.csv\n",
      "\n",
      "Resumo rápido:\n",
      "  Wr_ref: 166056.27285204153  | Wm_ref: 248106045.49944544\n",
      "  % NaN Wr_idx: 36.437866802755806 % | % NaN Wm_idx: 81.20268776048312 %\n",
      "\n",
      "Top 5 maiores Wr_idx:\n",
      "          timestamp    Wr_idx\n",
      "2024-09-29 14:00:00 12.490335\n",
      "2024-03-29 05:00:00 11.579226\n",
      "2024-03-29 16:00:00 11.470146\n",
      "2024-03-29 02:00:00 11.241728\n",
      "2024-03-29 01:00:00 10.761733\n",
      "\n",
      "Top 5 maiores Wm_idx:\n",
      "          timestamp   Wm_idx\n",
      "2024-09-29 14:00:00 3.428456\n",
      "2024-03-29 05:00:00 3.291170\n",
      "2024-03-29 16:00:00 3.263653\n",
      "2024-03-29 02:00:00 3.181821\n",
      "2024-03-29 01:00:00 3.140810\n",
      "\n",
      "[INFO] Parâmetros e fallbacks registrados em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\physics_params_mvp.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilso\\AppData\\Local\\Temp\\ipykernel_26704\\1751241561.py:124: RuntimeWarning: invalid value encountered in power\n",
      "  Wm = params[\"beta\"]  * np.power(v_curr_v, params[\"n_m\"]) * np.power(delta_v, params[\"m_m\"]) * np.exp(-tauK.values / params[\"Topt\"])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHYSICS-BASED (MVP): Wr/Wm + refs + índices\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "IN_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_padrao_dedup.parquet\")\n",
    "REFS_CSV= os.path.join(SRC_BASE, r\"outputs\\pedra\\A1_SECONDARIES_REFS.csv\")\n",
    "\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"data\\derivadas\")\n",
    "LOG_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "OUT_PQ  = os.path.join(OUT_DIR, \"tabela_wr_wm_com_desgaste.parquet\")\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"tabela_wr_wm_com_desgaste.csv\")\n",
    "OUT_LOG = os.path.join(LOG_DIR, \"physics_params_mvp.json\")\n",
    "\n",
    "# --- 1) Ler base deduplicada ---\n",
    "df = pd.read_parquet(IN_PQ)\n",
    "\n",
    "# Garantir numérico nas colunas usadas\n",
    "def to_num(s):\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "for c in [\"delta_proxy\",\"tau_densa\",\"tau_diluida\",\"tau_backpass\",\n",
    "          \"total_air_flow_knm3_h\",\"total_paf_air_flow_knm3_h\",\"flw_total_c_t_h\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = to_num(df[c])\n",
    "\n",
    "# --- 2) Ler referências (se houver) ---\n",
    "def read_csv_best_effort(path):\n",
    "    for enc in (\"utf-8-sig\",\"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "refs = read_csv_best_effort(REFS_CSV)\n",
    "\n",
    "# Extrair refs como únicos valores (esperado 1 linha). Se houver várias, pegar a 1ª não nula.\n",
    "def pick_ref(series):\n",
    "    if series is None:\n",
    "        return np.nan\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    s = s.dropna()\n",
    "    return s.iloc[0] if len(s) else np.nan\n",
    "\n",
    "ref_vals = {}\n",
    "if refs is not None:\n",
    "    for col in [\"delta_proxy_ref\",\"tau_densa_ref\",\"tau_diluida_ref\",\"tau_backpass_ref\",\n",
    "                \"v_proxy_total_ref\",\"v_proxy_primary_ref\"]:\n",
    "        if col in refs.columns:\n",
    "            ref_vals[col] = pick_ref(refs[col])\n",
    "        else:\n",
    "            ref_vals[col] = np.nan\n",
    "else:\n",
    "    # sem arquivo de refs\n",
    "    ref_vals = {k: np.nan for k in\n",
    "        [\"delta_proxy_ref\",\"tau_densa_ref\",\"tau_diluida_ref\",\"tau_backpass_ref\",\"v_proxy_total_ref\",\"v_proxy_primary_ref\"]}\n",
    "\n",
    "# --- 3) Definir variáveis de trabalho (MVP) ---\n",
    "# τ (°C) -> τK (K). Usaremos média das três zonas disponíveis.\n",
    "taus = [c for c in [\"tau_densa\",\"tau_diluida\",\"tau_backpass\"] if c in df.columns]\n",
    "df[\"tau_mean_C\"] = np.nan\n",
    "if taus:\n",
    "    df[\"tau_mean_C\"] = pd.concat([df[t] for t in taus], axis=1).mean(axis=1, skipna=True)\n",
    "\n",
    "# v (proxy de velocidade): preferimos ar total; se não tiver, ar primário; fallback: mediana.\n",
    "if \"total_air_flow_knm3_h\" in df.columns:\n",
    "    v_curr = df[\"total_air_flow_knm3_h\"].copy()\n",
    "elif \"total_paf_air_flow_knm3_h\" in df.columns:\n",
    "    v_curr = df[\"total_paf_air_flow_knm3_h\"].copy()\n",
    "else:\n",
    "    v_curr = pd.Series(np.nan, index=df.index)\n",
    "\n",
    "# delta_proxy atual\n",
    "delta_curr = df[\"delta_proxy\"] if \"delta_proxy\" in df.columns else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "# Refs: se existirem no CSV, usamos; senão, fallback pela mediana dos próprios dados\n",
    "v_ref = ref_vals.get(\"v_proxy_total_ref\", np.nan)\n",
    "if not np.isfinite(v_ref):\n",
    "    v_ref = np.nanmedian(v_curr.values)\n",
    "\n",
    "delta_ref = ref_vals.get(\"delta_proxy_ref\", np.nan)\n",
    "if not np.isfinite(delta_ref):\n",
    "    delta_ref = np.nanmedian(delta_curr.values)\n",
    "\n",
    "tau_ref_C_vals = [ref_vals.get(\"tau_densa_ref\", np.nan),\n",
    "                  ref_vals.get(\"tau_diluida_ref\", np.nan),\n",
    "                  ref_vals.get(\"tau_backpass_ref\", np.nan)]\n",
    "tau_ref_C = np.nanmean([x for x in tau_ref_C_vals if np.isfinite(x)]) if np.any(np.isfinite(tau_ref_C_vals)) else np.nan\n",
    "if not np.isfinite(tau_ref_C):\n",
    "    tau_ref_C = np.nanmedian(df[\"tau_mean_C\"].values)\n",
    "\n",
    "# --- 4) Constantes do MVP (ajustáveis) ---\n",
    "params = {\n",
    "    \"alpha\": 1.0,        # fatores escala (cancelam nos índices)\n",
    "    \"beta\":  1.0,\n",
    "    \"n_r\":   2.0,        # expoente de v para Wr\n",
    "    \"m_r\":   1.0,        # expoente de delta para Wr\n",
    "    \"n_m\":   3.0,        # expoente de v para Wm\n",
    "    \"m_m\":   0.5,        # expoente de delta para Wm\n",
    "    \"Tcrit\": 1200.0,     # K (refratário)\n",
    "    \"Topt\":  1250.0      # K (metálico)\n",
    "}\n",
    "\n",
    "# --- 5) Cálculo (Wr/Wm e refs) ---\n",
    "# Preparos\n",
    "tauK     = to_num(df[\"tau_mean_C\"]) + 273.15\n",
    "tau_refK = float(tau_ref_C) + 273.15\n",
    "\n",
    "v_curr_v = to_num(v_curr).values\n",
    "delta_v  = to_num(delta_curr).values\n",
    "\n",
    "# Wr e Wm atuais (proxy adimensional de desgaste)\n",
    "Wr = params[\"alpha\"] * np.power(v_curr_v, params[\"n_r\"]) * np.power(delta_v, params[\"m_r\"]) * np.exp(-tauK.values / params[\"Tcrit\"])\n",
    "Wm = params[\"beta\"]  * np.power(v_curr_v, params[\"n_m\"]) * np.power(delta_v, params[\"m_m\"]) * np.exp(-tauK.values / params[\"Topt\"])\n",
    "\n",
    "# Wr_ref e Wm_ref (constantes, usando refs)\n",
    "Wr_ref = params[\"alpha\"] * (v_ref ** params[\"n_r\"]) * (delta_ref ** params[\"m_r\"]) * np.exp(-tau_refK / params[\"Tcrit\"])\n",
    "Wm_ref = params[\"beta\"]  * (v_ref ** params[\"n_m\"]) * (delta_ref ** params[\"m_m\"]) * np.exp(-tau_refK / params[\"Topt\"])\n",
    "\n",
    "# Índices (cuidados para divisão por zero/NaN)\n",
    "def safe_div(a, b):\n",
    "    out = np.full_like(a, np.nan, dtype=\"float64\")\n",
    "    if b is not None and np.isfinite(b) and b != 0:\n",
    "        out = a / b\n",
    "    return out\n",
    "\n",
    "Wr_idx = safe_div(Wr, Wr_ref)\n",
    "Wm_idx = safe_div(Wm, Wm_ref)\n",
    "\n",
    "# --- 6) Gravar na tabela final (não sobrescreve a original) ---\n",
    "df_out = df.copy()\n",
    "df_out[\"Wr\"] = Wr\n",
    "df_out[\"Wm\"] = Wm\n",
    "df_out[\"Wr_ref\"] = Wr_ref if np.isfinite(Wr_ref) else np.nan\n",
    "df_out[\"Wm_ref\"] = Wm_ref if np.isfinite(Wm_ref) else np.nan\n",
    "df_out[\"Wr_idx\"] = Wr_idx\n",
    "df_out[\"Wm_idx\"] = Wm_idx\n",
    "\n",
    "df_out.to_parquet(OUT_PQ, index=False)\n",
    "df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# --- 7) Log de parâmetros e referências usadas ---\n",
    "log = {\n",
    "    \"refs_arquivo_encontrado\": os.path.exists(REFS_CSV),\n",
    "    \"refs_utilizadas\": ref_vals,\n",
    "    \"fallbacks\": {\n",
    "        \"v_ref_usou_median?\"     : not np.isfinite(ref_vals.get(\"v_proxy_total_ref\", np.nan)),\n",
    "        \"delta_ref_usou_median?\" : not np.isfinite(ref_vals.get(\"delta_proxy_ref\", np.nan)),\n",
    "        \"tau_ref_usou_median?\"   : not np.any(np.isfinite(tau_ref_C_vals)),\n",
    "    },\n",
    "    \"params\": params,\n",
    "    \"coluna_v_atual\": \"total_air_flow_knm3_h\" if \"total_air_flow_knm3_h\" in df.columns\n",
    "                      else (\"total_paf_air_flow_knm3_h\" if \"total_paf_air_flow_knm3_h\" in df.columns else None),\n",
    "    \"linhas\": len(df_out),\n",
    "    \"colunas\": list(df_out.columns)\n",
    "}\n",
    "with open(OUT_LOG, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(log, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- 8) Resumo em tela ---\n",
    "print(\"[OK] Tabela com desgaste salva:\")\n",
    "print(\" -\", OUT_PQ)\n",
    "print(\" -\", OUT_CSV)\n",
    "print(\"\\nResumo rápido:\")\n",
    "print(\"  Wr_ref:\", Wr_ref, \" | Wm_ref:\", Wm_ref)\n",
    "print(\"  % NaN Wr_idx:\", np.mean(~np.isfinite(Wr_idx)) * 100, \"% | % NaN Wm_idx:\", np.mean(~np.isfinite(Wm_idx)) * 100, \"%\")\n",
    "print(\"\\nTop 5 maiores Wr_idx:\")\n",
    "print(pd.DataFrame({\"timestamp\": df_out[\"timestamp\"], \"Wr_idx\": Wr_idx}).sort_values(\"Wr_idx\", ascending=False).head(5).to_string(index=False))\n",
    "print(\"\\nTop 5 maiores Wm_idx:\")\n",
    "print(pd.DataFrame({\"timestamp\": df_out[\"timestamp\"], \"Wm_idx\": Wm_idx}).sort_values(\"Wm_idx\", ascending=False).head(5).to_string(index=False))\n",
    "print(\"\\n[INFO] Parâmetros e fallbacks registrados em:\", OUT_LOG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fdb8b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Gráfico salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_idx_timeline.png\n",
      "[OK] Gráfico salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wm_idx_timeline.png\n",
      "[INFO] Período de baseline usado para sombreamento: 2024-03-04 20:00:00 → 2024-03-12 15:00:00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# GRÁFICOS TEMPORAIS DE Wr_idx / Wm_idx + MARCAÇÃO DE BASELINE\n",
    "# ============================================================\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "IN_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste.parquet\")\n",
    "LOG_JSON= os.path.join(DST_BASE, r\"outputs\\diagnosticos\\physics_params_mvp.json\")\n",
    "\n",
    "# possíveis fontes de período-baseline (opcionais)\n",
    "CANDIDATES_BASELINE = [\n",
    "    os.path.join(SRC_BASE, r\"outputs\\baseline_datasets\\physics_baseline_proxies.csv\"),\n",
    "    os.path.join(SRC_BASE, r\"outputs\\baseline_mask.csv\"),\n",
    "]\n",
    "\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Ler tabela com desgaste\n",
    "df = pd.read_parquet(IN_PQ)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"timestamp\")\n",
    "\n",
    "# 2) Carregar log para informar se refs vieram de medianas\n",
    "fallback_info = {}\n",
    "if os.path.exists(LOG_JSON):\n",
    "    with open(LOG_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        log_data = json.load(f)\n",
    "    fallback_info = log_data.get(\"fallbacks\", {})\n",
    "\n",
    "# 3) Descobrir período de baseline (se existir)\n",
    "baseline_range = None\n",
    "baseline_source = None\n",
    "\n",
    "# tenta arquivo de proxies de baseline (min..max do timestamp)\n",
    "bp = CANDIDATES_BASELINE[0]\n",
    "if os.path.exists(bp):\n",
    "    try:\n",
    "        bdf = pd.read_csv(bp, encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        bdf = pd.read_csv(bp, encoding=\"latin1\")\n",
    "    if \"timestamp\" in bdf.columns:\n",
    "        bdf[\"timestamp\"] = pd.to_datetime(bdf[\"timestamp\"], errors=\"coerce\")\n",
    "        bdf = bdf.dropna(subset=[\"timestamp\"])\n",
    "        if not bdf.empty:\n",
    "            baseline_range = (bdf[\"timestamp\"].min(), bdf[\"timestamp\"].max())\n",
    "            baseline_source = os.path.relpath(bp, SRC_BASE)\n",
    "\n",
    "# tenta máscara booleana\n",
    "if baseline_range is None:\n",
    "    bm = CANDIDATES_BASELINE[1]\n",
    "    if os.path.exists(bm):\n",
    "        try:\n",
    "            mdf = pd.read_csv(bm, encoding=\"utf-8-sig\")\n",
    "        except Exception:\n",
    "            mdf = pd.read_csv(bm, encoding=\"latin1\")\n",
    "        if {\"timestamp\",\"is_baseline\"}.issubset(mdf.columns):\n",
    "            mdf[\"timestamp\"] = pd.to_datetime(mdf[\"timestamp\"], errors=\"coerce\")\n",
    "            mdf = mdf[mdf[\"is_baseline\"]==1].dropna(subset=[\"timestamp\"])\n",
    "            if not mdf.empty:\n",
    "                baseline_range = (mdf[\"timestamp\"].min(), mdf[\"timestamp\"].max())\n",
    "                baseline_source = os.path.relpath(bm, SRC_BASE)\n",
    "\n",
    "# 4) Função helper para plotar série e sombrear baseline\n",
    "def plot_idx(df, col_idx, out_png, title):\n",
    "    s = df[[\"timestamp\", col_idx]].dropna().copy()\n",
    "    if s.empty:\n",
    "        print(f\"[INFO] Não há dados válidos para {col_idx}. Gráfico não gerado.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(s[\"timestamp\"], s[col_idx])\n",
    "    plt.axhline(1.0, linestyle=\"--\")  # linha do índice 1.0\n",
    "\n",
    "    if baseline_range is not None:\n",
    "        plt.axvspan(baseline_range[0], baseline_range[1], alpha=0.15)\n",
    "        subtitle = f\"Baseline sombreado (fonte: {baseline_source})\"\n",
    "    else:\n",
    "        # informar na figura que não há período explícito\n",
    "        # (refs podem ter vindo de constantes/medianas)\n",
    "        fb_txt = []\n",
    "        for k,v in fallback_info.items():\n",
    "            if isinstance(v, bool):\n",
    "                fb_txt.append(f\"{k}={'sim' if v else 'não'}\")\n",
    "        subtitle = \"Sem período de baseline localizado. \" + (\", \".join(fb_txt) if fb_txt else \"\")\n",
    "\n",
    "    plt.title(f\"{title}\\n{subtitle}\")\n",
    "    plt.xlabel(\"timestamp\")\n",
    "    plt.ylabel(col_idx)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=120)\n",
    "    plt.close()\n",
    "    print(\"[OK] Gráfico salvo em:\", out_png)\n",
    "\n",
    "# 5) Gerar gráficos\n",
    "plot_idx(df, \"Wr_idx\", os.path.join(OUT_DIR, \"wr_idx_timeline.png\"), \"Wr_idx ao longo do tempo\")\n",
    "plot_idx(df, \"Wm_idx\", os.path.join(OUT_DIR, \"wm_idx_timeline.png\"), \"Wm_idx ao longo do tempo\")\n",
    "\n",
    "# 6) Feedback em texto (terminal)\n",
    "if baseline_range is not None:\n",
    "    print(f\"[INFO] Período de baseline usado para sombreamento: {baseline_range[0]} → {baseline_range[1]}\")\n",
    "else:\n",
    "    print(\"[INFO] Não foi encontrado período explícito de baseline nos artefatos padrão.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6afd0154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Gráfico salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_idx_timeline_v2.png\n",
      "[OK] Gráfico salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wm_idx_timeline_v2.png\n",
      "\n",
      "[OK] v2 gerada:\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_com_desgaste_v2.parquet\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_com_desgaste_v2.csv\n",
      "Baseline representativo: 2023-12-02 10:00:00\n",
      "Wr_ref: 147.92725653180094   Wm_ref: 7204476.667854408\n",
      "% NaN Wr_idx: 36.437866802755806  | % NaN Wm_idx: 36.437866802755806\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PHYSICS v2 — Referência pelos mínimos absolutos + |Δ| e ε\n",
    "# ============================================================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "IN_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_padrao_dedup.parquet\")\n",
    "\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"data\\derivadas\")\n",
    "DIAG_DIR= os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(DIAG_DIR, exist_ok=True)\n",
    "\n",
    "OUT_PQ  = os.path.join(OUT_DIR, \"tabela_wr_wm_com_desgaste_v2.parquet\")\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"tabela_wr_wm_com_desgaste_v2.csv\")\n",
    "OUT_LOG = os.path.join(DIAG_DIR, \"physics_params_mvp_v2.json\")\n",
    "\n",
    "# ---------- 1) Ler base e preparar variáveis primárias ----------\n",
    "df = pd.read_parquet(IN_PQ).copy()\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "# Variáveis primárias\n",
    "v_col = \"total_air_flow_knm3_h\" if \"total_air_flow_knm3_h\" in df.columns else (\n",
    "        \"total_paf_air_flow_knm3_h\" if \"total_paf_air_flow_knm3_h\" in df.columns else None)\n",
    "\n",
    "if v_col is None or \"delta_proxy\" not in df.columns:\n",
    "    raise RuntimeError(\"Faltam variáveis primárias: não encontrei vazão de ar ('total_air_flow_knm3_h' ou 'total_paf_air_flow_knm3_h') e/ou 'delta_proxy'.\")\n",
    "\n",
    "# numéricos\n",
    "for c in [v_col, \"delta_proxy\", \"tau_densa\", \"tau_diluida\", \"tau_backpass\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# temperatura média (°C) e Kelvin\n",
    "taus = [c for c in [\"tau_densa\",\"tau_diluida\",\"tau_backpass\"] if c in df.columns]\n",
    "df[\"tau_mean_C\"] = pd.concat([df[t] for t in taus], axis=1).mean(axis=1, skipna=True)\n",
    "df[\"tau_K\"] = df[\"tau_mean_C\"] + 273.15\n",
    "\n",
    "# proxies ajustados (magnitude + epsilon)\n",
    "eps = 1e-9\n",
    "df[\"v_mag\"]     = np.maximum(np.abs(df[v_col].values), eps)\n",
    "df[\"delta_mag\"] = np.maximum(np.abs(df[\"delta_proxy\"].values), eps)\n",
    "\n",
    "# ---------- 2) Parâmetros do modelo (iguais ao MVP, só trocamos Δ→|Δ|) ----------\n",
    "params = dict(\n",
    "    alpha=1.0, beta=1.0,\n",
    "    n_r=2.0, m_r=1.0,     # Wr ∝ v^2 · Δ^1\n",
    "    n_m=3.0, m_m=0.5,     # Wm ∝ v^3 · Δ^0.5\n",
    "    Tcrit=1200.0, Topt=1250.0,\n",
    "    eps=eps, v_col=v_col, use_delta=\"|delta_proxy|\"\n",
    ")\n",
    "\n",
    "# ---------- 3) Wr* e Wm* (apenas onde tudo é finito) ----------\n",
    "valid = np.isfinite(df[\"v_mag\"]) & np.isfinite(df[\"delta_mag\"]) & np.isfinite(df[\"tau_K\"])\n",
    "Wr_star = np.full(len(df), np.nan, dtype=\"float64\")\n",
    "Wm_star = np.full(len(df), np.nan, dtype=\"float64\")\n",
    "\n",
    "idx = np.where(valid)[0]\n",
    "if len(idx) == 0:\n",
    "    raise RuntimeError(\"Nenhuma linha válida para cálculo após aplicar |Δ| e τ_K.\")\n",
    "\n",
    "v  = df.loc[valid, \"v_mag\"].values\n",
    "d  = df.loc[valid, \"delta_mag\"].values\n",
    "tK = df.loc[valid, \"tau_K\"].values\n",
    "\n",
    "Wr_star_valid = params[\"alpha\"] * np.power(v, params[\"n_r\"]) * np.power(d, params[\"m_r\"]) * np.exp(-tK/params[\"Tcrit\"])\n",
    "Wm_star_valid = params[\"beta\"]  * np.power(v, params[\"n_m\"]) * np.power(d, params[\"m_m\"]) * np.exp(-tK/params[\"Topt\"])\n",
    "\n",
    "Wr_star[idx] = Wr_star_valid\n",
    "Wm_star[idx] = Wm_star_valid\n",
    "\n",
    "# ---------- 4) Referências: mínimos absolutos por componente ----------\n",
    "Wr_ref = np.nanmin(Wr_star)\n",
    "Wm_ref = np.nanmin(Wm_star)\n",
    "\n",
    "# Garantia de positividade\n",
    "if not np.isfinite(Wr_ref) or Wr_ref <= 0 or not np.isfinite(Wm_ref) or Wm_ref <= 0:\n",
    "    raise RuntimeError(\"Referências inválidas (Wr_ref/Wm_ref não finitas ou ≤ 0). Verifique dados.\")\n",
    "\n",
    "# ---------- 5) Timestamp representativo para marcar no gráfico ----------\n",
    "# critério: ponto válido que minimiza a distância aos mínimos normalizados\n",
    "cand = df.loc[valid, [\"timestamp\"]].copy()\n",
    "cand[\"Wr_star\"] = Wr_star_valid\n",
    "cand[\"Wm_star\"] = Wm_star_valid\n",
    "cand[\"dist2\"] = (cand[\"Wr_star\"]/Wr_ref - 1.0)**2 + (cand[\"Wm_star\"]/Wm_ref - 1.0)**2\n",
    "cand = cand.sort_values([\"dist2\",\"timestamp\"])\n",
    "t_ref_repr = cand.iloc[0][\"timestamp\"]\n",
    "\n",
    "# ---------- 6) Índices normalizados (>=1 por construção) ----------\n",
    "Wr_idx = Wr_star / Wr_ref\n",
    "Wm_idx = Wm_star / Wm_ref\n",
    "\n",
    "df_out = df.copy()\n",
    "df_out[\"Wr\"] = Wr_star\n",
    "df_out[\"Wm\"] = Wm_star\n",
    "df_out[\"Wr_ref\"] = Wr_ref\n",
    "df_out[\"Wm_ref\"] = Wm_ref\n",
    "df_out[\"Wr_idx\"] = Wr_idx\n",
    "df_out[\"Wm_idx\"] = Wm_idx\n",
    "\n",
    "# ---------- 7) Salvar saídas v2 ----------\n",
    "df_out.to_parquet(OUT_PQ, index=False)\n",
    "df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------- 8) Gráficos com baseline marcado (linha/intervalo estreito) ----------\n",
    "# janela para sombreamento: usa passo de tempo mediano\n",
    "df_t = df_out.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
    "if len(df_t) >= 2:\n",
    "    dt = df_t[\"timestamp\"].diff().median()\n",
    "    if not pd.isna(dt) and dt > pd.Timedelta(0):\n",
    "        t0 = t_ref_repr - dt/2\n",
    "        t1 = t_ref_repr + dt/2\n",
    "    else:\n",
    "        t0 = t1 = t_ref_repr\n",
    "else:\n",
    "    t0 = t1 = t_ref_repr\n",
    "\n",
    "def plot_idx(series_col, title, png_name):\n",
    "    s = df_out[[\"timestamp\", series_col]].dropna().sort_values(\"timestamp\")\n",
    "    if s.empty:\n",
    "        print(f\"[INFO] Série vazia para {series_col}.\")\n",
    "        return\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(s[\"timestamp\"], s[series_col])\n",
    "    plt.axhline(1.0, linestyle=\"--\")\n",
    "    if t0 == t1:\n",
    "        plt.axvline(t_ref_repr, linestyle=\":\")\n",
    "        subtitle = f\"Baseline = {t_ref_repr}\"\n",
    "    else:\n",
    "        plt.axvspan(t0, t1, alpha=0.15)\n",
    "        subtitle = f\"Baseline ≈ {t_ref_repr} (faixa ~1 passo)\"\n",
    "    plt.title(f\"{title}\\n{subtitle}\")\n",
    "    plt.xlabel(\"timestamp\"); plt.ylabel(series_col)\n",
    "    plt.tight_layout()\n",
    "    out_png = os.path.join(DIAG_DIR, png_name)\n",
    "    plt.savefig(out_png, dpi=120); plt.close()\n",
    "    print(\"[OK] Gráfico salvo em:\", out_png)\n",
    "\n",
    "plot_idx(\"Wr_idx\", \"Wr_idx ao longo do tempo (v2)\", \"wr_idx_timeline_v2.png\")\n",
    "plot_idx(\"Wm_idx\", \"Wm_idx ao longo do tempo (v2)\", \"wm_idx_timeline_v2.png\")\n",
    "\n",
    "# ---------- 9) Log ----------\n",
    "log = {\n",
    "    \"params\": params,\n",
    "    \"t_ref_repr\": str(t_ref_repr),\n",
    "    \"Wr_ref\": float(Wr_ref),\n",
    "    \"Wm_ref\": float(Wm_ref),\n",
    "    \"percent_nan_Wr_idx\": float(np.mean(~np.isfinite(Wr_idx)) * 100.0),\n",
    "    \"percent_nan_Wm_idx\": float(np.mean(~np.isfinite(Wm_idx)) * 100.0),\n",
    "    \"valid_rows_used\": int(valid.sum()),\n",
    "    \"total_rows\": int(len(df)),\n",
    "}\n",
    "with open(OUT_LOG, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(log, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n[OK] v2 gerada:\")\n",
    "print(\" -\", OUT_PQ)\n",
    "print(\" -\", OUT_CSV)\n",
    "print(\"Baseline representativo:\", t_ref_repr)\n",
    "print(\"Wr_ref:\", Wr_ref, \"  Wm_ref:\", Wm_ref)\n",
    "print(\"% NaN Wr_idx:\", log[\"percent_nan_Wr_idx\"], \" | % NaN Wm_idx:\", log[\"percent_nan_Wm_idx\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9de249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Gráfico salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_idx_together_v2.png\n",
      "[OK] Gráfico salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_idx_together_v2_zoom_baseline.png\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# WR_IDX & WM_IDX JUNTOS + BASELINE (AZUL)  |  VERSÃO COM ZOOM\n",
    "# ============================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "IN_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste_v2.parquet\")\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Carrega série v2\n",
    "df = pd.read_parquet(IN_PQ)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"timestamp\")\n",
    "\n",
    "# 2) Descobrir baseline \"anterior\" nos artefatos antigos e escolher o de MAIOR duração\n",
    "candidates = []\n",
    "bp = os.path.join(SRC_BASE, r\"outputs\\baseline_datasets\\physics_baseline_proxies.csv\")\n",
    "if os.path.exists(bp):\n",
    "    try:\n",
    "        bdf = pd.read_csv(bp, encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        bdf = pd.read_csv(bp, encoding=\"latin1\")\n",
    "    if \"timestamp\" in bdf.columns:\n",
    "        bdf[\"timestamp\"] = pd.to_datetime(bdf[\"timestamp\"], errors=\"coerce\")\n",
    "        bdf = bdf.dropna(subset=[\"timestamp\"])\n",
    "        if not bdf.empty:\n",
    "            candidates.append((\"physics_baseline_proxies.csv\", bdf[\"timestamp\"].min(), bdf[\"timestamp\"].max()))\n",
    "\n",
    "bm = os.path.join(SRC_BASE, r\"outputs\\baseline_mask.csv\")\n",
    "if os.path.exists(bm):\n",
    "    try:\n",
    "        mdf = pd.read_csv(bm, encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        mdf = pd.read_csv(bm, encoding=\"latin1\")\n",
    "    if {\"timestamp\",\"is_baseline\"}.issubset(mdf.columns):\n",
    "        mdf[\"timestamp\"] = pd.to_datetime(mdf[\"timestamp\"], errors=\"coerce\")\n",
    "        mdf = mdf[mdf[\"is_baseline\"]==1].dropna(subset=[\"timestamp\"])\n",
    "        if not mdf.empty:\n",
    "            candidates.append((\"baseline_mask.csv\", mdf[\"timestamp\"].min(), mdf[\"timestamp\"].max()))\n",
    "\n",
    "baseline_range = None\n",
    "baseline_source = None\n",
    "if candidates:\n",
    "    # escolhe o com maior duração (proxy de \"maior estabilidade encontrada\")\n",
    "    spans = [(src, t0, t1, (t1 - t0)) for (src, t0, t1) in candidates if pd.notna(t0) and pd.notna(t1)]\n",
    "    if spans:\n",
    "        spans.sort(key=lambda x: x[3], reverse=True)\n",
    "        baseline_source, t0, t1, _ = spans[0]\n",
    "        baseline_range = (t0, t1)\n",
    "\n",
    "# 3) Função de plot geral\n",
    "def plot_both(df, xcol, y1, y2, title, subtitle, path_png, xlim=None, shade=None):\n",
    "    s = df[[xcol, y1, y2]].dropna(subset=[xcol]).copy()\n",
    "    plt.figure(figsize=(14,4.5))\n",
    "    plt.plot(s[xcol], s[y1], label=y1)\n",
    "    plt.plot(s[xcol], s[y2], label=y2)\n",
    "    plt.axhline(1.0, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "    if shade is not None:\n",
    "        t0, t1 = shade\n",
    "        plt.axvspan(t0, t1, color=\"blue\", alpha=0.12)\n",
    "\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim)\n",
    "\n",
    "    ttl = title\n",
    "    if subtitle:\n",
    "        ttl += f\"\\n{subtitle}\"\n",
    "    plt.title(ttl)\n",
    "    plt.xlabel(xcol)\n",
    "    plt.ylabel(\"índice (≥ 1 no v2)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path_png, dpi=130)\n",
    "    plt.close()\n",
    "    print(\"[OK] Gráfico salvo:\", path_png)\n",
    "\n",
    "# 4) Gráfico 1 — todo o período\n",
    "subtitle_all = \"\"\n",
    "shade = None\n",
    "if baseline_range is not None:\n",
    "    subtitle_all = f\"Baseline anterior sombreado (fonte: {baseline_source})\"\n",
    "    shade = baseline_range\n",
    "else:\n",
    "    subtitle_all = \"Baseline anterior não localizado nos artefatos antigos.\"\n",
    "\n",
    "png1 = os.path.join(OUT_DIR, \"wr_wm_idx_together_v2.png\")\n",
    "plot_both(df, \"timestamp\", \"Wr_idx\", \"Wm_idx\",\n",
    "          \"Wr_idx e Wm_idx ao longo do tempo (v2)\",\n",
    "          subtitle_all, png1, shade=shade)\n",
    "\n",
    "# 5) Gráfico 2 — zoom no intervalo de baseline (se existir)\n",
    "if baseline_range is not None:\n",
    "    t0, t1 = baseline_range\n",
    "    # pequena margem de 2% do intervalo para visualização\n",
    "    pad = (t1 - t0) * 0.02\n",
    "    xlim = (t0 - pad, t1 + pad)\n",
    "    png2 = os.path.join(OUT_DIR, \"wr_wm_idx_together_v2_zoom_baseline.png\")\n",
    "    plot_both(df, \"timestamp\", \"Wr_idx\", \"Wm_idx\",\n",
    "              \"Wr_idx e Wm_idx — ZOOM no baseline (v2)\",\n",
    "              f\"Período: {t0} → {t1}\", png2, xlim=xlim, shade=baseline_range)\n",
    "else:\n",
    "    print(\"[INFO] Sem intervalo de baseline anterior — gráfico de zoom não gerado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d73e882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Gráfico de quadrantes salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_quadrantes_ref_anterior.png\n",
      "Contagens por quadrante:\n",
      " - Q1 (Wr↑, Wm↑) : 5812 (77.8%)\n",
      " - Q2 (Wr↓, Wm↑) : 114 (1.5%)\n",
      " - Q3 (Wr↓, Wm↓) : 1438 (19.2%)\n",
      " - Q4 (Wr↑, Wm↓) : 109 (1.5%)\n",
      "Referências (anteriores): Wr_ref_old = 166056.27285204153 | Wm_ref_old = 248106045.49944544\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SCATTER EM QUADRANTES: X=Wr (v2)  |  Y=Wm (v2)\n",
    "# EIXOS CRUZANDO EM (Wr_ref_old, Wm_ref_old)  —  REF ANTERIOR\n",
    "# ============================================================\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"\n",
    "\n",
    "# Arquivos\n",
    "V2_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste_v2.parquet\")\n",
    "V1_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste.parquet\")  # fallback\n",
    "V1_LOG  = os.path.join(DST_BASE, r\"outputs\\diagnosticos\\physics_params_mvp.json\")      # principal\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PNG = os.path.join(OUT_DIR, \"wr_wm_quadrantes_ref_anterior.png\")\n",
    "\n",
    "# 1) Ler dados v2 (pontos a plotar)\n",
    "df = pd.read_parquet(V2_PQ)\n",
    "df[\"Wr\"] = pd.to_numeric(df[\"Wr\"], errors=\"coerce\")\n",
    "df[\"Wm\"] = pd.to_numeric(df[\"Wm\"], errors=\"coerce\")\n",
    "df_plot = df[[\"Wr\",\"Wm\"]].dropna().copy()\n",
    "\n",
    "# 2) Buscar referências ANTERIORES (v1) — preferir LOG; senão, parquet v1\n",
    "Wr_ref_old = np.nan\n",
    "Wm_ref_old = np.nan\n",
    "\n",
    "if os.path.exists(V1_LOG):\n",
    "    try:\n",
    "        with open(V1_LOG, \"r\", encoding=\"utf-8\") as f:\n",
    "            log1 = json.load(f)\n",
    "        Wr_ref_old = float(log1.get(\"Wr_ref\", np.nan))\n",
    "        Wm_ref_old = float(log1.get(\"Wm_ref\", np.nan))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not np.isfinite(Wr_ref_old) or not np.isfinite(Wm_ref_old):\n",
    "    if os.path.exists(V1_PQ):\n",
    "        try:\n",
    "            df_v1 = pd.read_parquet(V1_PQ)\n",
    "            if \"Wr_ref\" in df_v1.columns and \"Wm_ref\" in df_v1.columns:\n",
    "                # pega primeiro valor não nulo de cada\n",
    "                Wr_ref_old = pd.to_numeric(df_v1[\"Wr_ref\"], errors=\"coerce\").dropna().iloc[0]\n",
    "                Wm_ref_old = pd.to_numeric(df_v1[\"Wm_ref\"], errors=\"coerce\").dropna().iloc[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Último fallback: abortar se não achou ref anterior\n",
    "if not np.isfinite(Wr_ref_old) or not np.isfinite(Wm_ref_old):\n",
    "    raise RuntimeError(\"Não foi possível localizar os valores de referência ANTERIORES (v1). Verifique o log JSON ou o parquet v1.\")\n",
    "\n",
    "# 3) Contagens por quadrante em relação à (Wr_ref_old, Wm_ref_old)\n",
    "q1 = (df_plot[\"Wr\"] >= Wr_ref_old) & (df_plot[\"Wm\"] >= Wm_ref_old)  # alta/alta\n",
    "q2 = (df_plot[\"Wr\"] <  Wr_ref_old) & (df_plot[\"Wm\"] >= Wm_ref_old)  # baixa/alta\n",
    "q3 = (df_plot[\"Wr\"] <  Wr_ref_old) & (df_plot[\"Wm\"] <  Wm_ref_old)  # baixa/baixa\n",
    "q4 = (df_plot[\"Wr\"] >= Wr_ref_old) & (df_plot[\"Wm\"] <  Wm_ref_old)  # alta/baixa\n",
    "\n",
    "counts = {\n",
    "    \"Q1 (Wr↑, Wm↑)\": int(q1.sum()),\n",
    "    \"Q2 (Wr↓, Wm↑)\": int(q2.sum()),\n",
    "    \"Q3 (Wr↓, Wm↓)\": int(q3.sum()),\n",
    "    \"Q4 (Wr↑, Wm↓)\": int(q4.sum()),\n",
    "}\n",
    "total = len(df_plot)\n",
    "\n",
    "# 4) Plot\n",
    "plt.figure(figsize=(7.5,7))\n",
    "plt.scatter(df_plot[\"Wr\"], df_plot[\"Wm\"], s=10, alpha=0.35)\n",
    "\n",
    "# Linhas de referência (eixos do “quadro de quadrantes”)\n",
    "plt.axvline(Wr_ref_old, linestyle=\"--\", linewidth=1)\n",
    "plt.axhline(Wm_ref_old, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "plt.xlabel(\"Wr (v2)\")\n",
    "plt.ylabel(\"Wm (v2)\")\n",
    "plt.title(\"Wr (X) vs Wm (Y) — Eixos nos valores de referência ANTERIORES (v1)\")\n",
    "\n",
    "# Anotações dos quadrantes (percentuais)\n",
    "def pct(n): \n",
    "    return f\"{n} ({(100*n/total):.1f}%)\"\n",
    "\n",
    "xmin, xmax = df_plot[\"Wr\"].min(), df_plot[\"Wr\"].max()\n",
    "ymin, ymax = df_plot[\"Wm\"].min(), df_plot[\"Wm\"].max()\n",
    "xpad = (xmax - xmin) * 0.03\n",
    "ypad = (ymax - ymin) * 0.03\n",
    "\n",
    "plt.text(Wr_ref_old + xpad, Wm_ref_old + ypad,     f\"Q1: {pct(counts['Q1 (Wr↑, Wm↑)'])}\")\n",
    "plt.text(xmin + xpad,        Wm_ref_old + ypad,     f\"Q2: {pct(counts['Q2 (Wr↓, Wm↑)'])}\")\n",
    "plt.text(xmin + xpad,        ymin + ypad,           f\"Q3: {pct(counts['Q3 (Wr↓, Wm↓)'])}\")\n",
    "plt.text(Wr_ref_old + xpad,  ymin + ypad,           f\"Q4: {pct(counts['Q4 (Wr↑, Wm↓)'])}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_PNG, dpi=140)\n",
    "plt.close()\n",
    "\n",
    "print(\"[OK] Gráfico de quadrantes salvo em:\", OUT_PNG)\n",
    "print(\"Contagens por quadrante:\")\n",
    "for k, v in counts.items():\n",
    "    print(\" -\", k, \":\", v, f\"({v/total:.1%})\")\n",
    "print(\"Referências (anteriores): Wr_ref_old =\", Wr_ref_old, \"| Wm_ref_old =\", Wm_ref_old)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "350c4370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Gráfico salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_quadrantes_ref_anterior_com_roi.png\n",
      "Referências (v1):  Wr_ref_old = 166056.27285204153  | Wm_ref_old = 248106045.49944544\n",
      "ROI (±%): Wr = 10.0 %  |  Wm = 10.0 %\n",
      "Janela absoluta: Wr ∈ [ 149450.64556683737 , 182661.90013724568 ] ; Wm ∈ [ 223295440.9495009 , 272916650.04939 ]\n",
      "\n",
      "Contagens (pontos FORA por quadrante) e DENTRO da ROI:\n",
      " - ROI (normais) : 319 (4.3%)\n",
      " - Q1 (Wr↑, Wm↑) : 5717 (76.5%)\n",
      " - Q2 (Wr↓, Wm↑) : 55 (0.7%)\n",
      " - Q3 (Wr↓, Wm↓) : 1328 (17.8%)\n",
      " - Q4 (Wr↑, Wm↓) : 54 (0.7%)\n",
      "\n",
      "CSV com pontos fora por quadrante: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_quadrantes_outliers.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# QUADRANTES COM REGIÃO \"NORMAL\" (RETÂNGULO AO REDOR DA REF)\n",
    "# - X = Wr (v2), Y = Wm (v2)\n",
    "# - Eixos cruzando em (Wr_ref_old, Wm_ref_old) = referência ANTERIOR (v1)\n",
    "# - Retângulo de normalidade: ±(pct) em torno de cada referência\n",
    "# - Reconta pontos FORA do retângulo por quadrante\n",
    "# ============================================================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# -------------------- CONFIGURÁVEL --------------------\n",
    "# janela percentual ao redor das referências (ex.: 0.10 = ±10%)\n",
    "R_WR_PCT = 0.10\n",
    "R_WM_PCT = 0.10\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"\n",
    "\n",
    "V2_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste_v2.parquet\")\n",
    "V1_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste.parquet\")  # fallback\n",
    "V1_LOG  = os.path.join(DST_BASE, r\"outputs\\diagnosticos\\physics_params_mvp.json\")      # principal\n",
    "\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PNG = os.path.join(OUT_DIR, \"wr_wm_quadrantes_ref_anterior_com_roi.png\")\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"wr_wm_quadrantes_outliers.csv\")\n",
    "\n",
    "# -------------------- 1) DADOS --------------------\n",
    "df = pd.read_parquet(V2_PQ)\n",
    "df[\"Wr\"] = pd.to_numeric(df[\"Wr\"], errors=\"coerce\")\n",
    "df[\"Wm\"] = pd.to_numeric(df[\"Wm\"], errors=\"coerce\")\n",
    "df_plot = df[[\"Wr\",\"Wm\"]].dropna().copy()\n",
    "\n",
    "# referências ANTERIORES (v1)\n",
    "Wr_ref_old = np.nan\n",
    "Wm_ref_old = np.nan\n",
    "\n",
    "if os.path.exists(V1_LOG):\n",
    "    try:\n",
    "        with open(V1_LOG, \"r\", encoding=\"utf-8\") as f:\n",
    "            log1 = json.load(f)\n",
    "        Wr_ref_old = float(log1.get(\"Wr_ref\", np.nan))\n",
    "        Wm_ref_old = float(log1.get(\"Wm_ref\", np.nan))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if not np.isfinite(Wr_ref_old) or not np.isfinite(Wm_ref_old):\n",
    "    if os.path.exists(V1_PQ):\n",
    "        try:\n",
    "            df_v1 = pd.read_parquet(V1_PQ)\n",
    "            if \"Wr_ref\" in df_v1.columns and \"Wm_ref\" in df_v1.columns:\n",
    "                Wr_ref_old = pd.to_numeric(df_v1[\"Wr_ref\"], errors=\"coerce\").dropna().iloc[0]\n",
    "                Wm_ref_old = pd.to_numeric(df_v1[\"Wm_ref\"], errors=\"coerce\").dropna().iloc[0]\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "if not np.isfinite(Wr_ref_old) or not np.isfinite(Wm_ref_old):\n",
    "    raise RuntimeError(\"Não foi possível localizar Wr_ref_old / Wm_ref_old (v1).\")\n",
    "\n",
    "# -------------------- 2) REGIÃO NORMAL (ROI) --------------------\n",
    "wr_min = Wr_ref_old * (1 - R_WR_PCT)\n",
    "wr_max = Wr_ref_old * (1 + R_WR_PCT)\n",
    "wm_min = Wm_ref_old * (1 - R_WM_PCT)\n",
    "wm_max = Wm_ref_old * (1 + R_WM_PCT)\n",
    "\n",
    "inside_roi = (df_plot[\"Wr\"].between(wr_min, wr_max)) & (df_plot[\"Wm\"].between(wm_min, wm_max))\n",
    "outside_roi = ~inside_roi\n",
    "\n",
    "# -------------------- 3) QUADRANTES (APENAS FORA DA ROI) --------------------\n",
    "q1 = outside_roi & (df_plot[\"Wr\"] >= Wr_ref_old) & (df_plot[\"Wm\"] >= Wm_ref_old)  # alta/alta\n",
    "q2 = outside_roi & (df_plot[\"Wr\"] <  Wr_ref_old) & (df_plot[\"Wm\"] >= Wm_ref_old)  # baixa/alta\n",
    "q3 = outside_roi & (df_plot[\"Wr\"] <  Wr_ref_old) & (df_plot[\"Wm\"] <  Wm_ref_old)  # baixa/baixa\n",
    "q4 = outside_roi & (df_plot[\"Wr\"] >= Wr_ref_old) & (df_plot[\"Wm\"] <  Wm_ref_old)  # alta/baixa\n",
    "\n",
    "counts = {\n",
    "    \"ROI (normais)\": int(inside_roi.sum()),\n",
    "    \"Q1 (Wr↑, Wm↑)\": int(q1.sum()),\n",
    "    \"Q2 (Wr↓, Wm↑)\": int(q2.sum()),\n",
    "    \"Q3 (Wr↓, Wm↓)\": int(q3.sum()),\n",
    "    \"Q4 (Wr↑, Wm↓)\": int(q4.sum()),\n",
    "}\n",
    "total = len(df_plot)\n",
    "\n",
    "# salvar CSV dos pontos fora com rótulo de quadrante\n",
    "outliers = df_plot.loc[outside_roi].copy()\n",
    "quadrant = np.where(q1, \"Q1\",\n",
    "             np.where(q2, \"Q2\",\n",
    "             np.where(q3, \"Q3\",\n",
    "             np.where(q4, \"Q4\", \"NA\"))))\n",
    "outliers[\"quadrante\"] = quadrant[outside_roi.values]\n",
    "outliers.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# -------------------- 4) PLOT --------------------\n",
    "plt.figure(figsize=(8.2,8.2))\n",
    "\n",
    "# pontos fora (azul claro) e dentro da ROI (verde)\n",
    "plt.scatter(df_plot.loc[outside_roi, \"Wr\"], df_plot.loc[outside_roi, \"Wm\"], s=10, alpha=0.35, label=\"Fora da ROI\")\n",
    "plt.scatter(df_plot.loc[inside_roi,  \"Wr\"], df_plot.loc[inside_roi,  \"Wm\"], s=10, alpha=0.65, label=\"Dentro da ROI\")\n",
    "\n",
    "# eixos de referência (anteriores)\n",
    "plt.axvline(Wr_ref_old, linestyle=\"--\", linewidth=1)\n",
    "plt.axhline(Wm_ref_old, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# retângulo da ROI\n",
    "rect = Rectangle((wr_min, wm_min), wr_max - wr_min, wm_max - wm_min,\n",
    "                 fill=False, linestyle=\"-\", linewidth=1.5)\n",
    "plt.gca().add_patch(rect)\n",
    "\n",
    "plt.xlabel(\"Wr (v2)\")\n",
    "plt.ylabel(\"Wm (v2)\")\n",
    "plt.title(\"Wr (X) vs Wm (Y) — Eixos: ref ANTERIOR (v1)  |  ROI = retângulo em torno da ref\")\n",
    "\n",
    "# anotações com contagens (%)\n",
    "def pct(n): return f\"{n} ({(100*n/total):.1f}%)\"\n",
    "xmin, xmax = df_plot[\"Wr\"].min(), df_plot[\"Wr\"].max()\n",
    "ymin, ymax = df_plot[\"Wm\"].min(), df_plot[\"Wm\"].max()\n",
    "xpad = (xmax - xmin) * 0.02\n",
    "ypad = (ymax - ymin) * 0.02\n",
    "\n",
    "plt.text(Wr_ref_old + xpad, Wm_ref_old + ypad,     f\"Q1: {pct(counts['Q1 (Wr↑, Wm↑)'])}\")\n",
    "plt.text(xmin + xpad,        Wm_ref_old + ypad,     f\"Q2: {pct(counts['Q2 (Wr↓, Wm↑)'])}\")\n",
    "plt.text(xmin + xpad,        ymin + ypad,           f\"Q3: {pct(counts['Q3 (Wr↓, Wm↓)'])}\")\n",
    "plt.text(Wr_ref_old + xpad,  ymin + ypad,           f\"Q4: {pct(counts['Q4 (Wr↑, Wm↓)'])}\")\n",
    "plt.text(wr_min + xpad, wm_max + ypad, f\"ROI ±{int(R_WR_PCT*100)}% x ±{int(R_WM_PCT*100)}%\", fontsize=9)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_PNG, dpi=140)\n",
    "plt.close()\n",
    "\n",
    "print(\"[OK] Gráfico salvo:\", OUT_PNG)\n",
    "print(\"Referências (v1):  Wr_ref_old =\", Wr_ref_old, \" | Wm_ref_old =\", Wm_ref_old)\n",
    "print(\"ROI (±%): Wr =\", R_WR_PCT*100, \"%  |  Wm =\", R_WM_PCT*100, \"%\")\n",
    "print(\"Janela absoluta: Wr ∈ [\", wr_min, \",\", wr_max, \"] ; Wm ∈ [\", wm_min, \",\", wm_max, \"]\")\n",
    "print(\"\\nContagens (pontos FORA por quadrante) e DENTRO da ROI:\")\n",
    "for k, v in counts.items():\n",
    "    print(\" -\", k, \":\", v, f\"({v/total:.1%})\")\n",
    "print(\"\\nCSV com pontos fora por quadrante:\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4271162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Gráfico salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_quadrantes_ref_anterior_com_roi_labels_ok.png\n",
      "Contagens:\n",
      " - ROI (normais) : 319 (4.3%)\n",
      " - Q1 (Wr↑, Wm↑) : 5717 (76.5%)\n",
      " - Q2 (Wr↓, Wm↑) : 55 (0.7%)\n",
      " - Q3 (Wr↓, Wm↓) : 1328 (17.8%)\n",
      " - Q4 (Wr↑, Wm↓) : 54 (0.7%)\n",
      "CSV (fora da ROI): C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_quadrantes_outliers.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# AJUSTE DOS RÓTULOS DE QUADRANTES (SEM SOBREPOSIÇÃO)\n",
    "# - Usa posições relativas em cada quadrante (longe da interseção)\n",
    "# - Mantém ROI e contagens como antes\n",
    "# ============================================================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# ---------- Caminhos ----------\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "V2_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste_v2.parquet\")\n",
    "V1_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste.parquet\")\n",
    "V1_LOG  = os.path.join(DST_BASE, r\"outputs\\diagnosticos\\physics_params_mvp.json\")\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PNG = os.path.join(OUT_DIR, \"wr_wm_quadrantes_ref_anterior_com_roi_labels_ok.png\")\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"wr_wm_quadrantes_outliers.csv\")\n",
    "\n",
    "# ---------- Parâmetros ----------\n",
    "R_WR_PCT = 0.10   # ROI ±10% em Wr\n",
    "R_WM_PCT = 0.10   # ROI ±10% em Wm\n",
    "\n",
    "# ---------- Dados ----------\n",
    "df = pd.read_parquet(V2_PQ)\n",
    "df[\"Wr\"] = pd.to_numeric(df[\"Wr\"], errors=\"coerce\")\n",
    "df[\"Wm\"] = pd.to_numeric(df[\"Wm\"], errors=\"coerce\")\n",
    "df_plot = df[[\"Wr\",\"Wm\"]].dropna().copy()\n",
    "\n",
    "# Refs anteriores (v1)\n",
    "Wr_ref_old = np.nan\n",
    "Wm_ref_old = np.nan\n",
    "if os.path.exists(V1_LOG):\n",
    "    try:\n",
    "        with open(V1_LOG, \"r\", encoding=\"utf-8\") as f:\n",
    "            log1 = json.load(f)\n",
    "        Wr_ref_old = float(log1.get(\"Wr_ref\", np.nan))\n",
    "        Wm_ref_old = float(log1.get(\"Wm_ref\", np.nan))\n",
    "    except Exception:\n",
    "        pass\n",
    "if not np.isfinite(Wr_ref_old) or not np.isfinite(Wm_ref_old):\n",
    "    if os.path.exists(V1_PQ):\n",
    "        df_v1 = pd.read_parquet(V1_PQ)\n",
    "        Wr_ref_old = pd.to_numeric(df_v1[\"Wr_ref\"], errors=\"coerce\").dropna().iloc[0]\n",
    "        Wm_ref_old = pd.to_numeric(df_v1[\"Wm_ref\"], errors=\"coerce\").dropna().iloc[0]\n",
    "\n",
    "# ---------- ROI ----------\n",
    "wr_min = Wr_ref_old * (1 - R_WR_PCT)\n",
    "wr_max = Wr_ref_old * (1 + R_WR_PCT)\n",
    "wm_min = Wm_ref_old * (1 - R_WM_PCT)\n",
    "wm_max = Wm_ref_old * (1 + R_WM_PCT)\n",
    "\n",
    "inside_roi  = df_plot[\"Wr\"].between(wr_min, wr_max) & df_plot[\"Wm\"].between(wm_min, wm_max)\n",
    "outside_roi = ~inside_roi\n",
    "\n",
    "# Quadrantes (fora da ROI)\n",
    "q1 = outside_roi & (df_plot[\"Wr\"] >= Wr_ref_old) & (df_plot[\"Wm\"] >= Wm_ref_old)\n",
    "q2 = outside_roi & (df_plot[\"Wr\"] <  Wr_ref_old) & (df_plot[\"Wm\"] >= Wm_ref_old)\n",
    "q3 = outside_roi & (df_plot[\"Wr\"] <  Wr_ref_old) & (df_plot[\"Wm\"] <  Wm_ref_old)\n",
    "q4 = outside_roi & (df_plot[\"Wr\"] >= Wr_ref_old) & (df_plot[\"Wm\"] <  Wm_ref_old)\n",
    "\n",
    "counts = {\n",
    "    \"ROI (normais)\": int(inside_roi.sum()),\n",
    "    \"Q1 (Wr↑, Wm↑)\": int(q1.sum()),\n",
    "    \"Q2 (Wr↓, Wm↑)\": int(q2.sum()),\n",
    "    \"Q3 (Wr↓, Wm↓)\": int(q3.sum()),\n",
    "    \"Q4 (Wr↑, Wm↓)\": int(q4.sum()),\n",
    "}\n",
    "total = len(df_plot)\n",
    "\n",
    "# Salva CSV dos pontos fora por quadrante\n",
    "outliers = df_plot.loc[outside_roi].copy()\n",
    "quadrant = np.where(q1, \"Q1\", np.where(q2, \"Q2\", np.where(q3, \"Q3\", np.where(q4, \"Q4\", \"NA\"))))\n",
    "outliers[\"quadrante\"] = quadrant[outside_roi.values]\n",
    "outliers.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# ---------- Plot ----------\n",
    "plt.figure(figsize=(8.2,8.2))\n",
    "plt.scatter(df_plot.loc[outside_roi,\"Wr\"], df_plot.loc[outside_roi,\"Wm\"], s=10, alpha=0.35, label=\"Fora da ROI\")\n",
    "plt.scatter(df_plot.loc[inside_roi, \"Wr\"], df_plot.loc[inside_roi, \"Wm\"], s=10, alpha=0.65, label=\"Dentro da ROI\")\n",
    "\n",
    "# Eixos de referência\n",
    "plt.axvline(Wr_ref_old, linestyle=\"--\", linewidth=1)\n",
    "plt.axhline(Wm_ref_old, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# Retângulo ROI\n",
    "rect = Rectangle((wr_min, wm_min), wr_max-wr_min, wm_max-wm_min, fill=False, linestyle=\"-\", linewidth=1.5)\n",
    "plt.gca().add_patch(rect)\n",
    "\n",
    "plt.xlabel(\"Wr (v2)\")\n",
    "plt.ylabel(\"Wm (v2)\")\n",
    "plt.title(\"Wr (X) vs Wm (Y) — ROI e quadrantes com rótulos reposicionados\")\n",
    "\n",
    "# ------ Rótulos sem sobreposição ------\n",
    "xmin, xmax = df_plot[\"Wr\"].min(), df_plot[\"Wr\"].max()\n",
    "ymin, ymax = df_plot[\"Wm\"].min(), df_plot[\"Wm\"].max()\n",
    "\n",
    "# posições relativas (entra 15%–35% em cada quadrante, longe da interseção/ROI)\n",
    "x_q1 = Wr_ref_old + 0.18*(xmax - Wr_ref_old)\n",
    "y_q1 = Wm_ref_old + 0.18*(ymax - Wm_ref_old)\n",
    "\n",
    "x_q2 = Wr_ref_old - 0.30*(Wr_ref_old - xmin)\n",
    "y_q2 = Wm_ref_old + 0.18*(ymax - Wm_ref_old)\n",
    "\n",
    "x_q3 = Wr_ref_old - 0.30*(Wr_ref_old - xmin)\n",
    "y_q3 = Wm_ref_old - 0.30*(Wm_ref_old - ymin)\n",
    "\n",
    "x_q4 = Wr_ref_old + 0.18*(xmax - Wr_ref_old)\n",
    "y_q4 = Wm_ref_old - 0.30*(Wm_ref_old - ymin)\n",
    "\n",
    "box = dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"gray\", alpha=0.85)\n",
    "fmt = lambda n: f\"{n} ({(100*n/total):.1f}%)\"\n",
    "\n",
    "plt.text(x_q1, y_q1, f\"Q1: {fmt(counts['Q1 (Wr↑, Wm↑)'])}\", ha=\"left\",  va=\"bottom\", bbox=box)\n",
    "plt.text(x_q2, y_q2, f\"Q2: {fmt(counts['Q2 (Wr↓, Wm↑)'])}\", ha=\"right\", va=\"bottom\", bbox=box)\n",
    "plt.text(x_q3, y_q3, f\"Q3: {fmt(counts['Q3 (Wr↓, Wm↓)'])}\", ha=\"right\", va=\"top\",    bbox=box)\n",
    "plt.text(x_q4, y_q4, f\"Q4: {fmt(counts['Q4 (Wr↑, Wm↓)'])}\", ha=\"left\",  va=\"top\",    bbox=box)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_PNG, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"[OK] Gráfico salvo:\", OUT_PNG)\n",
    "print(\"Contagens:\")\n",
    "for k, v in counts.items():\n",
    "    print(\" -\", k, \":\", v, f\"({v/total:.1%})\")\n",
    "print(\"CSV (fora da ROI):\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "992eea0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1_n                       3175.000000\n",
      "peso_medio_v                  0.077052\n",
      "peso_medio_|Delta|            0.905529\n",
      "peso_medio_temp               0.000613\n",
      "%Q1_onde_v_domina             1.763780\n",
      "%Q1_onde_|Delta|_domina      98.236220\n",
      "%Q1_onde_temp_domina          0.000000\n",
      "[OK] Gráfico salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_roi_estatistica_quadrantes.png\n",
      "Contagens (fora da ROI):\n",
      " - ROI Elipse χ² (p=95%) (normais) : 4037 (54.0%)\n",
      " - Q1 (Wr↑, Wm↑) : 3175 (42.5%)\n",
      " - Q2 (Wr↓, Wm↑) : 8 (0.1%)\n",
      " - Q3 (Wr↓, Wm↓) : 230 (3.1%)\n",
      " - Q4 (Wr↑, Wm↓) : 23 (0.3%)\n",
      "\n",
      "Atribuição de drivers em Q1 (arquivo): C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\drivers_q1.csv\n",
      "Q1_n                       3175.000000\n",
      "peso_medio_v                  0.077052\n",
      "peso_medio_|Delta|            0.905529\n",
      "peso_medio_temp               0.000613\n",
      "%Q1_onde_v_domina             1.763780\n",
      "%Q1_onde_|Delta|_domina      98.236220\n",
      "%Q1_onde_temp_domina          0.000000\n",
      "\n",
      "ROI usada: Elipse χ² (p=95%)\n",
      "Estatísticas estimadas no baseline: 2024-03-04 20:00:00 → 2024-03-12 15:00:00\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ROI ESTATÍSTICA + QUADRANTES + ATRIBUIÇÃO DE DRIVERS (Q1)\n",
    "# - Retângulo ±k·σ (por eixo) e Elipse de Mahalanobis (χ²)\n",
    "# - Contagem por quadrante (fora da ROI escolhida)\n",
    "# - Decomposição de drivers (v, |Δ|, τ) para Q1\n",
    "# ============================================================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle, Ellipse\n",
    "\n",
    "# -------------------- CONFIG --------------------\n",
    "K_SIGMA   = 1.5       # retângulo ±k·σ (ajuste: 1.0, 1.5, 2.0...)\n",
    "CHI2_P    = 0.95      # elipse p-valor (0.90, 0.95, 0.99)\n",
    "USE_ELLIPSE = True    # True = usa elipse p/ \"normalidade\"; False = usa retângulo k·σ\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"\n",
    "\n",
    "V2_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste_v2.parquet\")\n",
    "V1_PQ   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste.parquet\")\n",
    "V1_LOG  = os.path.join(DST_BASE, r\"outputs\\diagnosticos\\physics_params_mvp.json\")\n",
    "BASELINE_PROXIES = os.path.join(SRC_BASE, r\"outputs\\baseline_datasets\\physics_baseline_proxies.csv\")\n",
    "BASELINE_MASK    = os.path.join(SRC_BASE, r\"outputs\\baseline_mask.csv\")\n",
    "\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "OUT_PNG = os.path.join(OUT_DIR, \"wr_wm_roi_estatistica_quadrantes.png\")\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"wr_wm_outliers_por_quadrante.csv\")\n",
    "OUT_DRV = os.path.join(OUT_DIR, \"drivers_q1.csv\")\n",
    "\n",
    "# -------------------- 1) CARREGAR DADOS --------------------\n",
    "df = pd.read_parquet(V2_PQ)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "for c in [\"Wr\",\"Wm\",\"delta_proxy\",\"total_air_flow_knm3_h\",\"total_paf_air_flow_knm3_h\",\"tau_densa\",\"tau_diluida\",\"tau_backpass\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Referências ANTERIORES (v1)\n",
    "Wr_ref_old = np.nan; Wm_ref_old = np.nan\n",
    "v_ref_old = np.nan; delta_ref_old = np.nan; tau_refC_old = np.nan\n",
    "if os.path.exists(V1_LOG):\n",
    "    with open(V1_LOG, \"r\", encoding=\"utf-8\") as f:\n",
    "        log1 = json.load(f)\n",
    "    Wr_ref_old = float(log1.get(\"Wr_ref\", np.nan))\n",
    "    Wm_ref_old = float(log1.get(\"Wm_ref\", np.nan))\n",
    "    refs_u = log1.get(\"refs_utilizadas\", {})\n",
    "    # v_ref_old (preferir total; senão primary)\n",
    "    v_ref_old = refs_u.get(\"v_proxy_total_ref\", refs_u.get(\"v_proxy_primary_ref\", np.nan))\n",
    "    delta_ref_old = refs_u.get(\"delta_proxy_ref\", np.nan)\n",
    "    tau_refC_old = np.nanmean([refs_u.get(\"tau_densa_ref\", np.nan),\n",
    "                               refs_u.get(\"tau_diluida_ref\", np.nan),\n",
    "                               refs_u.get(\"tau_backpass_ref\", np.nan)])\n",
    "# fallback às colunas do parquet v1 (se necessário)\n",
    "if (not np.isfinite(Wr_ref_old)) or (not np.isfinite(Wm_ref_old)):\n",
    "    if os.path.exists(V1_PQ):\n",
    "        df1 = pd.read_parquet(V1_PQ)\n",
    "        Wr_ref_old = pd.to_numeric(df1[\"Wr_ref\"], errors=\"coerce\").dropna().iloc[0]\n",
    "        Wm_ref_old = pd.to_numeric(df1[\"Wm_ref\"], errors=\"coerce\").dropna().iloc[0]\n",
    "\n",
    "# Checagem\n",
    "if not (np.isfinite(Wr_ref_old) and np.isfinite(Wm_ref_old)):\n",
    "    raise RuntimeError(\"Não foi possível recuperar Wr_ref_old/Wm_ref_old.\")\n",
    "\n",
    "# Wr/Wm válidos p/ scatter\n",
    "XY = df[[\"Wr\",\"Wm\",\"timestamp\"]].dropna().copy()\n",
    "\n",
    "# -------------------- 2) DEFINIR BASELINE PARA σ e Σ --------------------\n",
    "baseline_range = None\n",
    "if os.path.exists(BASELINE_PROXIES):\n",
    "    bdf = pd.read_csv(BASELINE_PROXIES, encoding=\"utf-8-sig\")\n",
    "    if \"timestamp\" in bdf.columns:\n",
    "        bdf[\"timestamp\"] = pd.to_datetime(bdf[\"timestamp\"], errors=\"coerce\")\n",
    "        bdf = bdf.dropna(subset=[\"timestamp\"])\n",
    "        if not bdf.empty:\n",
    "            baseline_range = (bdf[\"timestamp\"].min(), bdf[\"timestamp\"].max())\n",
    "\n",
    "if (baseline_range is None) and os.path.exists(BASELINE_MASK):\n",
    "    mdf = pd.read_csv(BASELINE_MASK, encoding=\"utf-8-sig\")\n",
    "    if {\"timestamp\",\"is_baseline\"}.issubset(mdf.columns):\n",
    "        mdf[\"timestamp\"] = pd.to_datetime(mdf[\"timestamp\"], errors=\"coerce\")\n",
    "        mdf = mdf[mdf[\"is_baseline\"]==1].dropna(subset=[\"timestamp\"])\n",
    "        if not mdf.empty:\n",
    "            baseline_range = (mdf[\"timestamp\"].min(), mdf[\"timestamp\"].max())\n",
    "\n",
    "if baseline_range is not None:\n",
    "    t0, t1 = baseline_range\n",
    "    base_df = df[(df[\"timestamp\"]>=t0) & (df[\"timestamp\"]<=t1)][[\"Wr\",\"Wm\"]].dropna().copy()\n",
    "else:\n",
    "    base_df = df[[\"Wr\",\"Wm\"]].dropna().copy()\n",
    "\n",
    "if len(base_df) < 10:\n",
    "    # se baseline muito pequeno, usa conjunto todo\n",
    "    base_df = df[[\"Wr\",\"Wm\"]].dropna().copy()\n",
    "\n",
    "# Estatísticas no baseline\n",
    "mu_wr = base_df[\"Wr\"].mean()\n",
    "mu_wm = base_df[\"Wm\"].mean()\n",
    "sigma_wr = base_df[\"Wr\"].std(ddof=1)\n",
    "sigma_wm = base_df[\"Wm\"].std(ddof=1)\n",
    "cov = np.cov(base_df[\"Wr\"], base_df[\"Wm\"])\n",
    "\n",
    "# -------------------- 3) ROI: retângulo ±k·σ OU elipse χ² --------------------\n",
    "# Centro SEMPRE na ref antiga (pedido do usuário)\n",
    "cx, cy = Wr_ref_old, Wm_ref_old\n",
    "\n",
    "def inside_rectangle(x, y):\n",
    "    return (x >= cx - K_SIGMA*sigma_wr) & (x <= cx + K_SIGMA*sigma_wr) & \\\n",
    "           (y >= cy - K_SIGMA*sigma_wm) & (y <= cy + K_SIGMA*sigma_wm)\n",
    "\n",
    "# Qui-quadrado crítico (2 g.l.)\n",
    "CHI2_TABLE = {0.90:4.605, 0.95:5.991, 0.99:9.210}\n",
    "chi2_thr = CHI2_TABLE.get(CHI2_P, 5.991)\n",
    "\n",
    "def inside_ellipse(x, y):\n",
    "    # distância de Mahalanobis ao centro usando Σ do baseline\n",
    "    diff = np.vstack([x - cx, y - cy])          # shape (2, n)\n",
    "    try:\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # se cov singular, usa diagonal variâncias\n",
    "        inv_cov = np.linalg.inv(np.diag([sigma_wr**2, sigma_wm**2]))\n",
    "    d2 = np.einsum(\"ij,jk,ik->i\", diff.T, inv_cov, diff.T)\n",
    "    return d2 <= chi2_thr\n",
    "\n",
    "# máscara de pontos \"normais\"\n",
    "if USE_ELLIPSE:\n",
    "    inside = inside_ellipse(XY[\"Wr\"].values, XY[\"Wm\"].values)\n",
    "    roi_label = f\"Elipse χ² (p={int(CHI2_P*100)}%)\"\n",
    "else:\n",
    "    inside = inside_rectangle(XY[\"Wr\"].values, XY[\"Wm\"].values)\n",
    "    roi_label = f\"Retângulo ±{K_SIGMA}σ\"\n",
    "\n",
    "outside = ~inside\n",
    "\n",
    "# -------------------- 4) QUADRANTES (fora da ROI) --------------------\n",
    "x = XY[\"Wr\"].values; y = XY[\"Wm\"].values\n",
    "q1 = outside & (x >= cx) & (y >= cy)\n",
    "q2 = outside & (x <  cx) & (y >= cy)\n",
    "q3 = outside & (x <  cx) & (y <  cy)\n",
    "q4 = outside & (x >= cx) & (y <  cy)\n",
    "\n",
    "counts = {\n",
    "    f\"ROI {roi_label} (normais)\": int(inside.sum()),\n",
    "    \"Q1 (Wr↑, Wm↑)\": int(q1.sum()),\n",
    "    \"Q2 (Wr↓, Wm↑)\": int(q2.sum()),\n",
    "    \"Q3 (Wr↓, Wm↓)\": int(q3.sum()),\n",
    "    \"Q4 (Wr↑, Wm↓)\": int(q4.sum()),\n",
    "}\n",
    "total = len(XY)\n",
    "\n",
    "# Salva CSV dos fora da ROI com quadrante\n",
    "outliers = XY.loc[outside].copy()\n",
    "quadrant = np.where(q1, \"Q1\", np.where(q2, \"Q2\", np.where(q3, \"Q3\", np.where(q4, \"Q4\",\"NA\"))))\n",
    "outliers[\"quadrante\"] = quadrant[outside]\n",
    "outliers.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# -------------------- 5) ATRIBUIÇÃO DE DRIVERS (apenas Q1) --------------------\n",
    "# Usamos a forma log-linear do MVP:\n",
    "# Δlog Wr = 2·Δlog v + 1·Δlog |Δ| - Δτ/Tcrit\n",
    "# Δlog Wm = 3·Δlog v + 0.5·Δlog |Δ| - Δτ/Topt\n",
    "# Referências (v1) para comparar: v_ref_old, delta_ref_old, tau_refC_old\n",
    "# Observação: usamos |Δ| (como no v2) e τK = τC + 273.15\n",
    "\n",
    "# Preparar variáveis primárias\n",
    "v_col = \"total_air_flow_knm3_h\" if \"total_air_flow_knm3_h\" in df.columns else \"total_paf_air_flow_knm3_h\"\n",
    "v = pd.to_numeric(df[v_col], errors=\"coerce\")\n",
    "delta_mag = np.maximum(np.abs(pd.to_numeric(df[\"delta_proxy\"], errors=\"coerce\")), 1e-9)\n",
    "tauC = pd.concat([df[c] for c in [\"tau_densa\",\"tau_diluida\",\"tau_backpass\"] if c in df.columns], axis=1).mean(axis=1, skipna=True)\n",
    "tauK = tauC + 273.15\n",
    "\n",
    "# refs antigas (módulo em Δ e Kelvin em τ)\n",
    "if not np.isfinite(v_ref_old):\n",
    "    v_ref_old = np.nanmedian(v)\n",
    "if not np.isfinite(delta_ref_old):\n",
    "    delta_ref_old = np.nanmedian(delta_mag)\n",
    "tau_refK_old = (tau_refC_old if np.isfinite(tau_refC_old) else np.nanmedian(tauC)) + 273.15\n",
    "\n",
    "# Log-diferenças e termos\n",
    "dlogv = np.log(v / v_ref_old)\n",
    "dlogd = np.log(delta_mag / np.abs(delta_ref_old))\n",
    "dtaur = -(tauK - tau_refK_old) / 1200.0\n",
    "dtaum = -(tauK - tau_refK_old) / 1250.0\n",
    "\n",
    "dlogWr = 2.0*dlogv + 1.0*dlogd + dtaur\n",
    "dlogWm = 3.0*dlogv + 0.5*dlogd + dtaum\n",
    "\n",
    "# Seleciona as mesmas linhas do scatter OUTSIDE & Q1\n",
    "mask_q1_idx = XY.index[outside & (x >= cx) & (y >= cy)]\n",
    "drv = pd.DataFrame({\n",
    "    \"timestamp\": df.loc[mask_q1_idx, \"timestamp\"],\n",
    "    \"dlogv\": dlogv.loc[mask_q1_idx].values,\n",
    "    \"dlogd\": dlogd.loc[mask_q1_idx].values,\n",
    "    \"dtaur\": dtaur.loc[mask_q1_idx].values,\n",
    "    \"dtaum\": dtaum.loc[mask_q1_idx].values,\n",
    "    \"dlogWr\": dlogWr.loc[mask_q1_idx].values,\n",
    "    \"dlogWm\": dlogWm.loc[mask_q1_idx].values,\n",
    "})\n",
    "# pesos médios absolutos por driver (combinando Wr e Wm)\n",
    "w_v = 2.5 * drv[\"dlogv\"].abs()                       # (2 + 3)/2\n",
    "w_d = 0.75 * drv[\"dlogd\"].abs()                      # (1 + 0.5)/2\n",
    "w_t = 0.5 * (drv[\"dtaur\"].abs() + drv[\"dtaum\"].abs())\n",
    "\n",
    "drv[\"w_v\"] = w_v\n",
    "drv[\"w_d\"] = w_d\n",
    "drv[\"w_t\"] = w_t\n",
    "\n",
    "# máscaras de dominância (booleans); .mean() em Series boolean retorna fração True\n",
    "mask_v_domina = (w_v > w_d) & (w_v > w_t)\n",
    "mask_d_domina = (w_d > w_v) & (w_d > w_t)\n",
    "mask_t_domina = (w_t > w_v) & (w_t > w_d)\n",
    "\n",
    "summary = pd.Series({\n",
    "    \"Q1_n\": int(len(drv)),\n",
    "    \"peso_medio_v\": w_v.mean(),\n",
    "    \"peso_medio_|Delta|\": w_d.mean(),\n",
    "    \"peso_medio_temp\": w_t.mean(),\n",
    "    \"%Q1_onde_v_domina\": mask_v_domina.mean() * 100.0,\n",
    "    \"%Q1_onde_|Delta|_domina\": mask_d_domina.mean() * 100.0,\n",
    "    \"%Q1_onde_temp_domina\": mask_t_domina.mean() * 100.0\n",
    "})\n",
    "\n",
    "# (opcional) sobrescreve arquivo com as novas colunas de pesos\n",
    "drv.to_csv(OUT_DRV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(summary.to_string())\n",
    "\n",
    "# -------------------- 6) PLOT (elipse/retângulo + quadrantes) --------------------\n",
    "plt.figure(figsize=(8.4,8.4))\n",
    "plt.scatter(XY.loc[outside, \"Wr\"], XY.loc[outside, \"Wm\"], s=10, alpha=0.35, label=\"Fora da ROI\")\n",
    "plt.scatter(XY.loc[inside,  \"Wr\"], XY.loc[inside,  \"Wm\"], s=10, alpha=0.65, label=\"Dentro da ROI\")\n",
    "plt.axvline(cx, linestyle=\"--\", linewidth=1)\n",
    "plt.axhline(cy, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "ax = plt.gca()\n",
    "if USE_ELLIPSE:\n",
    "    # desenha elipse aproximando eixos pela decomposição de cov\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    vals = np.clip(vals, 1e-12, None)\n",
    "    # fator para raio na elipse: sqrt(χ²)\n",
    "    scale = np.sqrt(chi2_thr)\n",
    "    width  = 2*scale*np.sqrt(vals[0])\n",
    "    height = 2*scale*np.sqrt(vals[1])\n",
    "    angle  = np.degrees(np.arctan2(vecs[1,0], vecs[0,0]))\n",
    "    e = Ellipse((cx, cy), width, height, angle=angle, fill=False, linewidth=1.5)\n",
    "    ax.add_patch(e)\n",
    "    roi_txt = roi_label\n",
    "else:\n",
    "    rect = Rectangle((cx - K_SIGMA*sigma_wr, cy - K_SIGMA*sigma_wm),\n",
    "                     2*K_SIGMA*sigma_wr, 2*K_SIGMA*sigma_wm,\n",
    "                     fill=False, linewidth=1.5)\n",
    "    ax.add_patch(rect)\n",
    "    roi_txt = roi_label\n",
    "\n",
    "plt.xlabel(\"Wr (v2)\"); plt.ylabel(\"Wm (v2)\")\n",
    "plt.title(f\"ROI estatística ({roi_txt}) e quadrantes\")\n",
    "# rótulos de quadrante afastados\n",
    "xmin, xmax = XY[\"Wr\"].min(), XY[\"Wr\"].max()\n",
    "ymin, ymax = XY[\"Wm\"].min(), XY[\"Wm\"].max()\n",
    "xpadR = 0.18*(xmax - cx); xpadL = 0.30*(cx - xmin)\n",
    "ypadU = 0.18*(ymax - cy); ypadD = 0.30*(cy - ymin)\n",
    "box = dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"gray\", alpha=0.85)\n",
    "fmt = lambda n: f\"{n} ({(100*n/total):.1f}%)\"\n",
    "plt.text(cx + xpadR, cy + ypadU, f\"Q1: {fmt(counts['Q1 (Wr↑, Wm↑)'])}\", ha=\"left\",  va=\"bottom\", bbox=box)\n",
    "plt.text(cx - xpadL, cy + ypadU, f\"Q2: {fmt(counts['Q2 (Wr↓, Wm↑)'])}\", ha=\"right\", va=\"bottom\", bbox=box)\n",
    "plt.text(cx - xpadL, cy - ypadD, f\"Q3: {fmt(counts['Q3 (Wr↓, Wm↓)'])}\", ha=\"right\", va=\"top\",    bbox=box)\n",
    "plt.text(cx + xpadR, cy - ypadD, f\"Q4: {fmt(counts['Q4 (Wr↑, Wm↓)'])}\", ha=\"left\",  va=\"top\",    bbox=box)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.tight_layout(); plt.savefig(OUT_PNG, dpi=150); plt.close()\n",
    "\n",
    "# -------------------- 7) PRINTS --------------------\n",
    "print(\"[OK] Gráfico salvo:\", OUT_PNG)\n",
    "print(\"Contagens (fora da ROI):\")\n",
    "for k, v in counts.items():\n",
    "    print(\" -\", k, \":\", v, f\"({v/total:.1%})\")\n",
    "print(\"\\nAtribuição de drivers em Q1 (arquivo):\", OUT_DRV)\n",
    "print(summary.to_string())\n",
    "print(\"\\nROI usada:\", roi_txt)\n",
    "if baseline_range is not None:\n",
    "    print(\"Estatísticas estimadas no baseline:\", baseline_range[0], \"→\", baseline_range[1])\n",
    "else:\n",
    "    print(\"Sem baseline explícito — σ e Σ estimados no conjunto todo (fallback).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018303d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] v3 (p5) gerada:\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_com_desgaste_p5.parquet\n",
      " - C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\derivadas\\tabela_wr_wm_com_desgaste_p5.csv\n",
      "\n",
      "[REFERÊNCIAS (p5) NA JANELA]\n",
      "  Janela: 2024-03-04 20:00:00 → 2024-03-12 15:00:00  |  Fonte: outputs\\baseline_datasets\\physics_baseline_proxies.csv\n",
      "  Wr_ref_p5 = 11218.414005750048\n",
      "  Wm_ref_p5 = 64086720.957561284\n",
      "\n",
      "[Vazão de AR — coluna: total_air_flow_knm3_h]\n",
      "  count_valid: 188\n",
      "  p5: 807.745\n",
      "  median: 822.1505\n",
      "  p95: 830.59695\n",
      "  mean: 821.0314946808512\n",
      "  std: 6.700817680567563\n",
      "  unit_hint: kNm³/h\n",
      "\n",
      "[Vazão de CARVÃO — coluna: flw_total_c_t_h]\n",
      "  count_valid: 188\n",
      "  p5: 61.6204\n",
      "  median: 65.37450000000001\n",
      "  p95: 71.19895\n",
      "  mean: 65.86998936170214\n",
      "  std: 2.9245689988036228\n",
      "  unit_hint: t/h\n",
      "\n",
      "[INFO] Log salvo em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\physics_refs_p5_summary.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# v3 (REFERÊNCIA ROBUSTA): usar p5 (5º percentil) como Wr_ref/Wm_ref\n",
    "# - Janela temporal: usa o \"baseline anterior\" (maior duração) se existir.\n",
    "#   Caso a interseção dessa janela com o dataset v2 fique vazia, cai para \"dataset inteiro\".\n",
    "# - Recalcula índices: Wr_idx_p5, Wm_idx_p5 (>= ~1 para ~95% dos pontos, salvo caudas).\n",
    "# - Reporta: valores de referência, janela usada, e estatísticas de\n",
    "#   vazão de AR (total ou primária) e vazão de CARVÃO (flw_total_c_t_h) no período.\n",
    "# - Salva: tabela_wr_wm_com_desgaste_p5.parquet/.csv e um log JSON com o resumo.\n",
    "# ============================================================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "SRC_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\"\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "IN_V2   = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste_v2.parquet\")\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"data\\derivadas\")\n",
    "LOG_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "OUT_PQ  = os.path.join(OUT_DIR, \"tabela_wr_wm_com_desgaste_p5.parquet\")\n",
    "OUT_CSV = os.path.join(OUT_DIR, \"tabela_wr_wm_com_desgaste_p5.csv\")\n",
    "OUT_LOG = os.path.join(LOG_DIR, \"physics_refs_p5_summary.json\")\n",
    "\n",
    "# possíveis fontes do baseline \"anterior\"\n",
    "CANDIDATES_BASELINE = [\n",
    "    os.path.join(SRC_BASE, r\"outputs\\baseline_datasets\\physics_baseline_proxies.csv\"),\n",
    "    os.path.join(SRC_BASE, r\"outputs\\baseline_mask.csv\"),\n",
    "]\n",
    "\n",
    "# 1) Carregar v2 (Wr/Wm já com |Δ| e ε)\n",
    "df = pd.read_parquet(IN_V2)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"timestamp\")\n",
    "\n",
    "# 2) Descobrir janela temporal do \"baseline anterior\" (maior duração)\n",
    "baseline_range = None\n",
    "baseline_source = None\n",
    "\n",
    "# a) proxies.csv → usa min..max do timestamp\n",
    "bp = CANDIDATES_BASELINE[0]\n",
    "if os.path.exists(bp):\n",
    "    try:\n",
    "        bdf = pd.read_csv(bp, encoding=\"utf-8-sig\")\n",
    "    except Exception:\n",
    "        bdf = pd.read_csv(bp, encoding=\"latin1\")\n",
    "    if \"timestamp\" in bdf.columns:\n",
    "        bdf[\"timestamp\"] = pd.to_datetime(bdf[\"timestamp\"], errors=\"coerce\")\n",
    "        bdf = bdf.dropna(subset=[\"timestamp\"])\n",
    "        if not bdf.empty:\n",
    "            baseline_range = (bdf[\"timestamp\"].min(), bdf[\"timestamp\"].max())\n",
    "            baseline_source = os.path.relpath(bp, SRC_BASE)\n",
    "\n",
    "# b) baseline_mask.csv → usa min..max das linhas com is_baseline==1\n",
    "if baseline_range is None:\n",
    "    bm = CANDIDATES_BASELINE[1]\n",
    "    if os.path.exists(bm):\n",
    "        try:\n",
    "            mdf = pd.read_csv(bm, encoding=\"utf-8-sig\")\n",
    "        except Exception:\n",
    "            mdf = pd.read_csv(bm, encoding=\"latin1\")\n",
    "        if {\"timestamp\",\"is_baseline\"}.issubset(mdf.columns):\n",
    "            mdf[\"timestamp\"] = pd.to_datetime(mdf[\"timestamp\"], errors=\"coerce\")\n",
    "            mdf = mdf[mdf[\"is_baseline\"]==1].dropna(subset=[\"timestamp\"])\n",
    "            if not mdf.empty:\n",
    "                baseline_range = (mdf[\"timestamp\"].min(), mdf[\"timestamp\"].max())\n",
    "                baseline_source = os.path.relpath(bm, SRC_BASE)\n",
    "\n",
    "# 3) Selecionar a janela para calcular p5\n",
    "if baseline_range is not None:\n",
    "    t0, t1 = baseline_range\n",
    "    mask_win = (df[\"timestamp\"] >= t0) & (df[\"timestamp\"] <= t1)\n",
    "    df_win = df.loc[mask_win].copy()\n",
    "    # se a janela não intersecta o dataset v2, cair para tudo\n",
    "    if df_win[\"Wr\"].notna().sum() < 10 or df_win[\"Wm\"].notna().sum() < 10:\n",
    "        df_win = df.copy()\n",
    "        baseline_source = \"DATASET INTEIRO (fallback — baseline vazio)\"\n",
    "        t0, t1 = df_win[\"timestamp\"].min(), df_win[\"timestamp\"].max()\n",
    "else:\n",
    "    df_win = df.copy()\n",
    "    t0, t1 = df_win[\"timestamp\"].min(), df_win[\"timestamp\"].max()\n",
    "    baseline_source = \"DATASET INTEIRO (sem baseline anterior)\"\n",
    "\n",
    "# 4) Referências robustas (p5) dentro da janela escolhida\n",
    "Wr_ref_p5 = float(np.nanpercentile(pd.to_numeric(df_win[\"Wr\"], errors=\"coerce\"), 5))\n",
    "Wm_ref_p5 = float(np.nanpercentile(pd.to_numeric(df_win[\"Wm\"], errors=\"coerce\"), 5))\n",
    "\n",
    "# 5) Recalcular índices com p5\n",
    "Wr_idx_p5 = df[\"Wr\"] / Wr_ref_p5\n",
    "Wm_idx_p5 = df[\"Wm\"] / Wm_ref_p5\n",
    "\n",
    "df_out = df.copy()\n",
    "df_out[\"Wr_ref_p5\"] = Wr_ref_p5\n",
    "df_out[\"Wm_ref_p5\"] = Wm_ref_p5\n",
    "df_out[\"Wr_idx_p5\"] = Wr_idx_p5\n",
    "df_out[\"Wm_idx_p5\"] = Wm_idx_p5\n",
    "\n",
    "df_out.to_parquet(OUT_PQ, index=False)\n",
    "df_out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# 6) Estatísticas operacionais na janela: vazão de AR e de CARVÃO (linha c)\n",
    "# ar: preferir total; fallback = primário\n",
    "air_col = \"total_air_flow_knm3_h\" if \"total_air_flow_knm3_h\" in df_win.columns else \\\n",
    "          (\"total_paf_air_flow_knm3_h\" if \"total_paf_air_flow_knm3_h\" in df_win.columns else None)\n",
    "coal_col = \"flw_total_c_t_h\" if \"flw_total_c_t_h\" in df_win.columns else None\n",
    "\n",
    "def stats_series(s):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return {\n",
    "        \"count_valid\": int(s.notna().sum()),\n",
    "        \"p5\": float(np.nanpercentile(s, 5)) if s.notna().any() else None,\n",
    "        \"median\": float(np.nanmedian(s)) if s.notna().any() else None,\n",
    "        \"p95\": float(np.nanpercentile(s, 95)) if s.notna().any() else None,\n",
    "        \"mean\": float(np.nanmean(s)) if s.notna().any() else None,\n",
    "        \"std\": float(np.nanstd(s, ddof=1)) if s.notna().sum() > 1 else None,\n",
    "        \"unit_hint\": \"kNm³/h\" if air_col and s.equals(pd.to_numeric(df_win[air_col], errors='coerce')) else (\"t/h\" if coal_col and s.equals(pd.to_numeric(df_win[coal_col], errors='coerce')) else None)\n",
    "    }\n",
    "\n",
    "air_stats = stats_series(df_win[air_col]) if air_col else None\n",
    "coal_stats = stats_series(df_win[coal_col]) if coal_col else None\n",
    "\n",
    "# 7) Log + prints\n",
    "log = {\n",
    "    \"janela_usada\": {\"inicio\": str(t0), \"fim\": str(t1), \"fonte\": baseline_source},\n",
    "    \"refs_p5\": {\"Wr_ref_p5\": Wr_ref_p5, \"Wm_ref_p5\": Wm_ref_p5},\n",
    "    \"vazoes_no_periodo\": {\n",
    "        \"ar_coluna\": air_col,\n",
    "        \"ar_stats\": air_stats,\n",
    "        \"carvao_coluna\": coal_col,\n",
    "        \"carvao_stats\": coal_stats\n",
    "    },\n",
    "    \"arquivos_saida\": {\"parquet\": OUT_PQ, \"csv\": OUT_CSV}\n",
    "}\n",
    "with open(OUT_LOG, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(log, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[OK] v3 (p5) gerada:\")\n",
    "print(\" -\", OUT_PQ)\n",
    "print(\" -\", OUT_CSV)\n",
    "print(\"\\n[REFERÊNCIAS (p5) NA JANELA]\")\n",
    "print(f\"  Janela: {t0} → {t1}  |  Fonte: {baseline_source}\")\n",
    "print(f\"  Wr_ref_p5 = {Wr_ref_p5}\")\n",
    "print(f\"  Wm_ref_p5 = {Wm_ref_p5}\")\n",
    "\n",
    "if air_col:\n",
    "    print(f\"\\n[Vazão de AR — coluna: {air_col}]\")\n",
    "    for k,v in air_stats.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "else:\n",
    "    print(\"\\n[Vazão de AR] coluna não encontrada.\")\n",
    "\n",
    "if coal_col:\n",
    "    print(f\"\\n[Vazão de CARVÃO — coluna: {coal_col}]\")\n",
    "    for k,v in coal_stats.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "else:\n",
    "    print(\"\\n[Vazão de CARVÃO] coluna 'flw_total_c_t_h' não encontrada.\")\n",
    "\n",
    "print(\"\\n[INFO] Log salvo em:\", OUT_LOG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87f93aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] RAW: gráfico -> C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_elipse_ref_p5_raw.png\n",
      "     Fora elipse: 6729 / 7473 (90.0%)\n",
      "      Q1 (↑,↑) : 6729\n",
      "      Q2 (↓,↑) : 0\n",
      "      Q3 (↓,↓) : 0\n",
      "      Q4 (↑,↓) : 0\n",
      "[OK] IDX: gráfico -> C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_elipse_ref_p5_idx.png\n",
      "     Fora elipse: 6729 / 7473 (90.0%)\n",
      "      Q1 (↑,↑) : 6729\n",
      "      Q2 (↓,↑) : 0\n",
      "      Q3 (↓,↓) : 0\n",
      "      Q4 (↑,↓) : 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Wr×Wm COM EIXOS NA REF P5 + ELIPSE χ²(95%)  (BRUTO e ÍNDICES)\n",
    "# ============================================================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "P5_PQ  = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste_p5.parquet\")\n",
    "P5_LOG = os.path.join(DST_BASE, r\"outputs\\diagnosticos\\physics_refs_p5_summary.json\")\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "PNG_RAW = os.path.join(OUT_DIR, \"wr_wm_elipse_ref_p5_raw.png\")\n",
    "PNG_IDX = os.path.join(OUT_DIR, \"wr_wm_elipse_ref_p5_idx.png\")\n",
    "CSV_OUT_RAW = os.path.join(OUT_DIR, \"wr_wm_outliers_elipse_raw.csv\")\n",
    "CSV_OUT_IDX = os.path.join(OUT_DIR, \"wr_wm_outliers_elipse_idx.csv\")\n",
    "\n",
    "CHI2_P = 0.95\n",
    "CHI2_TABLE = {0.90:4.605, 0.95:5.991, 0.99:9.210}\n",
    "CHI2_THR = CHI2_TABLE[CHI2_P]\n",
    "\n",
    "# --------- helpers ---------\n",
    "def baseline_window_from_log(df, log_path):\n",
    "    if not os.path.exists(log_path):\n",
    "        return df[\"timestamp\"].min(), df[\"timestamp\"].max(), \"DATASET INTEIRO (sem log p5)\"\n",
    "    with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    j = meta.get(\"janela_usada\", {})\n",
    "    t0 = pd.to_datetime(j.get(\"inicio\"))\n",
    "    t1 = pd.to_datetime(j.get(\"fim\"))\n",
    "    src = j.get(\"fonte\", \"desconhecida\")\n",
    "    if pd.isna(t0) or pd.isna(t1):\n",
    "        return df[\"timestamp\"].min(), df[\"timestamp\"].max(), \"DATASET INTEIRO (fallback)\"\n",
    "    return t0, t1, src\n",
    "\n",
    "def cov_from_window(df_xy, t0, t1):\n",
    "    win = df_xy[(df_xy[\"timestamp\"]>=t0) & (df_xy[\"timestamp\"]<=t1)]\n",
    "    if len(win) < 10:\n",
    "        win = df_xy  # fallback\n",
    "    # Explicitly select only numeric columns for covariance calculation\n",
    "    cov = np.cov(win[\"Wr\"].values, win[\"Wm\"].values)\n",
    "    return cov\n",
    "\n",
    "def ellipse_mask_and_patch(x, y, cx, cy, cov, chi2_thr, ax):\n",
    "    # eigendecomp\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    vals = np.clip(vals, 1e-12, None)\n",
    "    inv_cov = np.linalg.inv(cov if np.all(np.isfinite(cov)) else np.diag(vals))\n",
    "    # Mahalanobis\n",
    "    diff = np.vstack([x - cx, y - cy]).T  # (n,2)\n",
    "    d2 = np.einsum(\"ij,jk,ik->i\", diff, inv_cov, diff)\n",
    "    inside = d2 <= chi2_thr\n",
    "    # ellipse patch (scale = sqrt(chi2))\n",
    "    scale = np.sqrt(chi2_thr)\n",
    "    width  = 2*scale*np.sqrt(vals[0])\n",
    "    height = 2*scale*np.sqrt(vals[1])\n",
    "    angle  = np.degrees(np.arctan2(vecs[1,0], vecs[0,0]))\n",
    "    e = Ellipse((cx, cy), width, height, angle=angle, fill=False, linewidth=1.5)\n",
    "    ax.add_patch(e)\n",
    "    return inside\n",
    "\n",
    "def quad_counts(x, y, cx, cy, outside_mask):\n",
    "    q1 = outside_mask & (x>=cx) & (y>=cy)\n",
    "    q2 = outside_mask & (x< cx) & (y>=cy)\n",
    "    q3 = outside_mask & (x< cx) & (y< cy)\n",
    "    q4 = outside_mask & (x>=cx) & (y< cy)\n",
    "    return {\n",
    "        \"Q1 (↑,↑)\": int(q1.sum()),\n",
    "        \"Q2 (↓,↑)\": int(q2.sum()),\n",
    "        \"Q3 (↓,↓)\": int(q3.sum()),\n",
    "        \"Q4 (↑,↓)\": int(q4.sum()),\n",
    "    }, q1, q2, q3, q4\n",
    "\n",
    "def annotate_quads(ax, counts, cx, cy, xmin, xmax, ymin, ymax, total):\n",
    "    xpadR = 0.18*(xmax - cx); xpadL = 0.30*(cx - xmin)\n",
    "    ypadU = 0.18*(ymax - cy); ypadD = 0.30*(cy - ymin)\n",
    "    box = dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"gray\", alpha=0.85)\n",
    "    fmt = lambda n: f\"{n} ({(100*n/total):.1f}%)\"\n",
    "    ax.text(cx + xpadR, cy + ypadU, f\"Q1: {fmt(counts['Q1 (↑,↑)'])}\", ha=\"left\",  va=\"bottom\", bbox=box)\n",
    "    ax.text(cx - xpadL, cy + ypadU, f\"Q2: {fmt(counts['Q2 (↓,↑)'])}\", ha=\"right\", va=\"bottom\", bbox=box)\n",
    "    ax.text(cx - xpadL, cy - ypadD, f\"Q3: {fmt(counts['Q3 (↓,↓)'])}\", ha=\"right\", va=\"top\",    bbox=box)\n",
    "    ax.text(cx + xpadR, cy - ypadD, f\"Q4: {fmt(counts['Q4 (↑,↓)'])}\", ha=\"left\",  va=\"top\",    bbox=box)\n",
    "\n",
    "# --------- 1) carregar p5 ---------\n",
    "df = pd.read_parquet(P5_PQ)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"timestamp\")\n",
    "\n",
    "Wr_ref_p5 = float(df[\"Wr_ref_p5\"].dropna().iloc[0])\n",
    "Wm_ref_p5 = float(df[\"Wm_ref_p5\"].dropna().iloc[0])\n",
    "\n",
    "t0, t1, fonte = baseline_window_from_log(df, P5_LOG)\n",
    "\n",
    "# ============================================================\n",
    "# A) ESPAÇO BRUTO (Wr, Wm)\n",
    "# ============================================================\n",
    "raw = df[[\"timestamp\",\"Wr\",\"Wm\"]].dropna().copy()\n",
    "x, y = raw[\"Wr\"].values, raw[\"Wm\"].values\n",
    "cx, cy = Wr_ref_p5, Wm_ref_p5\n",
    "\n",
    "cov_raw = cov_from_window(raw, t0, t1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.4,8.4))\n",
    "ax.scatter(x, y, s=10, alpha=0.35, label=\"Todos os pontos\")\n",
    "ax.axvline(cx, linestyle=\"--\", linewidth=1)\n",
    "ax.axhline(cy, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "inside = ellipse_mask_and_patch(x, y, cx, cy, cov_raw, CHI2_THR, ax)\n",
    "outside = ~inside\n",
    "ax.scatter(x[inside],  y[inside],  s=10, alpha=0.65, label=\"Dentro elipse 95%\")\n",
    "ax.scatter(x[outside], y[outside], s=12, alpha=0.65, label=\"Fora elipse 95%\")\n",
    "\n",
    "counts, q1, q2, q3, q4 = quad_counts(x, y, cx, cy, outside)\n",
    "xmin, xmax = x.min(), x.max()\n",
    "ymin, ymax = y.min(), y.max()\n",
    "annotate_quads(ax, counts, cx, cy, xmin, xmax, ymin, ymax, len(x))\n",
    "\n",
    "ax.set_title(f\"Wr × Wm (bruto) — eixos na ref p5, elipse χ²(95%)\\nBaseline para Σ: {str(t0)} → {str(t1)} ({fonte})\")\n",
    "ax.set_xlabel(\"Wr (v2)\")\n",
    "ax.set_ylabel(\"Wm (v2)\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout(); plt.savefig(PNG_RAW, dpi=150); plt.close()\n",
    "\n",
    "# salvar CSV de outliers (bruto)\n",
    "out_raw = raw.loc[outside, [\"timestamp\",\"Wr\",\"Wm\"]].copy()\n",
    "out_raw[\"quadrante\"] = np.where(q1, \"Q1\", np.where(q2, \"Q2\", np.where(q3, \"Q3\", \"Q4\")))[outside]\n",
    "out_raw.to_csv(CSV_OUT_RAW, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[OK] RAW: gráfico ->\", PNG_RAW)\n",
    "print(\"     Fora elipse:\", outside.sum(), \"/\", len(x), f\"({100*outside.mean():.1f}%)\")\n",
    "for k,v in counts.items(): print(\"     \", k, \":\", v)\n",
    "\n",
    "# ============================================================\n",
    "# B) ESPAÇO DE ÍNDICES (Wr_idx_p5, Wm_idx_p5)\n",
    "# ============================================================\n",
    "idx = df[[\"timestamp\",\"Wr_idx_p5\",\"Wm_idx_p5\"]].dropna().copy()\n",
    "xi, yi = idx[\"Wr_idx_p5\"].values, idx[\"Wm_idx_p5\"].values\n",
    "cxi, cyi = 1.0, 1.0  # por definição da normalização p5\n",
    "\n",
    "cov_idx = cov_from_window(idx.rename(columns={\"Wr_idx_p5\":\"Wr\",\"Wm_idx_p5\":\"Wm\"}), t0, t1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.4,8.4))\n",
    "ax.scatter(xi, yi, s=10, alpha=0.35, label=\"Todos os pontos (idx)\")\n",
    "ax.axvline(cxi, linestyle=\"--\", linewidth=1)\n",
    "ax.axhline(cyi, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "inside_i = ellipse_mask_and_patch(xi, yi, cxi, cyi, cov_idx, CHI2_THR, ax)\n",
    "outside_i = ~inside_i\n",
    "ax.scatter(xi[inside_i],  yi[inside_i],  s=10, alpha=0.65, label=\"Dentro elipse 95%\")\n",
    "ax.scatter(xi[outside_i], yi[outside_i], s=12, alpha=0.65, label=\"Fora elipse 95%\")\n",
    "\n",
    "counts_i, q1i, q2i, q3i, q4i = quad_counts(xi, yi, cxi, cyi, outside_i)\n",
    "xmin, xmax = xi.min(), xi.max()\n",
    "ymin, ymax = yi.min(), yi.max()\n",
    "annotate_quads(ax, counts_i, cxi, cyi, xmin, xmax, ymin, ymax, len(xi))\n",
    "\n",
    "ax.set_title(f\"Wr_idx_p5 × Wm_idx_p5 — eixos em (1,1), elipse χ²(95%)\\nBaseline para Σ: {str(t0)} → {str(t1)} ({fonte})\")\n",
    "ax.set_xlabel(\"Wr_idx_p5\")\n",
    "ax.set_ylabel(\"Wm_idx_p5\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout(); plt.savefig(PNG_IDX, dpi=150); plt.close()\n",
    "\n",
    "# salvar CSV de outliers (índices)\n",
    "out_idx = idx.loc[outside_i, [\"timestamp\",\"Wr_idx_p5\",\"Wm_idx_p5\"]].copy()\n",
    "out_idx[\"quadrante\"] = np.where(q1i, \"Q1\", np.where(q2i, \"Q2\", np.where(q3i, \"Q3\", \"Q4\")))[outside_i]\n",
    "out_idx.to_csv(CSV_OUT_IDX, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[OK] IDX: gráfico ->\", PNG_IDX)\n",
    "print(\"     Fora elipse:\", outside_i.sum(), \"/\", len(xi), f\"({100*outside_i.mean():.1f}%)\")\n",
    "for k,v in counts_i.items(): print(\"     \", k, \":\", v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e2c90bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] RAW: gráfico -> C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_elipse_ref_p5_raw.png\n",
      "     Fora elipse: 6729 / 7473 (90.0%)\n",
      "      Q1 (↑,↑) : 6729\n",
      "      Q2 (↓,↑) : 0\n",
      "      Q3 (↓,↓) : 0\n",
      "      Q4 (↑,↓) : 0\n",
      "[OK] IDX: gráfico -> C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\wr_wm_elipse_ref_p5_idx.png\n",
      "     Fora elipse: 6729 / 7473 (90.0%)\n",
      "      Q1 (↑,↑) : 6729\n",
      "      Q2 (↓,↑) : 0\n",
      "      Q3 (↓,↓) : 0\n",
      "      Q4 (↑,↓) : 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Wr×Wm com eixos nas referências p5 + elipse χ²(95%)\n",
    "# E o mesmo gráfico no espaço dos ÍNDICES (Wr_idx_p5 × Wm_idx_p5)\n",
    "# ------------------------------------------------------------\n",
    "# FIX do erro DTypePromotionError:\n",
    "#   - A covariância agora é calculada explicitamente nas colunas numéricas\n",
    "#     (não usa iloc que pegava 'timestamp' sem querer).\n",
    "# ============================================================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "\n",
    "P5_PQ  = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste_p5.parquet\")\n",
    "P5_LOG = os.path.join(DST_BASE, r\"outputs\\diagnosticos\\physics_refs_p5_summary.json\")\n",
    "OUT_DIR = os.path.join(DST_BASE, r\"outputs\\diagnosticos\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "PNG_RAW = os.path.join(OUT_DIR, \"wr_wm_elipse_ref_p5_raw.png\")\n",
    "PNG_IDX = os.path.join(OUT_DIR, \"wr_wm_elipse_ref_p5_idx.png\")\n",
    "CSV_OUT_RAW = os.path.join(OUT_DIR, \"wr_wm_outliers_elipse_raw.csv\")\n",
    "CSV_OUT_IDX = os.path.join(OUT_DIR, \"wr_wm_outliers_elipse_idx.csv\")\n",
    "\n",
    "CHI2_P = 0.95\n",
    "CHI2_TABLE = {0.90:4.605, 0.95:5.991, 0.99:9.210}\n",
    "CHI2_THR = CHI2_TABLE[CHI2_P]\n",
    "\n",
    "# ----------------- helpers -----------------\n",
    "def baseline_window_from_log(df, log_path):\n",
    "    if not os.path.exists(log_path):\n",
    "        return df[\"timestamp\"].min(), df[\"timestamp\"].max(), \"DATASET INTEIRO (sem log p5)\"\n",
    "    with open(log_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    j = meta.get(\"janela_usada\", {})\n",
    "    t0 = pd.to_datetime(j.get(\"inicio\"))\n",
    "    t1 = pd.to_datetime(j.get(\"fim\"))\n",
    "    src = j.get(\"fonte\", \"desconhecida\")\n",
    "    if pd.isna(t0) or pd.isna(t1):\n",
    "        return df[\"timestamp\"].min(), df[\"timestamp\"].max(), \"DATASET INTEIRO (fallback)\"\n",
    "    return t0, t1, src\n",
    "\n",
    "def cov_from_window(df_xy, t0, t1, xcol, ycol):\n",
    "    win = df_xy[(df_xy[\"timestamp\"]>=t0) & (df_xy[\"timestamp\"]<=t1)].copy()\n",
    "    if len(win) < 10:\n",
    "        win = df_xy.copy()  # fallback\n",
    "    x = pd.to_numeric(win[xcol], errors=\"coerce\").astype(float)\n",
    "    y = pd.to_numeric(win[ycol], errors=\"coerce\").astype(float)\n",
    "    mask = x.notna() & y.notna()\n",
    "    x = x[mask].values\n",
    "    y = y[mask].values\n",
    "    if x.size < 2:\n",
    "        # variância ~0 -> usa diagonal pequena para não quebrar\n",
    "        return np.diag([1e-12, 1e-12])\n",
    "    cov = np.cov(x, y)\n",
    "    # robustez numérica mínima\n",
    "    if cov.shape != (2,2) or not np.all(np.isfinite(cov)):\n",
    "        cov = np.diag([np.nanvar(x) + 1e-12, np.nanvar(y) + 1e-12])\n",
    "    # ridge para garantir inversão\n",
    "    cov = cov + np.eye(2)*1e-12\n",
    "    return cov\n",
    "\n",
    "def ellipse_mask_and_patch(x, y, cx, cy, cov, chi2_thr, ax):\n",
    "    # eigendecomp para desenhar elipse\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    vals = np.clip(vals, 1e-12, None)\n",
    "    try:\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "    except np.linalg.LinAlgError:\n",
    "        inv_cov = np.linalg.inv(np.diag(vals))\n",
    "    # distância de Mahalanobis\n",
    "    diff = np.vstack([x - cx, y - cy]).T  # (n,2)\n",
    "    d2 = np.sum((diff @ inv_cov) * diff, axis=1)\n",
    "    inside = d2 <= chi2_thr\n",
    "    # patch da elipse (raios ~ sqrt(chi2)*sqrt(autovalor))\n",
    "    scale = np.sqrt(chi2_thr)\n",
    "    width  = 2*scale*np.sqrt(vals[0])\n",
    "    height = 2*scale*np.sqrt(vals[1])\n",
    "    angle  = np.degrees(np.arctan2(vecs[1,0], vecs[0,0]))\n",
    "    e = Ellipse((cx, cy), width, height, angle=angle, fill=False, linewidth=1.5)\n",
    "    ax.add_patch(e)\n",
    "    return inside\n",
    "\n",
    "def quad_counts(x, y, cx, cy, outside_mask):\n",
    "    q1 = outside_mask & (x>=cx) & (y>=cy)\n",
    "    q2 = outside_mask & (x< cx) & (y>=cy)\n",
    "    q3 = outside_mask & (x< cx) & (y< cy)\n",
    "    q4 = outside_mask & (x>=cx) & (y< cy)\n",
    "    return {\n",
    "        \"Q1 (↑,↑)\": int(q1.sum()),\n",
    "        \"Q2 (↓,↑)\": int(q2.sum()),\n",
    "        \"Q3 (↓,↓)\": int(q3.sum()),\n",
    "        \"Q4 (↑,↓)\": int(q4.sum()),\n",
    "    }, q1, q2, q3, q4\n",
    "\n",
    "def annotate_quads(ax, counts, cx, cy, xmin, xmax, ymin, ymax, total):\n",
    "    xpadR = 0.18*(xmax - cx); xpadL = 0.30*(cx - xmin)\n",
    "    ypadU = 0.18*(ymax - cy); ypadD = 0.30*(cy - ymin)\n",
    "    box = dict(boxstyle=\"round,pad=0.2\", fc=\"white\", ec=\"gray\", alpha=0.85)\n",
    "    fmt = lambda n: f\"{n} ({(100*n/total):.1f}%)\"\n",
    "    ax.text(cx + xpadR, cy + ypadU, f\"Q1: {fmt(counts['Q1 (↑,↑)'])}\", ha=\"left\",  va=\"bottom\", bbox=box)\n",
    "    ax.text(cx - xpadL, cy + ypadU, f\"Q2: {fmt(counts['Q2 (↓,↑)'])}\", ha=\"right\", va=\"bottom\", bbox=box)\n",
    "    ax.text(cx - xpadL, cy - ypadD, f\"Q3: {fmt(counts['Q3 (↓,↓)'])}\", ha=\"right\", va=\"top\",    bbox=box)\n",
    "    ax.text(cx + xpadR, cy - ypadD, f\"Q4: {fmt(counts['Q4 (↑,↓)'])}\", ha=\"left\",  va=\"top\",    bbox=box)\n",
    "\n",
    "# ----------------- carregar p5 -----------------\n",
    "df = pd.read_parquet(P5_PQ)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
    "df = df.sort_values(\"timestamp\")\n",
    "\n",
    "Wr_ref_p5 = float(pd.to_numeric(df[\"Wr_ref_p5\"], errors=\"coerce\").dropna().iloc[0])\n",
    "Wm_ref_p5 = float(pd.to_numeric(df[\"Wm_ref_p5\"], errors=\"coerce\").dropna().iloc[0])\n",
    "\n",
    "t0, t1, fonte = baseline_window_from_log(df, P5_LOG)\n",
    "\n",
    "# ============================================================\n",
    "# A) ESPAÇO BRUTO (Wr, Wm) — eixos cruzando na ref p5\n",
    "# ============================================================\n",
    "raw = df[[\"timestamp\",\"Wr\",\"Wm\"]].dropna().copy()\n",
    "x = pd.to_numeric(raw[\"Wr\"], errors=\"coerce\").astype(float).values\n",
    "y = pd.to_numeric(raw[\"Wm\"], errors=\"coerce\").astype(float).values\n",
    "cx, cy = Wr_ref_p5, Wm_ref_p5\n",
    "\n",
    "cov_raw = cov_from_window(raw, t0, t1, \"Wr\", \"Wm\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.4,8.4))\n",
    "ax.scatter(x, y, s=10, alpha=0.35, label=\"Todos os pontos\")\n",
    "ax.axvline(cx, linestyle=\"--\", linewidth=1)\n",
    "ax.axhline(cy, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "inside = ellipse_mask_and_patch(x, y, cx, cy, cov_raw, CHI2_THR, ax)\n",
    "outside = ~inside\n",
    "ax.scatter(x[inside],  y[inside],  s=10, alpha=0.65, label=\"Dentro elipse 95%\")\n",
    "ax.scatter(x[outside], y[outside], s=12, alpha=0.65, label=\"Fora elipse 95%\")\n",
    "\n",
    "counts, q1, q2, q3, q4 = quad_counts(x, y, cx, cy, outside)\n",
    "xmin, xmax = np.nanmin(x), np.nanmax(x)\n",
    "ymin, ymax = np.nanmin(y), np.nanmax(y)\n",
    "annotate_quads(ax, counts, cx, cy, xmin, xmax, ymin, ymax, len(x))\n",
    "\n",
    "ax.set_title(f\"Wr × Wm (bruto) — eixos na ref p5, elipse χ²(95%)\\nBaseline para Σ: {str(t0)} → {str(t1)} ({fonte})\")\n",
    "ax.set_xlabel(\"Wr (v2)\")\n",
    "ax.set_ylabel(\"Wm (v2)\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout(); plt.savefig(PNG_RAW, dpi=150); plt.close()\n",
    "\n",
    "# CSV outliers (bruto)\n",
    "out_raw = raw.iloc[outside.nonzero()[0]][[\"timestamp\",\"Wr\",\"Wm\"]].copy()\n",
    "lab_raw = np.where(q1, \"Q1\", np.where(q2, \"Q2\", np.where(q3, \"Q3\", \"Q4\")))\n",
    "out_raw[\"quadrante\"] = lab_raw[outside]\n",
    "out_raw.to_csv(CSV_OUT_RAW, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[OK] RAW: gráfico ->\", PNG_RAW)\n",
    "print(\"     Fora elipse:\", int(outside.sum()), \"/\", len(x), f\"({100*outside.mean():.1f}%)\")\n",
    "for k,v in counts.items(): print(\"     \", k, \":\", v)\n",
    "\n",
    "# ============================================================\n",
    "# B) ESPAÇO DE ÍNDICES (Wr_idx_p5, Wm_idx_p5) — eixos (1,1)\n",
    "# ============================================================\n",
    "idx = df[[\"timestamp\",\"Wr_idx_p5\",\"Wm_idx_p5\"]].dropna().copy()\n",
    "xi = pd.to_numeric(idx[\"Wr_idx_p5\"], errors=\"coerce\").astype(float).values\n",
    "yi = pd.to_numeric(idx[\"Wm_idx_p5\"], errors=\"coerce\").astype(float).values\n",
    "cxi, cyi = 1.0, 1.0\n",
    "\n",
    "cov_idx = cov_from_window(idx.rename(columns={\"Wr_idx_p5\":\"Wr\",\"Wm_idx_p5\":\"Wm\"}),\n",
    "                          t0, t1, \"Wr\", \"Wm\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8.4,8.4))\n",
    "ax.scatter(xi, yi, s=10, alpha=0.35, label=\"Todos os pontos (idx)\")\n",
    "ax.axvline(cxi, linestyle=\"--\", linewidth=1)\n",
    "ax.axhline(cyi, linestyle=\"--\", linewidth=1)\n",
    "\n",
    "inside_i = ellipse_mask_and_patch(xi, yi, cxi, cyi, cov_idx, CHI2_THR, ax)\n",
    "outside_i = ~inside_i\n",
    "ax.scatter(xi[inside_i],  yi[inside_i],  s=10, alpha=0.65, label=\"Dentro elipse 95%\")\n",
    "ax.scatter(xi[outside_i], yi[outside_i], s=12, alpha=0.65, label=\"Fora elipse 95%\")\n",
    "\n",
    "counts_i, q1i, q2i, q3i, q4i = quad_counts(xi, yi, cxi, cyi, outside_i)\n",
    "xmin, xmax = np.nanmin(xi), np.nanmax(xi)\n",
    "ymin, ymax = np.nanmin(yi), np.nanmax(yi)\n",
    "annotate_quads(ax, counts_i, cxi, cyi, xmin, xmax, ymin, ymax, len(xi))\n",
    "\n",
    "ax.set_title(f\"Wr_idx_p5 × Wm_idx_p5 — eixos em (1,1), elipse χ²(95%)\\nBaseline para Σ: {str(t0)} → {str(t1)} ({fonte})\")\n",
    "ax.set_xlabel(\"Wr_idx_p5\")\n",
    "ax.set_ylabel(\"Wm_idx_p5\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "plt.tight_layout(); plt.savefig(PNG_IDX, dpi=150); plt.close()\n",
    "\n",
    "# CSV outliers (índices)\n",
    "out_idx = idx.iloc[outside_i.nonzero()[0]][[\"timestamp\",\"Wr_idx_p5\",\"Wm_idx_p5\"]].copy()\n",
    "lab_idx = np.where(q1i, \"Q1\", np.where(q2i, \"Q2\", np.where(q3i, \"Q3\", \"Q4\")))\n",
    "out_idx[\"quadrante\"] = lab_idx[outside_i]\n",
    "out_idx.to_csv(CSV_OUT_IDX, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"[OK] IDX: gráfico ->\", PNG_IDX)\n",
    "print(\"     Fora elipse:\", int(outside_i.sum()), \"/\", len(xi), f\"({100*outside_i.mean():.1f}%)\")\n",
    "for k,v in counts_i.items(): print(\"     \", k, \":\", v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ebc7211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TOTAL] linhas na base: 11,757\n",
      "[PLOT ] linhas com Wr_idx_p5 & Wm_idx_p5 válidos: 7,473 (63.6%)\n",
      "\n",
      "[CAUSAS para linhas fora do gráfico (Wr/Wm NaN) — contagens]\n",
      " - faltou vazão de AR (total e primária ao mesmo tempo): 10\n",
      " - faltou delta_proxy: 0\n",
      " - faltaram TODAS as taus (densa/diluida/backpass): 0\n",
      "\n",
      "[ELIPSE 95% nos ÍNDICES]\n",
      " - janela para Σ: 2024-03-04 20:00:00 → 2024-03-12 15:00:00\n",
      " - dentro da elipse: 744 (10.0% dos 7,473 plotados)\n",
      " - fora   da elipse: 6,729 (90.0% dos 7,473 plotados)\n"
     ]
    }
   ],
   "source": [
    "# ================== CHECK: contagens e diagnóstico ==================\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import inv, eigh\n",
    "\n",
    "DST_BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "P5_PQ  = os.path.join(DST_BASE, r\"data\\derivadas\\tabela_wr_wm_com_desgaste_p5.parquet\")\n",
    "P5_LOG = os.path.join(DST_BASE, r\"outputs\\diagnosticos\\physics_refs_p5_summary.json\")\n",
    "\n",
    "CHI2_THR = 5.991  # 95%, 2 g.l.\n",
    "\n",
    "df = pd.read_parquet(P5_PQ).sort_values(\"timestamp\")\n",
    "# --- totais vs. o que entra no gráfico de índices ---\n",
    "n_total = len(df)\n",
    "idx = df[[\"timestamp\",\"Wr_idx_p5\",\"Wm_idx_p5\"]].dropna().copy()\n",
    "n_plot = len(idx)\n",
    "print(f\"[TOTAL] linhas na base: {n_total:,}\")\n",
    "print(f\"[PLOT ] linhas com Wr_idx_p5 & Wm_idx_p5 válidos: {n_plot:,} ({n_plot/n_total:.1%})\")\n",
    "\n",
    "# --- diagnóstico do porquê ficaram de fora (usa causas na base v2) ---\n",
    "miss = df[(df[\"Wr\"].isna()) | (df[\"Wm\"].isna())].copy()\n",
    "v_missing = (df.get(\"total_air_flow_knm3_h\").isna() if \"total_air_flow_knm3_h\" in df.columns else True) & \\\n",
    "            (df.get(\"total_paf_air_flow_knm3_h\").isna() if \"total_paf_air_flow_knm3_h\" in df.columns else True)\n",
    "delta_missing = df.get(\"delta_proxy\").isna() if \"delta_proxy\" in df.columns else True\n",
    "tau_missing = pd.concat([df[c] for c in [\"tau_densa\",\"tau_diluida\",\"tau_backpass\"] if c in df.columns], axis=1).isna().all(axis=1)\n",
    "\n",
    "print(\"\\n[CAUSAS para linhas fora do gráfico (Wr/Wm NaN) — contagens]\")\n",
    "print(\" - faltou vazão de AR (total e primária ao mesmo tempo):\", int(v_missing.sum()))\n",
    "print(\" - faltou delta_proxy:\", int(delta_missing.sum()))\n",
    "print(\" - faltaram TODAS as taus (densa/diluida/backpass):\", int(tau_missing.sum()))\n",
    "\n",
    "# --- janela do baseline para a elipse ---\n",
    "if os.path.exists(P5_LOG):\n",
    "    with open(P5_LOG, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    j = meta.get(\"janela_usada\", {})\n",
    "    t0 = pd.to_datetime(j.get(\"inicio\")); t1 = pd.to_datetime(j.get(\"fim\"))\n",
    "else:\n",
    "    t0 = idx[\"timestamp\"].min(); t1 = idx[\"timestamp\"].max()\n",
    "\n",
    "# --- elipse 95% no espaço dos ÍNDICES (centrada em 1,1) ---\n",
    "win = idx[(idx[\"timestamp\"]>=t0) & (idx[\"timestamp\"]<=t1)]\n",
    "if len(win) < 10: win = idx\n",
    "X = win[\"Wr_idx_p5\"].astype(float).values\n",
    "Y = win[\"Wm_idx_p5\"].astype(float).values\n",
    "cov = np.cov(X, Y) + np.eye(2)*1e-12\n",
    "\n",
    "# Mahalanobis de TODOS os pontos de índice\n",
    "xi = idx[\"Wr_idx_p5\"].astype(float).values\n",
    "yi = idx[\"Wm_idx_p5\"].astype(float).values\n",
    "diff = np.vstack([xi - 1.0, yi - 1.0]).T\n",
    "inv_cov = inv(cov)\n",
    "d2 = np.sum((diff @ inv_cov) * diff, axis=1)\n",
    "inside = d2 <= CHI2_THR\n",
    "n_inside = int(inside.sum()); n_out = int((~inside).sum())\n",
    "\n",
    "print(f\"\\n[ELIPSE 95% nos ÍNDICES]\")\n",
    "print(f\" - janela para Σ: {t0} → {t1}\")\n",
    "print(f\" - dentro da elipse: {n_inside:,} ({n_inside/n_plot:.1%} dos {n_plot:,} plotados)\")\n",
    "print(f\" - fora   da elipse: {n_out:,} ({n_out/n_plot:.1%} dos {n_plot:,} plotados)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be9da0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== ARTEFATOS SALVOS ========\n",
      "E0 (MVP supervisionado):\n",
      "  - Modelo:    C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\freeze\\v1_20250818_0635\\models\\mvp\\model_mvp.joblib\n",
      "  - Manifesto: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\freeze\\v1_20250818_0635\\models\\mvp\\manifest_mvp.json\n",
      "  - Card:      C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\freeze\\v1_20250818_0635\\models\\mvp\\model_card.md\n",
      "\n",
      "Modelo Físico (MVP):\n",
      "  - Modelo:    C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\freeze\\v1_20250818_0635\\models\\phys\\model_phys_mvp.joblib\n",
      "  - Manifesto: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\freeze\\v1_20250818_0635\\models\\phys\\manifest_phys_mvp.json\n",
      "  - Card:      C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\freeze\\v1_20250818_0635\\models\\phys\\model_card_phys.md\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PROJETO A1 — Empacote do MODELO FÍSICO (igual ao E0)\n",
    "# ============================================================\n",
    "import os, re, json, math, joblib, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# --------- PATHS (ajuste se necessário)\n",
    "FREEZE_DIR = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\freeze\\v1_20250818_0635\"\n",
    "PATH_ROT_GOLD = os.path.join(FREEZE_DIR, \"A1_ML_DL_rotulado_v4_gold.csv\")   # insumos físicos costumam estar aqui\n",
    "PHYS_DIR      = os.path.join(FREEZE_DIR, \"models\", \"phys\")\n",
    "MVP_DIR       = os.path.join(FREEZE_DIR, \"models\", \"mvp\")                   # onde já fixamos o E0\n",
    "\n",
    "os.makedirs(PHYS_DIR, exist_ok=True)\n",
    "os.makedirs(MVP_DIR,  exist_ok=True)\n",
    "\n",
    "# ============================================================\n",
    "# 1) Implementação do modelo físico (MVP)\n",
    "# ============================================================\n",
    "class PhysicsBasedModel:\n",
    "    \"\"\"\n",
    "    Modelo físico determinístico para Wr/Wm.\n",
    "    Fórmulas (MVP):\n",
    "\n",
    "      Wr = α · ν^2 · δ^1 · exp(−T/Tcrit) · g\n",
    "      Wm = β · ν^3 · √δ · exp(−T/Topt) · h\n",
    "\n",
    "      com g = σ_mat / ρ_fuel\n",
    "          h = σ_steel / μ_gas\n",
    "\n",
    "    Notas:\n",
    "      - Unidades esperadas (convenção MVP):\n",
    "        ν [m/s], δ [m], T [K], μ_gas [Pa·s], ρ_fuel [kg/m³], σ_mat e σ_steel (adimensionais/escalares).\n",
    "      - Os coeficientes (α, β, Tcrit, Topt, C_DELTA) são os \"knobs\" do MVP.\n",
    "      - Se insumos alternativos estiverem presentes (ex.: QN_total, T_gás, P), pode-se\n",
    "        preparar ν, μ_gas, etc., antes de chamar predict (ou criar um pré-processador).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha=1.0, beta=1.0,\n",
    "                 Tcrit=1400.0, Topt=1100.0,\n",
    "                 ):\n",
    "        self.alpha = float(alpha)\n",
    "        self.beta  = float(beta)\n",
    "        self.Tcrit = float(Tcrit)\n",
    "        self.Topt  = float(Topt)\n",
    "\n",
    "        # Campos (nomes canônicos) esperados no DataFrame de entrada:\n",
    "        # você pode alterar/expandir esse mapeamento se seus nomes forem diferentes.\n",
    "        self.required = [\n",
    "            \"nu_m_s\",             # ν (velocidade)\n",
    "            \"delta_eff_m\",        # δ (diâmetro/escala efetiva)\n",
    "            \"T_gas_K\",            # T (Kelvin)\n",
    "            \"mu_gas_Pa_s\",        # μ_gas\n",
    "            \"sigma_mat\",          # σ_mat\n",
    "            \"rho_fuel_kg_m3\",     # ρ_fuel\n",
    "            \"sigma_steel\",        # σ_steel\n",
    "        ]\n",
    "\n",
    "    # --- utilitários (opcionais) ---\n",
    "    @staticmethod\n",
    "    def sutherland_mu(TK, mu0=1.716e-5, T0=273.15, S=110.4):\n",
    "        \"\"\"Viscosidade do ar (Pa·s) pela lei de Sutherland (aproximação).\"\"\"\n",
    "        TK = np.clip(np.asarray(TK, dtype=float), 1.0, None)\n",
    "        return mu0 * (T0 + S)/(TK + S) * (TK/T0)**1.5\n",
    "\n",
    "    def predict(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Verifica insumos\n",
    "        missing = [c for c in self.required if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(\n",
    "                \"Faltam insumos para o modelo físico: \"\n",
    "                + \", \".join(missing)\n",
    "                + \".\\nAdapte os nomes/prepare os sinais antes de chamar .predict().\"\n",
    "            )\n",
    "\n",
    "        # Extrai vetores\n",
    "        nu   = pd.to_numeric(df[\"nu_m_s\"], errors=\"coerce\").values          # m/s\n",
    "        delt = pd.to_numeric(df[\"delta_eff_m\"], errors=\"coerce\").values     # m\n",
    "        TK   = pd.to_numeric(df[\"T_gas_K\"], errors=\"coerce\").values         # K\n",
    "        mu   = pd.to_numeric(df[\"mu_gas_Pa_s\"], errors=\"coerce\").values     # Pa·s\n",
    "        smat = pd.to_numeric(df[\"sigma_mat\"], errors=\"coerce\").values\n",
    "        rhoF = pd.to_numeric(df[\"rho_fuel_kg_m3\"], errors=\"coerce\").values  # kg/m³\n",
    "        sstl = pd.to_numeric(df[\"sigma_steel\"], errors=\"coerce\").values\n",
    "\n",
    "        # Fatores g e h\n",
    "        # Proteções para zero/NaN\n",
    "        rhoF = np.where(np.abs(rhoF) < 1e-12, np.nan, rhoF)\n",
    "        mu   = np.where(np.abs(mu)   < 1e-12, np.nan, mu)\n",
    "\n",
    "        g = smat / rhoF\n",
    "        h = sstl / mu\n",
    "\n",
    "        # Componentes\n",
    "        term_wr = (nu**2) * (delt**1.0) * np.exp(-TK / self.Tcrit) * g\n",
    "        term_wm = (nu**3) * np.sqrt(np.clip(delt, 0.0, None)) * np.exp(-TK / self.Topt) * h\n",
    "\n",
    "        # Saídas\n",
    "        Wr = self.alpha * term_wr\n",
    "        Wm = self.beta  * term_wm\n",
    "\n",
    "        out = pd.DataFrame({\n",
    "            \"wr_kg_m2_h_pred_phys\": Wr,\n",
    "            \"wm_kg_m2_h_pred_phys\": Wm,\n",
    "        })\n",
    "        return out\n",
    "\n",
    "# ============================================================\n",
    "# 2) (Opcional) Auto-mapeamento de colunas no rotulado GOLD\n",
    "#     -> tenta encontrar sinais físicos com heurísticas de nome\n",
    "# ============================================================\n",
    "def _auto_map_columns(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Heurística simples para mapear colunas do rotulado para os canônicos:\n",
    "      - nu_m_s:         r'(?:^|_)(nu|velocidade|velocity).*'\n",
    "      - delta_eff_m:    r'(?:^|_)(delta|diam(etro)?|d_eff).*'\n",
    "      - T_gas_K:        r'(?:^|_)(t_gas|temp.*gas|tk|t_k|t_gas_k).*'\n",
    "      - mu_gas_Pa_s:    r'(?:^|_)(mu_gas|viscos.*gas).*'\n",
    "      - sigma_mat:      r'(?:^|_)(sigma_mat|sig_mat|s_mat).*'\n",
    "      - rho_fuel_kg_m3: r'(?:^|_)(rho_fuel|dens.*comb|dens.*fuel).*'\n",
    "      - sigma_steel:    r'(?:^|_)(sigma_steel|sig_steel|s_steel).*'\n",
    "    \"\"\"\n",
    "    patterns = {\n",
    "        \"nu_m_s\":          re.compile(r'(?:^|_)(nu|velocidade|velocity)', re.I),\n",
    "        \"delta_eff_m\":     re.compile(r'(?:^|_)(delta|diam(?:etro)?|d_eff)', re.I),\n",
    "        \"T_gas_K\":         re.compile(r'(?:^|_)(t_gas|temp.*gas|tk\\b|t_k\\b|t_gas_k)', re.I),\n",
    "        \"mu_gas_Pa_s\":     re.compile(r'(?:^|_)(mu_gas|viscos.*gas)', re.I),\n",
    "        \"sigma_mat\":       re.compile(r'(?:^|_)(sigma_mat|sig_mat|s_mat)', re.I),\n",
    "        \"rho_fuel_kg_m3\":  re.compile(r'(?:^|_)(rho_fuel|dens.*comb|dens.*fuel)', re.I),\n",
    "        \"sigma_steel\":     re.compile(r'(?:^|_)(sigma_steel|sig_steel|s_steel)', re.I),\n",
    "    }\n",
    "    colmap = {}\n",
    "    cols = list(df.columns)\n",
    "    for target, pat in patterns.items():\n",
    "        for c in cols:\n",
    "            if pat.search(str(c)):\n",
    "                colmap[target] = c\n",
    "                break\n",
    "    return colmap\n",
    "\n",
    "# ============================================================\n",
    "# 3) Carrega rotulado GOLD (se existir) e tenta verificação\n",
    "# ============================================================\n",
    "df_rot = None\n",
    "colmap  = {}\n",
    "verified = False\n",
    "verification_stats = None\n",
    "sample_preds_path = None\n",
    "\n",
    "if os.path.exists(PATH_ROT_GOLD):\n",
    "    dfr = pd.read_csv(PATH_ROT_GOLD, header=[0,1], engine=\"python\")\n",
    "    df_rot = dfr.copy()\n",
    "    df_rot.columns = [c for (c,_) in dfr.columns]  # usa linha 1 (nomes)\n",
    "    # tenta mapear\n",
    "    colmap = _auto_map_columns(df_rot)\n",
    "    # renomeia cópia se houver mapeamento mínimo\n",
    "    needed = [\"nu_m_s\",\"delta_eff_m\",\"T_gas_K\",\"mu_gas_Pa_s\",\"sigma_mat\",\"rho_fuel_kg_m3\",\"sigma_steel\"]\n",
    "    has_min = all(k in colmap for k in needed if k not in [\"sigma_mat\",\"rho_fuel_kg_m3\",\"sigma_steel\"])  # tolera faltas nos escalares\n",
    "else:\n",
    "    has_min = False\n",
    "\n",
    "# ============================================================\n",
    "# 4) Instancia o modelo e (se der) verifica em amostra\n",
    "# ============================================================\n",
    "phys = PhysicsBasedModel(\n",
    "    alpha=1.0,   # deixe 1.0 — coeficientes do MVP podem ser ajustados depois se desejado\n",
    "    beta=1.0,\n",
    "    Tcrit=1400.0,\n",
    "    Topt=1100.0\n",
    ")\n",
    "\n",
    "if df_rot is not None and has_min:\n",
    "    # monta DF de entrada para o modelo físico\n",
    "    df_in = pd.DataFrame(index=df_rot.index)\n",
    "\n",
    "    def pick(name, default=None):\n",
    "        if name in colmap:\n",
    "            return pd.to_numeric(df_rot[colmap[name]], errors=\"coerce\")\n",
    "        else:\n",
    "            return pd.Series(default, index=df_rot.index, dtype=\"float64\")\n",
    "\n",
    "    df_in[\"nu_m_s\"]          = pick(\"nu_m_s\", np.nan)\n",
    "    df_in[\"delta_eff_m\"]     = pick(\"delta_eff_m\", np.nan)\n",
    "    df_in[\"T_gas_K\"]         = pick(\"T_gas_K\", np.nan)\n",
    "    df_in[\"mu_gas_Pa_s\"]     = pick(\"mu_gas_Pa_s\", phys.sutherland_mu(df_rot[colmap.get(\"T_gas_K\", df_rot.columns[0])]) if \"T_gas_K\" in colmap else np.nan)\n",
    "    df_in[\"sigma_mat\"]       = pick(\"sigma_mat\", 1.0)         # default neutro\n",
    "    df_in[\"rho_fuel_kg_m3\"]  = pick(\"rho_fuel_kg_m3\", 1.0)    # evita div/0\n",
    "    df_in[\"sigma_steel\"]     = pick(\"sigma_steel\", 1.0)\n",
    "\n",
    "    # predição física\n",
    "    preds = phys.predict(df_in)\n",
    "\n",
    "    # se rótulos estiverem no rotulado, podemos checar (Wr/Wm)\n",
    "    ycols = []\n",
    "    for y in [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]:\n",
    "        if y in df_rot.columns:\n",
    "            ycols.append(y)\n",
    "    if len(ycols) == 2:\n",
    "        err_wr = (pd.to_numeric(df_rot[\"wr_kg_m2_h\"], errors=\"coerce\") - preds[\"wr_kg_m2_h_pred_phys\"]).abs().median()\n",
    "        err_wm = (pd.to_numeric(df_rot[\"wm_kg_m2_h\"], errors=\"coerce\") - preds[\"wm_kg_m2_h_pred_phys\"]).abs().median()\n",
    "        verification_stats = {\"median_abs_err_wr\": float(err_wr), \"median_abs_err_wm\": float(err_wm)}\n",
    "        verified = True\n",
    "\n",
    "    # salva uma amostra pequena\n",
    "    sample = pd.concat([df_in.head(200), preds.head(200)], axis=1)\n",
    "    sample_preds_path = os.path.join(PHYS_DIR, \"sample_preds_phys.csv\")\n",
    "    sample.to_csv(sample_preds_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ============================================================\n",
    "# 5) Persistência: joblib + manifesto + card\n",
    "# ============================================================\n",
    "tag = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "phys_path = os.path.join(PHYS_DIR, \"model_phys_mvp.joblib\")\n",
    "joblib.dump({\n",
    "    \"model\": phys,\n",
    "    \"y_names\": [\"wr_kg_m2_h\",\"wm_kg_m2_h\"],\n",
    "    \"input_names_canonical\": phys.required,\n",
    "    \"notes\": \"Deterministic physics-based MVP. Provide inputs with canonical names/units.\",\n",
    "}, phys_path)\n",
    "\n",
    "manifest = {\n",
    "    \"tag\": \"MVP_PHYSICS\",\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "    \"freeze_dir\": FREEZE_DIR,\n",
    "    \"artifact\": os.path.basename(phys_path),\n",
    "    \"y_names\": [\"wr_kg_m2_h\",\"wm_kg_m2_h\"],\n",
    "    \"inputs_expected\": phys.required,\n",
    "    \"defaults\": {\"sigma_mat\": 1.0, \"rho_fuel_kg_m3\": 1.0, \"sigma_steel\": 1.0},\n",
    "    \"formulae\": {\n",
    "        \"Wr\": \"alpha * (nu^2) * (delta^1) * exp(-T/Tcrit) * (sigma_mat / rho_fuel)\",\n",
    "        \"Wm\": \"beta  * (nu^3) * sqrt(delta) * exp(-T/Topt) * (sigma_steel / mu_gas)\"\n",
    "    },\n",
    "    \"constants\": {\"alpha\": phys.alpha, \"beta\": phys.beta, \"Tcrit\": phys.Tcrit, \"Topt\": phys.Topt},\n",
    "    \"auto_mapping_used\": colmap if colmap else {},\n",
    "    \"verification\": {\n",
    "        \"performed\": bool(verified),\n",
    "        \"stats\": verification_stats,\n",
    "        \"sample_preds\": sample_preds_path if sample_preds_path else None\n",
    "    }\n",
    "}\n",
    "manifest_path = os.path.join(PHYS_DIR, \"manifest_phys_mvp.json\")\n",
    "with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Model card (curto)\n",
    "card = f\"\"\"\n",
    "# Projeto A1 — Model Card (Physics-Based MVP)\n",
    "\n",
    "## 1) Identificação\n",
    "- **Modelo:** PhysicsBasedModel (determinístico)\n",
    "- **Tag:** MVP_PHYSICS\n",
    "- **Freeze dir:** {FREEZE_DIR}\n",
    "- **Arquivo:** models\\\\phys\\\\model_phys_mvp.joblib\n",
    "- **Gerado em:** {datetime.now().date().isoformat()}\n",
    "\n",
    "## 2) Fórmulas\n",
    "- Wr = α · ν² · δ¹ · exp(−T/Tcrit) · (σ_mat/ρ_fuel)\n",
    "- Wm = β · ν³ · √δ · exp(−T/Topt) · (σ_steel/μ_gas)\n",
    "\n",
    "## 3) Insumos esperados (nomes canônicos)\n",
    "{', '.join(phys.required)}\n",
    "\n",
    "## 4) Constantes (MVP)\n",
    "α={phys.alpha}, β={phys.beta}, Tcrit={phys.Tcrit} K, Topt={phys.Topt} K\n",
    "\n",
    "## 5) Notas\n",
    "- Unidade esperada: ν[m/s], δ[m], T[K], μ_gas[Pa·s], ρ_fuel[kg/m³].\n",
    "- O artefato **não aprende**; aplica equações físicas. Calibração de α/β/Tcrit/Topt pode ser feita em futura versão.\n",
    "- Se o CSV tiver nomes diferentes, renomeie ou mapeie para os canônicos antes de inferir.\n",
    "\n",
    "## 6) Verificação\n",
    "- Executada: {\"Sim\" if verified else \"Não\"}\n",
    "- Estatísticas: {json.dumps(verification_stats) if verification_stats else \"—\"}\n",
    "- Amostra de predição: {sample_preds_path if sample_preds_path else \"—\"}\n",
    "\"\"\"\n",
    "card_path = os.path.join(PHYS_DIR, \"model_card_phys.md\")\n",
    "with open(card_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(card.strip()+\"\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# 6) Relatório final de caminhos (E0 e Físico)\n",
    "# ============================================================\n",
    "e0_model_path   = os.path.join(MVP_DIR, \"model_mvp.joblib\")\n",
    "e0_manifest     = os.path.join(MVP_DIR, \"manifest_mvp.json\")\n",
    "e0_card         = os.path.join(MVP_DIR, \"model_card.md\")\n",
    "\n",
    "print(\"======== ARTEFATOS SALVOS ========\")\n",
    "print(\"E0 (MVP supervisionado):\")\n",
    "print(\"  - Modelo:   \", e0_model_path)\n",
    "print(\"  - Manifesto:\", e0_manifest)\n",
    "print(\"  - Card:     \", e0_card)\n",
    "print(\"\")\n",
    "print(\"Modelo Físico (MVP):\")\n",
    "print(\"  - Modelo:   \", phys_path)\n",
    "print(\"  - Manifesto:\", manifest_path)\n",
    "print(\"  - Card:     \", card_path)\n",
    "if sample_preds_path:\n",
    "    print(\"  - Amostra preds:\", sample_preds_path)\n",
    "print(\"==================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
