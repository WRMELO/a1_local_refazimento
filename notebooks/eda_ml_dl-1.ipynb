{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f82941",
   "metadata": {},
   "source": [
    "# MVP — Predição de Desgaste (Wr/Wm) — Execução\n",
    "\n",
    "> **Granularidade temporal:** 1 amostra por **hora** (timestamps horários).  \n",
    "> **Dataset base:** `A1_ML_DL.csv` (2 linhas de cabeçalho: **[nome | dimensão]**).\n",
    "\n",
    "## Objetivo\n",
    "1) **Gerar Wr e Wm por hora** usando as variáveis físicas da **coluna 152 (`TAU_DENSA`) até a última** com base no equacionamento do PDF interno.  \n",
    "2) **Evitar vazamento:** as colunas usadas no cálculo (152→fim) **não** entram como features para o modelo.  \n",
    "3) Produzir dois arquivos:\n",
    "   - `A1_ML_DL_rotulado.csv` → contém todas as colunas **+** `wr_kg_m2_h`, `wm_kg_m2_h` (2ª linha preserva dimensões).\n",
    "   - `A1_ML_DL_features.csv` → versão para modelagem **sem** as colunas 152→fim (mantém Wr/Wm como alvos).\n",
    "4) **EDA rápida** dos rótulos (distribuição, nulos, estatísticas).\n",
    "\n",
    "## Por que **não** embaralhar (shuffle)?\n",
    "É **série temporal**. Embaralhar traria informação do **futuro** para o treino (vazamento) e deixaria as métricas **otimistas**.  \n",
    "Correto: separação por **blocos de tempo** (Treino → Validação → Teste) e, se possível, **backtesting** (janela móvel).\n",
    "\n",
    "## Rotas de modelagem (resumo)\n",
    "- **Supervisionada (MVP principal):** prever `Wr(t+H)` e `Wm(t+H)` com:\n",
    "  - **Tabular** (XGBoost/LightGBM) usando features agregadas por janela (W=1–4h; H=1–4h).\n",
    "  - **Sequencial** (TCN/GRU) usando a sequência horária normalizada.\n",
    "- **Não-supervisionada (radar complementar):** PCA+monitoramento, Autoencoder sequencial, change-points, clustering de janelas — para detectar **mudanças de regime** e **anomalias** que antecedem aumento de desgaste.\n",
    "\n",
    "## Convenções\n",
    "- Leitura do CSV: `header=[0,1]`.  \n",
    "- 1ª linha: **nomes** | 2ª linha: **dimensões**.  \n",
    "- A partir de **`TAU_DENSA`** (coluna **152**, 1-indexado) até o final → **apenas** para calcular Wr/Wm.  \n",
    "- **Busca semântica ativada**: quando você citar um nome de variável, o pipeline tenta localizar a coluna correspondente (PT/EN) sem exigir igualdade literal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662d1f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Início do bloco físico para cálculo: coluna #151 -> tau_densa (até o final). Total usadas: 16\n",
      "\n",
      "[Mapeamento] Colunas encontradas:\n",
      "  - tau_main    : leito_temp_average\n",
      "  - temp_leito  : leito_temp_average\n",
      "  - air_total   : air_total_knm3_h\n",
      "  - air_primary : air_primary_knm3_h\n",
      "  - air_secondary: air_secondary_knm3_h\n",
      "  - dp_furn     : pressao_fornalha\n",
      "  - p_furn_a    : pressao_fornalha_a_inf\n",
      "  - p_furn_b    : pressao_fornalha_b_inf\n",
      "  - p_abs       : pressao_fornalha\n",
      "  - rho_co2     : o2_medio\n",
      "  - o2          : o2_medio\n",
      "\n",
      "[Leak-guard] Colunas usadas no cálculo Wr/Wm (serão EXCLUÍDAS das features):\n",
      "  - air_primary_knm3_h\n",
      "  - air_primary_nm3_h\n",
      "  - air_primary_press_z\n",
      "  - air_primary_share_pct\n",
      "  - air_secondary_knm3_h\n",
      "  - air_secondary_nm3_h\n",
      "  - air_secondary_press_z\n",
      "  - air_total_knm3_h\n",
      "  - air_total_nm3_h\n",
      "  - coal_flow_furnace_t_h\n",
      "  - coal_flow_total_ref_t_h\n",
      "  - leito_temp_average\n",
      "  - o2_excess_pct\n",
      "  - o2_medio\n",
      "  - pressao_fornalha\n",
      "  - pressao_fornalha_a_inf\n",
      "  - pressao_fornalha_b_inf\n",
      "  - tau_backpass\n",
      "  - tau_densa\n",
      "  - tau_diluida\n",
      "  - tau_global\n",
      "\n",
      "✅ Rotulado salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_rotulado.csv\n",
      "✅ Features (SEM colunas usadas no cálculo) salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_features.csv\n",
      "\n",
      "--- wr_kg_m2_h ---\n",
      "N válidos: 4745 | N nulos: 7012\n",
      "count    4.745000e+03\n",
      "mean     3.989668e+05\n",
      "std      4.117409e+05\n",
      "min      0.000000e+00\n",
      "1%       0.000000e+00\n",
      "5%       1.908192e+02\n",
      "25%      6.790607e+03\n",
      "50%      5.682445e+04\n",
      "75%      8.094846e+05\n",
      "95%      9.393033e+05\n",
      "99%      1.065432e+06\n",
      "max      1.698249e+06\n",
      "\n",
      "--- wm_kg_m2_h ---\n",
      "N válidos: 4636 | N nulos: 7121\n",
      "count    4.636000e+03\n",
      "mean     3.968066e+12\n",
      "std      7.858238e+12\n",
      "min      9.712316e+05\n",
      "1%       2.072525e+10\n",
      "5%       1.932416e+11\n",
      "25%      1.499840e+12\n",
      "50%      3.997710e+12\n",
      "75%      4.606478e+12\n",
      "95%      7.728654e+12\n",
      "99%      2.626260e+13\n",
      "max      4.483858e+14\n",
      "\n",
      "Concluído.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# (1) IMPORTS & CONFIGURAÇÃO GERAL\n",
    "# ==============================================================\n",
    "import os, re, unicodedata, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Caminho base (ajuste se necessário)\n",
    "CURATED_DIR = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\"\n",
    "PATH_IN   = os.path.join(CURATED_DIR, \"A1_ML_DL.csv\")\n",
    "PATH_OUTL = os.path.join(CURATED_DIR, \"A1_ML_DL_rotulado.csv\")\n",
    "PATH_OUTF = os.path.join(CURATED_DIR, \"A1_ML_DL_features.csv\")\n",
    "\n",
    "# Constantes físicas e padrões\n",
    "T_N      = 273.15      # K\n",
    "P_N      = 1.01325e5   # Pa\n",
    "R        = 8.314462618 # J/mol/K\n",
    "M_AIR    = 0.029       # kg/mol\n",
    "M_CO2    = 0.044       # kg/mol\n",
    "\n",
    "# Parâmetros de engenharia (ajustáveis conforme planta)\n",
    "A_EF     = 15.0        # m²  (área efetiva)\n",
    "L_REF    = 3.0         # m   (altura efetiva para ΔP)\n",
    "BETA_DP  = 0.25        # expoente fraco para ajuste por ΔP\n",
    "GAMMA_DEFAULT = 0.25   # fração CO2 padrão, se não houver coluna\n",
    "C_DELTA  = 1e-3        # ganho para trazer δ para ~0.1–0.3 mm (m)\n",
    "\n",
    "# Constantes BASE (Wr/Wm)\n",
    "ALPHA_BASE = 4.2e-3\n",
    "BETA_BASE  = 1.2e-4\n",
    "TCRIT_BASE = 1200.0\n",
    "TOPT_BASE  = 1250.0\n",
    "SIGMA_MAT_BASE   = 6.0e9\n",
    "RHO_FUEL_BASE    = 700.0\n",
    "SIGMA_STEEL_BASE = 2.5e9\n",
    "\n",
    "# ==============================================================\n",
    "# (2) FUNÇÕES AUXILIARES — NORMALIZAÇÃO & SIMILARIDADE\n",
    "# ==============================================================\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFKD\", str(s)) if not unicodedata.combining(c))\n",
    "\n",
    "def normalize_token(s: str) -> str:\n",
    "    s = strip_accents(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9_\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def split_tokens(s: str):\n",
    "    return set(t for t in re.split(r\"[ _]+\", normalize_token(s)) if t)\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "def hybrid_similarity(a: str, b: str) -> float:\n",
    "    # mistura jaccard (tokens) + razão de sequência (robusta a variações)\n",
    "    ta, tb = split_tokens(a), split_tokens(b)\n",
    "    jac = len(ta & tb) / len(ta | tb) if (ta and tb) else 0.0\n",
    "    seq = SequenceMatcher(None, normalize_token(a), normalize_token(b)).ratio()\n",
    "    return 0.6*jac + 0.4*seq\n",
    "\n",
    "def best_match(query: str, candidates: list):\n",
    "    scores = [(c, hybrid_similarity(query, c)) for c in candidates]\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[0] if scores else (None, 0.0)\n",
    "\n",
    "# ==============================================================\n",
    "# (3) LEITURA DO DATASET (2 CABEÇALHOS) & DICIONÁRIO DE DIMENSÕES\n",
    "# ==============================================================\n",
    "df_raw = pd.read_csv(PATH_IN, header=[0,1], engine=\"python\")\n",
    "dim_por_col = {col: dim for (col, dim) in df_raw.columns}  # {nome: dimensão}\n",
    "df = df_raw.copy()\n",
    "df.columns = [col for (col, dim) in df_raw.columns]        # “achata” para os nomes\n",
    "colunas = list(df.columns)\n",
    "\n",
    "# Localiza início do bloco físico (TAU_DENSA->fim). Se não existir, fallback = posição 152 (1-indexado)\n",
    "if \"TAU_DENSA\" in colunas:\n",
    "    idx_tau = colunas.index(\"TAU_DENSA\")\n",
    "else:\n",
    "    c, s = best_match(\"tau_densa\", colunas)\n",
    "    idx_tau = colunas.index(c) if c else 151  # 151 (0-index) ~ 152 (1-index)\n",
    "print(f\"Início do bloco físico para cálculo: coluna #{idx_tau+1} -> {colunas[idx_tau]} (até o final). Total usadas: {len(colunas[idx_tau:])}\")\n",
    "\n",
    "# ==============================================================\n",
    "# (4) MAPEAMENTO SEMÂNTICO ROBUSTO (com dicas por dimensão/região)\n",
    "# ==============================================================\n",
    "def _dim_text(colname: str) -> str:\n",
    "    return str(dim_por_col.get(colname, \"\") or \"\").lower()\n",
    "\n",
    "# Dicas por dimensão (2ª linha do cabeçalho)\n",
    "DIM_HINTS = {\n",
    "    \"temp\": [\"°c\",\" c\",\"k\",\"°k\",\"temp\",\"leito\"],\n",
    "    \"flowN\": [\"nm3/h\",\"knm3/h\",\"mn3/h\",\"nm3 h\",\"knm3 h\"],\n",
    "    \"press\": [\"pa\",\"kpa\",\"bar\",\"mbar\",\"mpa\",\"press\",\"dp\"],\n",
    "    \"co2rho\": [\"kg/m3\",\"kg/m³\",\"densidade\",\"co2\"]\n",
    "}\n",
    "\n",
    "# Aliases ampliados (PT/EN/siglas)\n",
    "ALIASES = {\n",
    "    \"tau_main\": [\"tau_densa\",\"tau\",\"t_leito\",\"leito_temp_average\",\"bed_temp\",\"temperatura\"],\n",
    "    \"temp_leito\": [\"leito_temp_average\",\"bed_temp\",\"temperatura_leito\",\"fornalha_leito_temp_average\"],\n",
    "    \"air_total\": [\"air_total_knm3_h\",\"air_total_nm3_h\",\"air_total_mn3_h\",\"air_total\"],\n",
    "    \"air_primary\": [\"air_primary_knm3_h\",\"air_primary_nm3_h\",\"air_primary\"],\n",
    "    \"air_secondary\":[\"air_secondary_knm3_h\",\"air_secondary_nm3_h\",\"air_secondary\"],\n",
    "    \"dp_furn\": [\"dp_fornalha\",\"delta_p_fornalha\",\"fornalha_dp\",\"dp_furnace\"],\n",
    "    \"p_furn_a\": [\"pressao_fornalha_a\",\"furnace_a_pressure\",\"fornalha_a_press\"],\n",
    "    \"p_furn_b\": [\"pressao_fornalha_b\",\"furnace_b_pressure\",\"fornalha_b_press\"],\n",
    "    \"p_abs\": [\"pressao_fornalha\",\"furnace_pressure\",\"pressao_plenum\",\"pressao_leito\",\"pressao_tambor\"],\n",
    "    \"rho_co2\": [\"densidade_co2\",\"co2_density\",\"rho_co2\"],\n",
    "    \"o2\": [\"o2_medio\",\"o2_excess_pct\",\"oxygen\"]\n",
    "}\n",
    "\n",
    "def _choose(qs, kind_hint=None):\n",
    "    best, best_s = None, -1.0\n",
    "    for q in qs:\n",
    "        c, s = best_match(q, colunas)\n",
    "        if c:\n",
    "            # bônus por dimensão, se solicitado\n",
    "            if kind_hint == \"temp\" and any(h in _dim_text(c) for h in DIM_HINTS[\"temp\"]): s += 0.12\n",
    "            if kind_hint == \"flowN\" and any(h in _dim_text(c) for h in DIM_HINTS[\"flowN\"]): s += 0.12\n",
    "            if kind_hint == \"press\" and any(h in _dim_text(c) for h in DIM_HINTS[\"press\"]): s += 0.12\n",
    "            if s > best_s:\n",
    "                best, best_s = c, s\n",
    "    return best, best_s\n",
    "\n",
    "tau_main, _       = _choose(ALIASES[\"tau_main\"], kind_hint=\"temp\")\n",
    "temp_leito_col, _ = _choose(ALIASES[\"temp_leito\"], kind_hint=\"temp\")\n",
    "air_total_col,_   = _choose(ALIASES[\"air_total\"], kind_hint=\"flowN\")\n",
    "air_pri_col,_     = _choose(ALIASES[\"air_primary\"], kind_hint=\"flowN\")\n",
    "air_sec_col,_     = _choose(ALIASES[\"air_secondary\"], kind_hint=\"flowN\")\n",
    "dp_furn_col,_     = _choose(ALIASES[\"dp_furn\"], kind_hint=\"press\")\n",
    "p_furn_a_col,_    = _choose(ALIASES[\"p_furn_a\"], kind_hint=\"press\")\n",
    "p_furn_b_col,_    = _choose(ALIASES[\"p_furn_b\"], kind_hint=\"press\")\n",
    "p_abs_col,_       = _choose(ALIASES[\"p_abs\"], kind_hint=\"press\")\n",
    "rho_co2_col,_     = _choose(ALIASES[\"rho_co2\"], kind_hint=None)\n",
    "o2_col,_          = _choose(ALIASES[\"o2\"], kind_hint=None)\n",
    "\n",
    "print(\"\\n[Mapeamento] Colunas encontradas:\")\n",
    "for label, name in [\n",
    "    (\"tau_main\", tau_main), (\"temp_leito\", temp_leito_col),\n",
    "    (\"air_total\", air_total_col), (\"air_primary\", air_pri_col), (\"air_secondary\", air_sec_col),\n",
    "    (\"dp_furn\", dp_furn_col), (\"p_furn_a\", p_furn_a_col), (\"p_furn_b\", p_furn_b_col),\n",
    "    (\"p_abs\", p_abs_col), (\"rho_co2\", rho_co2_col), (\"o2\", o2_col)\n",
    "]:\n",
    "    print(f\"  - {label:12s}: {name}\")\n",
    "\n",
    "# ==============================================================\n",
    "# (5) PROXIES DIMENSIONAIS + CÁLCULO Wr/Wm\n",
    "# ==============================================================\n",
    "def _to_pa(x, dimtxt):\n",
    "    s = (dimtxt or \"\").lower()\n",
    "    v = pd.to_numeric(x, errors=\"coerce\")\n",
    "    if \"mpa\" in s:   return v * 1e6\n",
    "    if \"kpa\" in s:   return v * 1e3\n",
    "    if \"bar\" in s:   return v * 1.0e5\n",
    "    if \"mbar\" in s:  return v * 1.0e2\n",
    "    return v  # assume Pa\n",
    "\n",
    "def _tk_from_c_or_k(series, dimtxt, fallback_name=\"\"):\n",
    "    # regra adicional: se o nome contiver 'tau' e a dimensão for desconhecida, assumir °C (seu comentário)\n",
    "    if ((\"°c\" in (dimtxt or \"\").lower()) or ((\"c\" in (dimtxt or \"\").lower() and \"k\" not in (dimtxt or \"\").lower()))\n",
    "        or (\"tau\" in fallback_name.lower() and not dimtxt)):\n",
    "        return pd.to_numeric(series, errors=\"coerce\") + 273.15\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "# --- Temperatura em Kelvin (TK) ---\n",
    "if tau_main:\n",
    "    TK = _tk_from_c_or_k(df[tau_main], _dim_text(tau_main), fallback_name=tau_main)\n",
    "elif temp_leito_col:\n",
    "    TK = _tk_from_c_or_k(df[temp_leito_col], _dim_text(temp_leito_col), fallback_name=temp_leito_col)\n",
    "else:\n",
    "    raise RuntimeError(\"Não encontrei coluna de temperatura (TAU/LEITO). Ajuste aliases ou dimensões.\")\n",
    "\n",
    "# --- ΔP e P_abs ---\n",
    "if dp_furn_col:\n",
    "    DP = _to_pa(df[dp_furn_col], _dim_text(dp_furn_col))\n",
    "elif p_furn_a_col and p_furn_b_col:\n",
    "    Pa = _to_pa(df[p_furn_a_col], _dim_text(p_furn_a_col))\n",
    "    Pb = _to_pa(df[p_furn_b_col], _dim_text(p_furn_b_col))\n",
    "    DP = (Pa - Pb).abs()\n",
    "else:\n",
    "    DP = pd.Series(np.nan, index=df.index)\n",
    "\n",
    "if p_abs_col:\n",
    "    Pabs = _to_pa(df[p_abs_col], _dim_text(p_abs_col))\n",
    "    # se for muito baixa (aparente gauge), somar atmosfera\n",
    "    if Pabs.median(skipna=True) < 2e5:\n",
    "        Pabs = Pabs + P_N\n",
    "else:\n",
    "    Pabs = pd.Series(P_N, index=df.index)\n",
    "\n",
    "# --- Vazão real (m3/s) a partir de Nm3/h ---\n",
    "QN_total = None\n",
    "if air_total_col:\n",
    "    QN_total = pd.to_numeric(df[air_total_col], errors=\"coerce\")\n",
    "else:\n",
    "    parts = []\n",
    "    if air_pri_col: parts.append(pd.to_numeric(df[air_pri_col], errors=\"coerce\"))\n",
    "    if air_sec_col: parts.append(pd.to_numeric(df[air_sec_col], errors=\"coerce\"))\n",
    "    if parts:\n",
    "        QN_total = sum(parts)\n",
    "\n",
    "if QN_total is not None:\n",
    "    Q_real = (QN_total * 1000.0 / 3600.0) * (TK / T_N) * (P_N / Pabs)\n",
    "else:\n",
    "    Q_real = pd.Series(np.nan, index=df.index)\n",
    "\n",
    "# --- Velocidade ν (m/s) ---\n",
    "nu_base = Q_real / A_EF\n",
    "DP_ref  = np.nanmedian(DP) if np.isfinite(DP).any() else 1.0\n",
    "nu_proxy = nu_base * np.power(np.maximum(DP, 1.0) / max(DP_ref, 1.0), BETA_DP)\n",
    "\n",
    "# --- Fração de CO2 (γ) por densidade CO2, se houver ---\n",
    "if rho_co2_col:\n",
    "    rho_co2_meas = pd.to_numeric(df[rho_co2_col], errors=\"coerce\")\n",
    "    rho_co2_pure = (Pabs * M_CO2) / (R * TK)\n",
    "    gamma = np.clip(rho_co2_meas / np.maximum(rho_co2_pure, 1e-9), 0.0, 1.0)\n",
    "else:\n",
    "    gamma = pd.Series(GAMMA_DEFAULT, index=df.index)\n",
    "\n",
    "# --- Densidade do gás ρg (kg/m3) ---\n",
    "M_mix = (1 - gamma) * M_AIR + gamma * M_CO2\n",
    "rho_g = (Pabs * M_mix) / (R * TK)\n",
    "\n",
    "# --- Viscosidade μ_gas (Pa·s) via Sutherland (ar/CO2) & mistura ---\n",
    "def mu_air_suth(TK):\n",
    "    mu0, T0, S = 1.716e-5, 273.15, 111.0\n",
    "    return mu0 * ((T0 + S) / (TK + S)) * (TK / T0) ** 1.5\n",
    "\n",
    "def mu_co2_suth(TK):\n",
    "    mu0, T0, S = 1.37e-5, 273.15, 240.0  # aproximação\n",
    "    return mu0 * ((T0 + S) / (TK + S)) * (TK / T0) ** 1.5\n",
    "\n",
    "mu_gas = (1 - gamma) * mu_air_suth(TK) + gamma * mu_co2_suth(TK)\n",
    "\n",
    "# --- Diâmetro δ (m) por proxy dimensional (Ergun-like) ---\n",
    "epsP = 1.0  # Pa\n",
    "delta_proxy = (rho_g * (nu_proxy ** 2) * L_REF) / np.maximum(DP, epsP)   # [m]\n",
    "delta_m = C_DELTA * delta_proxy  # ganho para faixa ~0.1–0.3 mm\n",
    "\n",
    "# --- Fatores adimensionais g e h ---\n",
    "sigma_mat   = pd.Series(SIGMA_MAT_BASE,   index=df.index)  # sem coluna específica → BASE\n",
    "rho_fuel    = pd.Series(RHO_FUEL_BASE,    index=df.index)\n",
    "sigma_steel = pd.Series(SIGMA_STEEL_BASE, index=df.index)\n",
    "\n",
    "g = sigma_mat / np.maximum(rho_fuel, 1e-12)\n",
    "h = sigma_steel / np.maximum(mu_gas,  1e-12)\n",
    "\n",
    "# --- EQUAÇÕES Wr/Wm ---\n",
    "nu_pos    = np.maximum(nu_proxy.fillna(0.0), 0.0)\n",
    "delta_pos = np.maximum(delta_m.fillna(np.nan), 1e-9)\n",
    "TK_pos    = np.maximum(TK.fillna(np.nan), 1.0)\n",
    "\n",
    "wr = (ALPHA_BASE * (nu_pos ** 2.0) * (delta_pos ** 1.0) * np.exp(-TK_pos / TCRIT_BASE) * g).astype(float)\n",
    "wm = (BETA_BASE  * (nu_pos ** 3.0) * np.sqrt(delta_pos)    * np.exp(-TK_pos / TOPT_BASE)  * h).astype(float)\n",
    "\n",
    "wr_dim = \"kg/m2·h\"\n",
    "wm_dim = \"kg/m2·h\"\n",
    "\n",
    "# ==============================================================\n",
    "# (6) SALVAR ARQUIVOS + LEAK-GUARD + EDA RESUMO\n",
    "# ==============================================================\n",
    "# DataFrame rotulado\n",
    "df_labeled = df.copy()\n",
    "df_labeled[\"wr_kg_m2_h\"] = wr\n",
    "df_labeled[\"wm_kg_m2_h\"] = wm\n",
    "\n",
    "# Atualiza dicionário de dimensões\n",
    "dim_por_col[\"wr_kg_m2_h\"] = wr_dim\n",
    "dim_por_col[\"wm_kg_m2_h\"] = wm_dim\n",
    "\n",
    "# -- Leak-guard: todas as colunas usadas para targets + bloco físico (TAU_DENSA..fim)\n",
    "used_cols_for_targets = set()\n",
    "for name in [tau_main, temp_leito_col, air_total_col, air_pri_col, air_sec_col,\n",
    "             dp_furn_col, p_furn_a_col, p_furn_b_col, p_abs_col, rho_co2_col, o2_col]:\n",
    "    if name: used_cols_for_targets.add(name)\n",
    "used_cols_for_targets.update(colunas[idx_tau:])\n",
    "\n",
    "print(\"\\n[Leak-guard] Colunas usadas no cálculo Wr/Wm (serão EXCLUÍDAS das features):\")\n",
    "for c in sorted(used_cols_for_targets):\n",
    "    print(\"  -\", c)\n",
    "\n",
    "# -- Salvar A1_ML_DL_rotulado.csv (2 cabeçalhos) --\n",
    "with open(PATH_OUTL, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    f.write(\",\".join(df_labeled.columns) + \"\\n\")\n",
    "    f.write(\",\".join([str(dim_por_col.get(c, \"\")) for c in df_labeled.columns]) + \"\\n\")\n",
    "df_labeled.to_csv(PATH_OUTL, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n✅ Rotulado salvo: {PATH_OUTL}\")\n",
    "\n",
    "# -- Montar A1_ML_DL_features.csv (REMOVENDO o leak) --\n",
    "orig_cols = [c for c in df.columns if c not in [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]]\n",
    "cols_excluir = set(colunas[idx_tau:]).union(used_cols_for_targets)\n",
    "cols_keep = [c for c in orig_cols if c not in cols_excluir] + [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]\n",
    "df_features = df_labeled[cols_keep].copy()\n",
    "\n",
    "with open(PATH_OUTF, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    f.write(\",\".join(df_features.columns) + \"\\n\")\n",
    "    f.write(\",\".join([str(dim_por_col.get(c, \"\")) for c in df_features.columns]) + \"\\n\")\n",
    "df_features.to_csv(PATH_OUTF, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Features (SEM colunas usadas no cálculo) salvo: {PATH_OUTF}\")\n",
    "\n",
    "# -- EDA resumo dos rótulos --\n",
    "def _eda_rapida(s: pd.Series, nome: str):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    print(f\"\\n--- {nome} ---\")\n",
    "    print(\"N válidos:\", s.notna().sum(), \"| N nulos:\", s.isna().sum())\n",
    "    desc = s.describe(percentiles=[0.01,0.05,0.25,0.5,0.75,0.95,0.99])\n",
    "    print(desc.to_string())\n",
    "\n",
    "_eda_rapida(df_labeled[\"wr_kg_m2_h\"], \"wr_kg_m2_h\")\n",
    "_eda_rapida(df_labeled[\"wm_kg_m2_h\"], \"wm_kg_m2_h\")\n",
    "\n",
    "print(\"\\nConcluído.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edeee97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Início do bloco físico para cálculo: coluna #151 -> tau_densa (até o final). Total usadas: 16\n",
      "\n",
      "[Mapeamento] Colunas encontradas:\n",
      "  - tau_main    : tau_densa\n",
      "  - temp_leito  : leito_temp_average\n",
      "  - air_total   : air_total_knm3_h\n",
      "  - air_primary : air_primary_knm3_h\n",
      "  - air_secondary: air_secondary_knm3_h\n",
      "  - dp_furn     : pressao_fornalha\n",
      "  - p_furn_a    : pressao_fornalha_a_inf\n",
      "  - p_furn_b    : pressao_fornalha_b_inf\n",
      "  - p_abs       : pressao_fornalha\n",
      "  - rho_co2     : o2_medio\n",
      "  - o2          : o2_medio\n",
      "\n",
      "[QN_total] NaNs (total): 4284 | após prim+sec: 4284 | após blocos: 1745\n",
      "\n",
      "[FIX-WM] Verificando mapeamento de rho_co2...\n",
      "  - rho_co2 estava mapeado para O2. Ignorando e usando GAMMA_DEFAULT.\n",
      "✅ Rotulado (Wm corrigido) salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_rotulado_v3.csv\n",
      "✅ Features (v3, sem vazamento) salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_features_v3.csv\n",
      "\n",
      "--- wr_kg_m2_h (v3) ---\n",
      "N válidos: 10005 | N nulos: 1752\n",
      "count    1.000500e+04\n",
      "mean     9.367193e-08\n",
      "std      2.664596e-07\n",
      "min      0.000000e+00\n",
      "1%       0.000000e+00\n",
      "5%       0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      2.380783e-09\n",
      "95%      7.008619e-07\n",
      "99%      1.248027e-06\n",
      "max      4.542722e-06\n",
      "\n",
      "--- wm_kg_m2_h (v3) ---\n",
      "N válidos: 3496 | N nulos: 8261\n",
      "count    3.496000e+03\n",
      "mean     1.484755e+05\n",
      "std      1.889328e+05\n",
      "min      6.191371e-05\n",
      "1%       1.404785e-01\n",
      "5%       4.674247e+00\n",
      "25%      2.400021e+02\n",
      "50%      2.538008e+04\n",
      "75%      2.844884e+05\n",
      "95%      5.170181e+05\n",
      "99%      6.576644e+05\n",
      "max      1.486353e+06\n",
      "\n",
      "[FIX-WM v4] Robustecendo μ_gas e Wm...\n",
      "  - μ_gas NaNs após fix: 0\n",
      "✅ Rotulado (v4) salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_rotulado_v4.csv\n",
      "✅ Features (v4, sem vazamento) salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_features_v4.csv\n",
      "\n",
      "--- wr_kg_m2_h (v4) ---\n",
      "N válidos: 10005 | N nulos: 1752\n",
      "count    1.000500e+04\n",
      "mean     9.367193e-08\n",
      "std      2.664596e-07\n",
      "min      0.000000e+00\n",
      "1%       0.000000e+00\n",
      "5%       0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      2.380783e-09\n",
      "95%      7.008619e-07\n",
      "99%      1.248027e-06\n",
      "max      4.542722e-06\n",
      "\n",
      "--- wm_kg_m2_h (v4) ---\n",
      "N válidos: 10005 | N nulos: 1752\n",
      "count    1.000500e+04\n",
      "mean     5.188108e+04\n",
      "std      1.322216e+05\n",
      "min      0.000000e+00\n",
      "1%       0.000000e+00\n",
      "5%       0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      4.240104e+02\n",
      "95%      3.840107e+05\n",
      "99%      5.725709e+05\n",
      "max      1.486353e+06\n",
      "\n",
      "[Leak-guard] Colunas a excluir das features:\n",
      "  - air_primary_knm3_h\n",
      "  - air_primary_nm3_h\n",
      "  - air_primary_press_z\n",
      "  - air_primary_share_pct\n",
      "  - air_secondary_knm3_h\n",
      "  - air_secondary_nm3_h\n",
      "  - air_secondary_press_z\n",
      "  - air_total_knm3_h\n",
      "  - air_total_knm3_h_filled\n",
      "  - air_total_nm3_h\n",
      "  - coal_flow_furnace_t_h\n",
      "  - coal_flow_total_ref_t_h\n",
      "  - leito_temp_average\n",
      "  - o2_excess_pct\n",
      "  - o2_medio\n",
      "  - pressao_fornalha\n",
      "  - pressao_fornalha_a_inf\n",
      "  - pressao_fornalha_b_inf\n",
      "  - tau_backpass\n",
      "  - tau_densa\n",
      "  - tau_diluida\n",
      "  - tau_global\n",
      "\n",
      "✅ Rotulado salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_rotulado_v2.csv\n",
      "✅ Features (sem vazamento) salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_features_v2.csv\n",
      "\n",
      "--- wr_kg_m2_h ---\n",
      "N válidos: 10005 | N nulos: 1752\n",
      "count    1.000500e+04\n",
      "mean     9.367193e-08\n",
      "std      2.664596e-07\n",
      "min      0.000000e+00\n",
      "1%       0.000000e+00\n",
      "5%       0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      2.380783e-09\n",
      "95%      7.008619e-07\n",
      "99%      1.248027e-06\n",
      "max      4.542722e-06\n",
      "\n",
      "--- wm_kg_m2_h ---\n",
      "N válidos: 10005 | N nulos: 1752\n",
      "count    1.000500e+04\n",
      "mean     5.188108e+04\n",
      "std      1.322216e+05\n",
      "min      0.000000e+00\n",
      "1%       0.000000e+00\n",
      "5%       0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      4.240104e+02\n",
      "95%      3.840107e+05\n",
      "99%      5.725709e+05\n",
      "max      1.486353e+06\n",
      "\n",
      "Concluído.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# (1) IMPORTS & CONFIGURAÇÃO\n",
    "# ==============================================================\n",
    "import os, re, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Caminhos\n",
    "CURATED_DIR = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\"\n",
    "PATH_IN     = os.path.join(CURATED_DIR, \"A1_ML_DL.csv\")\n",
    "PATH_OUTL   = os.path.join(CURATED_DIR, \"A1_ML_DL_rotulado_v2.csv\")\n",
    "PATH_OUTF   = os.path.join(CURATED_DIR, \"A1_ML_DL_features_v2.csv\")\n",
    "\n",
    "# Constantes físicas\n",
    "T_N, P_N = 273.15, 1.01325e5   # K, Pa\n",
    "R = 8.314462618                # J/mol/K\n",
    "M_AIR, M_CO2 = 0.029, 0.044    # kg/mol\n",
    "\n",
    "# Parâmetros de engenharia (ajuste se conhecer a planta)\n",
    "A_EF   = 15.0     # m²  (área efetiva)\n",
    "L_REF  = 3.0      # m   (altura efetiva para DP)\n",
    "BETA_DP = 0.25    # expoente fraco para ajuste por ΔP\n",
    "C_DELTA = 1e-3    # ganho para trazer δ para ~0.1–0.3 mm (m)\n",
    "GAMMA_DEFAULT = 0.25  # fração CO2 padrão (se não houver dado)\n",
    "\n",
    "# Constantes BASE de Wr/Wm\n",
    "ALPHA_BASE = 4.2e-3\n",
    "BETA_BASE  = 1.2e-4\n",
    "TCRIT_BASE = 1200.0\n",
    "TOPT_BASE  = 1250.0\n",
    "SIGMA_MAT_BASE   = 6.0e9\n",
    "RHO_FUEL_BASE    = 700.0\n",
    "SIGMA_STEEL_BASE = 2.5e9\n",
    "\n",
    "# ==============================================================\n",
    "# (2) FUNÇÕES AUXILIARES\n",
    "# ==============================================================\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFKD\", str(s)) if not unicodedata.combining(c))\n",
    "\n",
    "def normalize_token(s: str) -> str:\n",
    "    s = strip_accents(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9_\\s]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def split_tokens(s: str):\n",
    "    return set(t for t in re.split(r\"[ _]+\", normalize_token(s)) if t)\n",
    "\n",
    "def hybrid_similarity(a: str, b: str) -> float:\n",
    "    ta, tb = split_tokens(a), split_tokens(b)\n",
    "    jac = len(ta & tb) / len(ta | tb) if (ta and tb) else 0.0\n",
    "    seq = SequenceMatcher(None, normalize_token(a), normalize_token(b)).ratio()\n",
    "    return 0.6*jac + 0.4*seq\n",
    "\n",
    "def best_match(query: str, candidates: list):\n",
    "    scores = [(c, hybrid_similarity(query, c)) for c in candidates]\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[0] if scores else (None, 0.0)\n",
    "\n",
    "def dim_text(colname: str) -> str:\n",
    "    return str(dim_por_col.get(colname, \"\") or \"\").lower()\n",
    "\n",
    "def to_pa(x, dimtxt):\n",
    "    s = (dimtxt or \"\").lower()\n",
    "    v = pd.to_numeric(x, errors=\"coerce\")\n",
    "    if \"mpa\" in s:   return v * 1e6\n",
    "    if \"kpa\" in s:   return v * 1e3\n",
    "    if \"bar\" in s:   return v * 1.0e5\n",
    "    if \"mbar\" in s:  return v * 1.0e2\n",
    "    return v  # assume Pa\n",
    "\n",
    "def TK_from_series(series, dimtxt, fallback_name=\"\"):\n",
    "    # Se for °C (ou nome contém 'tau' e dimensão ausente), converte para K\n",
    "    is_c = (\"°c\" in (dimtxt or \"\").lower()) or ((\"c\" in (dimtxt or \"\").lower() and \"k\" not in (dimtxt or \"\").lower()))\n",
    "    if is_c or (\"tau\" in fallback_name.lower() and not dimtxt):\n",
    "        return pd.to_numeric(series, errors=\"coerce\") + 273.15\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "def mu_air_suth(TK):\n",
    "    mu0, T0, S = 1.716e-5, 273.15, 111.0\n",
    "    return mu0 * ((T0 + S) / (TK + S)) * (TK / T0) ** 1.5\n",
    "\n",
    "def mu_co2_suth(TK):\n",
    "    mu0, T0, S = 1.37e-5, 273.15, 240.0  # aproximação para CO2\n",
    "    return mu0 * ((T0 + S) / (TK + S)) * (TK / T0) ** 1.5\n",
    "\n",
    "def fill_blocks_avg(series: pd.Series) -> pd.Series:\n",
    "    vals = series.values.astype(float)\n",
    "    n = len(vals)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if not np.isnan(vals[i]):\n",
    "            i += 1\n",
    "            continue\n",
    "        start = i\n",
    "        while i < n and np.isnan(vals[i]):\n",
    "            i += 1\n",
    "        end = i - 1\n",
    "        # vizinhos válidos\n",
    "        prev_idx = start - 1\n",
    "        while prev_idx >= 0 and np.isnan(vals[prev_idx]):\n",
    "            prev_idx -= 1\n",
    "        next_idx = end + 1\n",
    "        while next_idx < n and np.isnan(vals[next_idx]):\n",
    "            next_idx += 1\n",
    "        if prev_idx >= 0 and next_idx < n and np.isfinite(vals[prev_idx]) and np.isfinite(vals[next_idx]):\n",
    "            vals[start:end+1] = 0.5 * (vals[prev_idx] + vals[next_idx])\n",
    "        # bordas ficam NaN\n",
    "    return pd.Series(vals, index=series.index)\n",
    "\n",
    "# ==============================================================\n",
    "# (3) LEITURA DO CSV (2 CABEÇALHOS) E PREPARO\n",
    "# ==============================================================\n",
    "df_raw = pd.read_csv(PATH_IN, header=[0,1], engine=\"python\")\n",
    "dim_por_col = {col: dim for (col, dim) in df_raw.columns}\n",
    "df = df_raw.copy()\n",
    "df.columns = [col for (col, dim) in df_raw.columns]\n",
    "colunas = list(df.columns)\n",
    "\n",
    "# Localiza início do bloco físico (TAU_DENSA → fim). Se não existir, fallback = 152º (1-indexado)\n",
    "if \"TAU_DENSA\" in colunas:\n",
    "    idx_tau = colunas.index(\"TAU_DENSA\")\n",
    "else:\n",
    "    c, _ = best_match(\"tau_densa\", colunas)\n",
    "    idx_tau = colunas.index(c) if c else 151\n",
    "print(f\"Início do bloco físico para cálculo: coluna #{idx_tau+1} -> {colunas[idx_tau]} (até o final). Total usadas: {len(colunas[idx_tau:])}\")\n",
    "\n",
    "# ==============================================================\n",
    "# (4) MAPEAMENTO SEMÂNTICO DAS COLUNAS-CHAVE\n",
    "# ==============================================================\n",
    "ALIASES = {\n",
    "    \"tau_main\":     [\"tau_densa\",\"tau\",\"t_leito\",\"leito_temp_average\",\"bed_temp\",\"temperatura\"],\n",
    "    \"temp_leito\":   [\"leito_temp_average\",\"bed_temp\",\"temperatura_leito\",\"fornalha_leito_temp_average\"],\n",
    "    \"air_total\":    [\"air_total_knm3_h\",\"air_total_nm3_h\",\"air_total_mn3_h\",\"air_total\"],\n",
    "    \"air_primary\":  [\"air_primary_knm3_h\",\"air_primary_nm3_h\",\"air_primary\"],\n",
    "    \"air_secondary\":[\"air_secondary_knm3_h\",\"air_secondary_nm3_h\",\"air_secondary\"],\n",
    "    \"dp_furn\":      [\"dp_fornalha\",\"delta_p_fornalha\",\"fornalha_dp\",\"dp_furnace\"],\n",
    "    \"p_furn_a\":     [\"pressao_fornalha_a_inf\",\"pressao_fornalha_a\",\"furnace_a_pressure\",\"fornalha_a_press\"],\n",
    "    \"p_furn_b\":     [\"pressao_fornalha_b_inf\",\"pressao_fornalha_b\",\"furnace_b_pressure\",\"fornalha_b_press\"],\n",
    "    \"p_abs\":        [\"pressao_fornalha\",\"furnace_pressure\",\"pressao_plenum\",\"pressao_leito\",\"pressao_tambor\"],\n",
    "    \"rho_co2\":      [\"densidade_co2\",\"co2_density\",\"rho_co2\"],  # NÃO mapear para O2\n",
    "    \"o2\":           [\"o2_medio\",\"o2_excess_pct\",\"oxygen\"]\n",
    "}\n",
    "def choose(qs):\n",
    "    best, best_s = None, -1.0\n",
    "    for q in qs:\n",
    "        c, s = best_match(q, colunas)\n",
    "        if s > best_s:\n",
    "            best, best_s = c, s\n",
    "    return best\n",
    "\n",
    "tau_main       = choose(ALIASES[\"tau_main\"])\n",
    "temp_leito_col = choose(ALIASES[\"temp_leito\"])\n",
    "air_total_col  = choose(ALIASES[\"air_total\"])\n",
    "air_pri_col    = choose(ALIASES[\"air_primary\"])\n",
    "air_sec_col    = choose(ALIASES[\"air_secondary\"])\n",
    "dp_furn_col    = choose(ALIASES[\"dp_furn\"])\n",
    "p_furn_a_col   = choose(ALIASES[\"p_furn_a\"])\n",
    "p_furn_b_col   = choose(ALIASES[\"p_furn_b\"])\n",
    "p_abs_col      = choose(ALIASES[\"p_abs\"])\n",
    "rho_co2_col    = choose(ALIASES[\"rho_co2\"])\n",
    "o2_col         = choose(ALIASES[\"o2\"])\n",
    "\n",
    "print(\"\\n[Mapeamento] Colunas encontradas:\")\n",
    "for label, name in [\n",
    "    (\"tau_main\", tau_main), (\"temp_leito\", temp_leito_col),\n",
    "    (\"air_total\", air_total_col), (\"air_primary\", air_pri_col), (\"air_secondary\", air_sec_col),\n",
    "    (\"dp_furn\", dp_furn_col), (\"p_furn_a\", p_furn_a_col), (\"p_furn_b\", p_furn_b_col),\n",
    "    (\"p_abs\", p_abs_col), (\"rho_co2\", rho_co2_col), (\"o2\", o2_col)\n",
    "]:\n",
    "    print(f\"  - {label:12s}: {name}\")\n",
    "\n",
    "# ==============================================================\n",
    "# (5) TEMPERATURA (K), PRESSÕES (Pa) E ΔP ROBUSTO\n",
    "# ==============================================================\n",
    "# Temperatura (TK)\n",
    "if tau_main:\n",
    "    TK = TK_from_series(df[tau_main], dim_text(tau_main), fallback_name=tau_main)\n",
    "elif temp_leito_col:\n",
    "    TK = TK_from_series(df[temp_leito_col], dim_text(temp_leito_col), fallback_name=temp_leito_col)\n",
    "else:\n",
    "    raise RuntimeError(\"Não encontrei coluna de temperatura (TAU/LEITO).\")\n",
    "\n",
    "# Pressão absoluta (Pabs)\n",
    "if p_abs_col:\n",
    "    Pabs = to_pa(df[p_abs_col], dim_text(p_abs_col))\n",
    "    # se parecer gauge, soma atmosfera\n",
    "    if Pabs.median(skipna=True) < 2e5:\n",
    "        Pabs = Pabs + P_N\n",
    "else:\n",
    "    Pabs = pd.Series(P_N, index=df.index)\n",
    "\n",
    "# ΔP (preferir dp_furn se for realmente ΔP; senão, usar |A−B|)\n",
    "DP_from_col = to_pa(df[dp_furn_col], dim_text(dp_furn_col)) if dp_furn_col else pd.Series(np.nan, index=df.index)\n",
    "DP_from_ab  = None\n",
    "if p_furn_a_col and p_furn_b_col:\n",
    "    Pa = to_pa(df[p_furn_a_col], dim_text(p_furn_a_col))\n",
    "    Pb = to_pa(df[p_furn_b_col], dim_text(p_furn_b_col))\n",
    "    DP_from_ab = (Pa - Pb).abs()\n",
    "\n",
    "def looks_like_absolute(dp_name: str) -> bool:\n",
    "    if not dp_name: return False\n",
    "    n = dp_name.lower()\n",
    "    return (\"delta\" not in n) and (\"dp\" not in n)\n",
    "\n",
    "use_ab_all = False\n",
    "if dp_furn_col and p_abs_col and (dp_furn_col == p_abs_col):\n",
    "    use_ab_all = True\n",
    "elif dp_furn_col and looks_like_absolute(dp_furn_col) and DP_from_ab is not None:\n",
    "    use_ab_all = True\n",
    "\n",
    "if use_ab_all and DP_from_ab is not None:\n",
    "    DP = DP_from_ab.copy()\n",
    "else:\n",
    "    DP = DP_from_col.copy()\n",
    "    if DP_from_ab is not None:\n",
    "        need = ~np.isfinite(DP) & np.isfinite(DP_from_ab)\n",
    "        DP.loc[need] = DP_from_ab.loc[need]\n",
    "\n",
    "# ==============================================================\n",
    "# (6) VAZÃO TOTAL DE AR (Nm3/h) — PREENCHIMENTO E PERSISTÊNCIA\n",
    "# ==============================================================\n",
    "QN_total_total = pd.to_numeric(df[air_total_col], errors=\"coerce\") if air_total_col else pd.Series(np.nan, index=df.index)\n",
    "QN_primary     = pd.to_numeric(df[air_pri_col],   errors=\"coerce\") if air_pri_col   else pd.Series(np.nan, index=df.index)\n",
    "QN_secondary   = pd.to_numeric(df[air_sec_col],   errors=\"coerce\") if air_sec_col   else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "QN_sum_ps = QN_primary.add(QN_secondary, fill_value=np.nan)\n",
    "\n",
    "QN0 = QN_total_total.copy()\n",
    "mask_fill_sum = QN0.isna() & QN_sum_ps.notna()\n",
    "QN0.loc[mask_fill_sum] = QN_sum_ps.loc[mask_fill_sum]\n",
    "\n",
    "QN_total_filled = fill_blocks_avg(QN0)\n",
    "\n",
    "print(f\"\\n[QN_total] NaNs (total): {int(QN_total_total.isna().sum())} | \"\n",
    "      f\"após prim+sec: {int(QN0.isna().sum())} | após blocos: {int(QN_total_filled.isna().sum())}\")\n",
    "\n",
    "# Persistir para auditoria e diagnóstico\n",
    "df[\"air_total_knm3_h_filled\"] = QN_total_filled\n",
    "dim_por_col[\"air_total_knm3_h_filled\"] = \"kNm3/h (preenchido)\"\n",
    "\n",
    "# ==============================================================\n",
    "# (7) PROXIES: Q_real, ν, γ, ρg, μgas, δ\n",
    "# ==============================================================\n",
    "# Q_real (m3/s) no ponto de operação\n",
    "Q_real = (QN_total_filled * 1000.0 / 3600.0) * (TK / T_N) * (P_N / Pabs)\n",
    "\n",
    "# Velocidade ν (m/s)\n",
    "nu_base = Q_real / A_EF\n",
    "DP_ref  = np.nanmedian(DP) if np.isfinite(DP).any() else 1.0\n",
    "nu_proxy = nu_base * np.power(np.maximum(DP, 1.0) / max(DP_ref, 1.0), BETA_DP)\n",
    "\n",
    "# Fração de CO2 (γ) — se existir densidade de CO2; senão, default\n",
    "if rho_co2_col:\n",
    "    rho_co2_meas = pd.to_numeric(df[rho_co2_col], errors=\"coerce\")\n",
    "    rho_co2_pure = (Pabs * M_CO2) / (R * TK)\n",
    "    gamma = np.clip(rho_co2_meas / np.maximum(rho_co2_pure, 1e-9), 0.0, 1.0)\n",
    "else:\n",
    "    gamma = pd.Series(GAMMA_DEFAULT, index=df.index)\n",
    "\n",
    "# Densidade do gás (kg/m3)\n",
    "M_mix = (1 - gamma) * M_AIR + gamma * M_CO2\n",
    "rho_g = (Pabs * M_mix) / (R * TK)\n",
    "\n",
    "# Viscosidade do gás (Pa·s) — mistura ar/CO2 via Sutherland\n",
    "mu_gas = (1 - gamma) * mu_air_suth(TK) + gamma * mu_co2_suth(TK)\n",
    "\n",
    "# Diâmetro δ (m) — Ergun-like + ganho C_DELTA\n",
    "epsP = 1.0  # Pa\n",
    "delta_proxy = (rho_g * (nu_proxy ** 2) * L_REF) / np.maximum(DP, epsP)\n",
    "delta_m = C_DELTA * delta_proxy\n",
    "\n",
    "# ==================== PATCH PARA CORRIGIR WM (cole após a seção 7) ====================\n",
    "print(\"\\n[FIX-WM] Verificando mapeamento de rho_co2...\")\n",
    "\n",
    "# 1) Se rho_co2 caiu em o2_medio, ignorar esse mapeamento (usar default)\n",
    "if 'rho_co2_col' in globals() and 'o2_col' in globals() and (rho_co2_col is not None) and (o2_col is not None):\n",
    "    if rho_co2_col == o2_col:\n",
    "        print(\"  - rho_co2 estava mapeado para O2. Ignorando e usando GAMMA_DEFAULT.\")\n",
    "        rho_co2_col = None\n",
    "\n",
    "# 2) Recalcular gamma de forma robusta (sem usar O2 por padrão)\n",
    "if rho_co2_col:\n",
    "    rho_co2_meas = pd.to_numeric(df[rho_co2_col], errors=\"coerce\")\n",
    "    rho_co2_pure = (Pabs * M_CO2) / (R * TK)\n",
    "    gamma = np.clip(rho_co2_meas / np.maximum(rho_co2_pure, 1e-9), 0.0, 1.0)\n",
    "else:\n",
    "    gamma = pd.Series(GAMMA_DEFAULT, index=df.index)\n",
    "\n",
    "gamma = gamma.fillna(GAMMA_DEFAULT).clip(0.0, 1.0)\n",
    "\n",
    "# 3) Recalcular μ_gas (mistura ar/CO2) com a nova gamma\n",
    "mu_air = mu_air_suth(TK)\n",
    "mu_co2 = mu_co2_suth(TK)\n",
    "mu_gas = (1 - gamma) * mu_air + gamma * mu_co2\n",
    "mu_gas = pd.to_numeric(mu_gas, errors=\"coerce\").fillna(mu_air_suth(TK))  # fallback: ar puro\n",
    "\n",
    "# 4) Recalcular h e Wm (Wr não depende de μ_gas)\n",
    "sigma_steel = pd.Series(SIGMA_STEEL_BASE, index=df.index)  # pode trocar por coluna se existir\n",
    "h = sigma_steel / np.maximum(mu_gas, 1e-12)\n",
    "\n",
    "nu_pos    = np.maximum(nu_proxy.fillna(0.0), 0.0)\n",
    "delta_pos = np.maximum(delta_m.fillna(np.nan), 1e-9)\n",
    "TK_pos    = np.maximum(TK.fillna(np.nan), 1.0)\n",
    "\n",
    "wm = (BETA_BASE * (nu_pos ** 3.0) * np.sqrt(delta_pos) * np.exp(-TK_pos / TOPT_BASE) * h).astype(float)\n",
    "\n",
    "# 5) Atualizar no df_labeled e salvar v3 (mantendo 2 linhas de cabeçalho)\n",
    "df_labeled = df.copy()\n",
    "df_labeled[\"wr_kg_m2_h\"] = wr  # Wr do cálculo vigente\n",
    "df_labeled[\"wm_kg_m2_h\"] = wm  # Wm corrigido\n",
    "\n",
    "dim_por_col[\"wr_kg_m2_h\"] = \"kg/m2·h\"\n",
    "dim_por_col[\"wm_kg_m2_h\"] = \"kg/m2·h\"\n",
    "\n",
    "PATH_OUTL3 = os.path.join(CURATED_DIR, \"A1_ML_DL_rotulado_v3.csv\")\n",
    "with open(PATH_OUTL3, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    f.write(\",\".join(df_labeled.columns) + \"\\n\")\n",
    "    f.write(\",\".join([str(dim_por_col.get(c, \"\")) for c in df_labeled.columns]) + \"\\n\")\n",
    "df_labeled.to_csv(PATH_OUTL3, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Rotulado (Wm corrigido) salvo: {PATH_OUTL3}\")\n",
    "\n",
    "# 6) Refazer FEATURES v3, excluindo tudo que foi usado no cálculo (inclui 'air_total_knm3_h_filled' e TAU_DENSA→fim)\n",
    "if 'used_cols_for_targets' not in globals():\n",
    "    used_cols_for_targets = set()\n",
    "used_cols_for_targets.update([tau_main, temp_leito_col, air_total_col, air_pri_col, air_sec_col,\n",
    "                              dp_furn_col, p_furn_a_col, p_furn_b_col, p_abs_col,\n",
    "                              rho_co2_col, o2_col, \"air_total_knm3_h_filled\"])\n",
    "used_cols_for_targets = {c for c in used_cols_for_targets if c}\n",
    "used_cols_for_targets.update(colunas[idx_tau:])\n",
    "\n",
    "orig_cols = [c for c in df.columns if c not in [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]]\n",
    "cols_excluir = set(colunas[idx_tau:]).union(used_cols_for_targets)\n",
    "cols_keep = [c for c in orig_cols if c not in cols_excluir] + [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]\n",
    "\n",
    "df_features = df_labeled[cols_keep].copy()\n",
    "PATH_OUTF3 = os.path.join(CURATED_DIR, \"A1_ML_DL_features_v3.csv\")\n",
    "with open(PATH_OUTF3, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    f.write(\",\".join(df_features.columns) + \"\\n\")\n",
    "    f.write(\",\".join([str(dim_por_col.get(c, \"\")) for c in df_features.columns]) + \"\\n\")\n",
    "df_features.to_csv(PATH_OUTF3, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Features (v3, sem vazamento) salvo: {PATH_OUTF3}\")\n",
    "\n",
    "# 7) EDA rápida pós-correção\n",
    "def _eda_rapida(s: pd.Series, nome: str):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    print(f\"\\n--- {nome} ---\")\n",
    "    print(\"N válidos:\", s.notna().sum(), \"| N nulos:\", s.isna().sum())\n",
    "    print(s.describe(percentiles=[0.01,0.05,0.25,0.5,0.75,0.95,0.99]).to_string())\n",
    "\n",
    "_eda_rapida(df_labeled[\"wr_kg_m2_h\"], \"wr_kg_m2_h (v3)\")\n",
    "_eda_rapida(df_labeled[\"wm_kg_m2_h\"], \"wm_kg_m2_h (v3)\")\n",
    "# ==================== FIM DO PATCH ====================\n",
    "# ==================== FIX DEFINITIVO DE WM (cole após a Seção 7) ====================\n",
    "\n",
    "print(\"\\n[FIX-WM v4] Robustecendo μ_gas e Wm...\")\n",
    "\n",
    "# (1) Blindagem: se rho_co2 mapeou para algo com 'o2'/'oxygen', invalida (usa GAMMA_DEFAULT)\n",
    "if 'rho_co2_col' in globals() and rho_co2_col is not None:\n",
    "    name_low = str(rho_co2_col).lower()\n",
    "    if ('o2' in name_low) or ('oxygen' in name_low):\n",
    "        print(f\"  - rho_co2='{rho_co2_col}' parece O2; ignorando e usando GAMMA_DEFAULT.\")\n",
    "        rho_co2_col = None\n",
    "\n",
    "# (2) Gamma robusto (sem usar O2). Se densidade de CO2 existir, usa relação ideal; senão, constante.\n",
    "if rho_co2_col:\n",
    "    rho_co2_meas = pd.to_numeric(df[rho_co2_col], errors=\"coerce\")\n",
    "    rho_co2_pure = (Pabs * M_CO2) / (R * TK)                 # pode dar NaN se TK NaN\n",
    "    gamma = np.clip(rho_co2_meas / np.maximum(rho_co2_pure, 1e-9), 0.0, 1.0)\n",
    "else:\n",
    "    gamma = pd.Series(GAMMA_DEFAULT, index=df.index)\n",
    "\n",
    "gamma = gamma.replace([np.inf, -np.inf], np.nan).fillna(GAMMA_DEFAULT).clip(0.0, 1.0)\n",
    "\n",
    "# (3) TK para viscosidade: se TK for NaN, usa 1100 K (faixa típica do leito)\n",
    "TK_mu = TK.copy()\n",
    "if isinstance(TK_mu, pd.Series):\n",
    "    TK_mu = TK_mu.where(np.isfinite(TK_mu), 1100.0)\n",
    "else:\n",
    "    TK_mu = 1100.0 if not np.isfinite(TK_mu) else TK_mu\n",
    "\n",
    "# (4) μ_gas pela mistura ar/CO2 com TK robusto (garantir finito)\n",
    "mu_air = mu_air_suth(TK_mu)\n",
    "mu_co2 = mu_co2_suth(TK_mu)\n",
    "mu_gas = (1 - gamma) * mu_air + gamma * mu_co2\n",
    "# fallback de segurança absoluto:\n",
    "mu_gas = pd.to_numeric(mu_gas, errors=\"coerce\")\n",
    "mu_gas = mu_gas.replace([np.inf, -np.inf], np.nan).fillna(mu_air_suth(pd.Series(np.full(len(df), 1100.0))))\n",
    "\n",
    "# (5) Recalcular h e Wm com μ_gas robusto (Wr fica como está)\n",
    "sigma_steel = pd.Series(SIGMA_STEEL_BASE, index=df.index)  # use coluna se houver\n",
    "h = sigma_steel / np.maximum(mu_gas, 1e-12)\n",
    "\n",
    "nu_pos    = np.maximum(nu_proxy.fillna(0.0), 0.0)\n",
    "delta_pos = np.maximum(delta_m.fillna(np.nan), 1e-9)\n",
    "TK_pos    = np.maximum(TK.fillna(np.nan), 1.0)\n",
    "\n",
    "wm = (BETA_BASE * (nu_pos ** 3.0) * np.sqrt(delta_pos) * np.exp(-TK_pos / TOPT_BASE) * h).astype(float)\n",
    "\n",
    "print(f\"  - μ_gas NaNs após fix: {int(pd.isna(mu_gas).sum())}\")\n",
    "\n",
    "# (6) Atualizar rotulado e features — V4\n",
    "df_labeled_v4 = df.copy()\n",
    "df_labeled_v4[\"wr_kg_m2_h\"] = wr              # WR do cálculo vigente\n",
    "df_labeled_v4[\"wm_kg_m2_h\"] = wm              # WM corrigido e robusto\n",
    "\n",
    "dim_por_col[\"wr_kg_m2_h\"] = \"kg/m2·h\"\n",
    "dim_por_col[\"wm_kg_m2_h\"] = \"kg/m2·h\"\n",
    "\n",
    "PATH_OUTL4 = os.path.join(CURATED_DIR, \"A1_ML_DL_rotulado_v4.csv\")\n",
    "with open(PATH_OUTL4, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    f.write(\",\".join(df_labeled_v4.columns) + \"\\n\")\n",
    "    f.write(\",\".join([str(dim_por_col.get(c, \"\")) for c in df_labeled_v4.columns]) + \"\\n\")\n",
    "df_labeled_v4.to_csv(PATH_OUTL4, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Rotulado (v4) salvo: {PATH_OUTL4}\")\n",
    "\n",
    "# Reutiliza a mesma lista de exclusões (used_cols_for_targets + TAU_DENSA→fim)\n",
    "if 'used_cols_for_targets' not in globals():\n",
    "    used_cols_for_targets = set()\n",
    "used_cols_for_targets.update([\n",
    "    tau_main, temp_leito_col, air_total_col, air_pri_col, air_sec_col,\n",
    "    dp_furn_col, p_furn_a_col, p_furn_b_col, p_abs_col, rho_co2_col,\n",
    "    o2_col, \"air_total_knm3_h_filled\"\n",
    "])\n",
    "used_cols_for_targets = {c for c in used_cols_for_targets if c}\n",
    "used_cols_for_targets.update(colunas[idx_tau:])\n",
    "\n",
    "orig_cols = [c for c in df.columns if c not in [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]]\n",
    "cols_excluir = set(colunas[idx_tau:]).union(used_cols_for_targets)\n",
    "cols_keep = [c for c in orig_cols if c not in cols_excluir] + [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]\n",
    "\n",
    "df_features_v4 = df_labeled_v4[cols_keep].copy()\n",
    "PATH_OUTF4 = os.path.join(CURATED_DIR, \"A1_ML_DL_features_v4.csv\")\n",
    "with open(PATH_OUTF4, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    f.write(\",\".join(df_features_v4.columns) + \"\\n\")\n",
    "    f.write(\",\".join([str(dim_por_col.get(c, \"\")) for c in df_features_v4.columns]) + \"\\n\")\n",
    "df_features_v4.to_csv(PATH_OUTF4, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Features (v4, sem vazamento) salvo: {PATH_OUTF4}\")\n",
    "\n",
    "# (7) EDA rápida pós-fix\n",
    "def _eda(s: pd.Series, nome: str):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    print(f\"\\n--- {nome} ---\")\n",
    "    print(\"N válidos:\", s.notna().sum(), \"| N nulos:\", s.isna().sum())\n",
    "    print(s.describe(percentiles=[0.01,0.05,0.25,0.5,0.75,0.95,0.99]).to_string())\n",
    "\n",
    "_eda(df_labeled_v4[\"wr_kg_m2_h\"], \"wr_kg_m2_h (v4)\")\n",
    "_eda(df_labeled_v4[\"wm_kg_m2_h\"], \"wm_kg_m2_h (v4)\")\n",
    "# ==================== FIM DO FIX v4 ====================\n",
    "\n",
    "# ==============================================================\n",
    "# (8) Wr / Wm E FATORES g/h\n",
    "# ==============================================================\n",
    "sigma_mat   = pd.Series(SIGMA_MAT_BASE,   index=df.index)\n",
    "rho_fuel    = pd.Series(RHO_FUEL_BASE,    index=df.index)\n",
    "sigma_steel = pd.Series(SIGMA_STEEL_BASE, index=df.index)\n",
    "\n",
    "g = sigma_mat / np.maximum(rho_fuel, 1e-12)\n",
    "h = sigma_steel / np.maximum(mu_gas,  1e-12)\n",
    "\n",
    "nu_pos    = np.maximum(nu_proxy.fillna(0.0), 0.0)\n",
    "delta_pos = np.maximum(delta_m.fillna(np.nan), 1e-9)\n",
    "TK_pos    = np.maximum(TK.fillna(np.nan), 1.0)\n",
    "\n",
    "wr = (ALPHA_BASE * (nu_pos ** 2.0) * (delta_pos ** 1.0) * np.exp(-TK_pos / TCRIT_BASE) * g).astype(float)\n",
    "wm = (BETA_BASE  * (nu_pos ** 3.0) * np.sqrt(delta_pos)    * np.exp(-TK_pos / TOPT_BASE)  * h).astype(float)\n",
    "\n",
    "wr_dim = \"kg/m2·h\"\n",
    "wm_dim = \"kg/m2·h\"\n",
    "\n",
    "# ==============================================================\n",
    "# (9) SALVAR ROTULADO + FEATURES (sem vazamento)\n",
    "# ==============================================================\n",
    "df_labeled = df.copy()\n",
    "df_labeled[\"wr_kg_m2_h\"] = wr\n",
    "df_labeled[\"wm_kg_m2_h\"] = wm\n",
    "\n",
    "dim_por_col[\"wr_kg_m2_h\"] = wr_dim\n",
    "dim_por_col[\"wm_kg_m2_h\"] = wm_dim\n",
    "\n",
    "# Leak-guard: tudo que foi usado no cálculo + bloco TAU_DENSA→fim\n",
    "used_cols_for_targets = set([\n",
    "    tau_main, temp_leito_col, air_total_col, air_pri_col, air_sec_col,\n",
    "    dp_furn_col, p_furn_a_col, p_furn_b_col, p_abs_col, rho_co2_col, o2_col,\n",
    "    \"air_total_knm3_h_filled\"\n",
    "])\n",
    "used_cols_for_targets = {c for c in used_cols_for_targets if c}  # remove None\n",
    "used_cols_for_targets.update(colunas[idx_tau:])\n",
    "\n",
    "print(\"\\n[Leak-guard] Colunas a excluir das features:\")\n",
    "for c in sorted(used_cols_for_targets):\n",
    "    print(\"  -\", c)\n",
    "\n",
    "# (9.1) Salvar ROTULADO (2 linhas de cabeçalho)\n",
    "with open(PATH_OUTL, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    f.write(\",\".join(df_labeled.columns) + \"\\n\")\n",
    "    f.write(\",\".join([str(dim_por_col.get(c, \"\")) for c in df_labeled.columns]) + \"\\n\")\n",
    "df_labeled.to_csv(PATH_OUTL, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n✅ Rotulado salvo: {PATH_OUTL}\")\n",
    "\n",
    "# (9.2) Montar FEATURES (removendo tudo que contribui para Wr/Wm)\n",
    "orig_cols = [c for c in df.columns if c not in [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]]\n",
    "cols_excluir = set(colunas[idx_tau:]).union(used_cols_for_targets)\n",
    "cols_keep = [c for c in orig_cols if c not in cols_excluir] + [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]\n",
    "df_features = df_labeled[cols_keep].copy()\n",
    "\n",
    "with open(PATH_OUTF, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "    f.write(\",\".join(df_features.columns) + \"\\n\")\n",
    "    f.write(\",\".join([str(dim_por_col.get(c, \"\")) for c in df_features.columns]) + \"\\n\")\n",
    "df_features.to_csv(PATH_OUTF, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "print(f\"✅ Features (sem vazamento) salvo: {PATH_OUTF}\")\n",
    "\n",
    "# ==============================================================\n",
    "# (10) RESUMO EDA DOS ALVOS\n",
    "# ==============================================================\n",
    "def eda_quick(s: pd.Series, nome: str):\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    print(f\"\\n--- {nome} ---\")\n",
    "    print(\"N válidos:\", s.notna().sum(), \"| N nulos:\", s.isna().sum())\n",
    "    print(s.describe(percentiles=[0.01,0.05,0.25,0.5,0.75,0.95,0.99]).to_string())\n",
    "\n",
    "eda_quick(df_labeled[\"wr_kg_m2_h\"], \"wr_kg_m2_h\")\n",
    "eda_quick(df_labeled[\"wm_kg_m2_h\"], \"wm_kg_m2_h\")\n",
    "\n",
    "print(\"\\nConcluído.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf1c979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOLD: linhas=10005 | salvo:\n",
      "  Rotulado: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_rotulado_v4_gold.csv\n",
      "  Features: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_features_v4_gold.csv\n",
      "SILVER: linhas=10005 | salvo:\n",
      "  Rotulado: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_rotulado_v4_silver.csv\n",
      "  Features: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_features_v4_silver.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# GOLD & SILVER a partir de A1_ML_DL_rotulado_v4.csv\n",
    "# ==============================================================\n",
    "\n",
    "import os, re, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# ---------- caminhos ----------\n",
    "CURATED_DIR = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\"\n",
    "PATH_ROT_V4 = os.path.join(CURATED_DIR, \"A1_ML_DL_rotulado_v4.csv\")\n",
    "\n",
    "PATH_GOLD_L = os.path.join(CURATED_DIR, \"A1_ML_DL_rotulado_v4_gold.csv\")\n",
    "PATH_GOLD_F = os.path.join(CURATED_DIR, \"A1_ML_DL_features_v4_gold.csv\")\n",
    "\n",
    "PATH_SILV_L = os.path.join(CURATED_DIR, \"A1_ML_DL_rotulado_v4_silver.csv\")\n",
    "PATH_SILV_F = os.path.join(CURATED_DIR, \"A1_ML_DL_features_v4_silver.csv\")\n",
    "\n",
    "# ---------- constantes físicas e parâmetros ----------\n",
    "T_N, P_N = 273.15, 1.01325e5   # K, Pa\n",
    "R = 8.314462618\n",
    "M_AIR, M_CO2 = 0.029, 0.044\n",
    "\n",
    "A_EF   = 15.0     # m²\n",
    "L_REF  = 3.0      # m\n",
    "BETA_DP = 0.25\n",
    "C_DELTA = 1e-3\n",
    "GAMMA_DEFAULT = 0.25\n",
    "\n",
    "ALPHA_BASE = 4.2e-3\n",
    "BETA_BASE  = 1.2e-4\n",
    "TCRIT_BASE = 1200.0\n",
    "TOPT_BASE  = 1250.0\n",
    "SIGMA_MAT_BASE   = 6.0e9\n",
    "RHO_FUEL_BASE    = 700.0\n",
    "SIGMA_STEEL_BASE = 2.5e9\n",
    "\n",
    "# ---------- utilidades ----------\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFKD\", str(s)) if not unicodedata.combining(c))\n",
    "def normalize_token(s: str) -> str:\n",
    "    s = strip_accents(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9_\\s]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "def split_tokens(s: str): return set(t for t in re.split(r\"[ _]+\", normalize_token(s)) if t)\n",
    "def hybrid_similarity(a: str, b: str) -> float:\n",
    "    ta, tb = split_tokens(a), split_tokens(b)\n",
    "    jac = len(ta & tb) / len(ta | tb) if (ta and tb) else 0.0\n",
    "    seq = SequenceMatcher(None, normalize_token(a), normalize_token(b)).ratio()\n",
    "    return 0.6*jac + 0.4*seq\n",
    "def best_match(q, cands):\n",
    "    sc = [(c, hybrid_similarity(q, c)) for c in cands]\n",
    "    sc.sort(key=lambda x: x[1], reverse=True)\n",
    "    return sc[0] if sc else (None, 0.0)\n",
    "\n",
    "def read_multiheader_csv(path):\n",
    "    dfr = pd.read_csv(path, header=[0,1], engine=\"python\")\n",
    "    dim = {c: d for (c,d) in dfr.columns}\n",
    "    df  = dfr.copy()\n",
    "    df.columns = [c for (c,d) in dfr.columns]\n",
    "    return df, dim\n",
    "\n",
    "def write_multiheader_csv(df, dim, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        f.write(\",\".join(df.columns) + \"\\n\")\n",
    "        f.write(\",\".join([str(dim.get(c,\"\")) for c in df.columns]) + \"\\n\")\n",
    "    df.to_csv(path, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "def dim_text(dim_map, col): return str(dim_map.get(col,\"\") or \"\").lower()\n",
    "def to_pa(x, dimtxt):\n",
    "    s = (dimtxt or \"\").lower()\n",
    "    v = pd.to_numeric(x, errors=\"coerce\")\n",
    "    if \"mpa\" in s:   return v * 1e6\n",
    "    if \"kpa\" in s:   return v * 1e3\n",
    "    if \"bar\" in s:   return v * 1e5\n",
    "    if \"mbar\" in s:  return v * 1e2\n",
    "    return v\n",
    "\n",
    "def TK_from_series(series, dimtxt, fallback_name=\"\"):\n",
    "    is_c = (\"°c\" in (dimtxt or \"\").lower()) or ((\"c\" in (dimtxt or \"\").lower() and \"k\" not in (dimtxt or \"\").lower()))\n",
    "    if is_c or (\"tau\" in str(fallback_name).lower() and not dimtxt):\n",
    "        return pd.to_numeric(series, errors=\"coerce\") + 273.15\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "def mu_air_suth(TK):\n",
    "    mu0, T0, S = 1.716e-5, 273.15, 111.0\n",
    "    return mu0 * ((T0 + S) / (TK + S)) * (TK / T0) ** 1.5\n",
    "def mu_co2_suth(TK):\n",
    "    mu0, T0, S = 1.37e-5, 273.15, 240.0\n",
    "    return mu0 * ((T0 + S) / (TK + S)) * (TK / T0) ** 1.5\n",
    "\n",
    "def fill_blocks_avg(series):\n",
    "    vals = series.values.astype(float)\n",
    "    n = len(vals); i = 0\n",
    "    while i < n:\n",
    "        if not np.isnan(vals[i]): i += 1; continue\n",
    "        s = i\n",
    "        while i < n and np.isnan(vals[i]): i += 1\n",
    "        e = i - 1\n",
    "        prev_i = s - 1\n",
    "        while prev_i >= 0 and np.isnan(vals[prev_i]): prev_i -= 1\n",
    "        next_i = e + 1\n",
    "        while next_i < n and np.isnan(vals[next_i]): next_i += 1\n",
    "        if prev_i >= 0 and next_i < n and np.isfinite(vals[prev_i]) and np.isfinite(vals[next_i]):\n",
    "            vals[s:e+1] = 0.5*(vals[prev_i] + vals[next_i])\n",
    "    return pd.Series(vals, index=series.index)\n",
    "\n",
    "# ---------- 1) Ler o rotulado v4 ----------\n",
    "df, dim_map = read_multiheader_csv(PATH_ROT_V4)\n",
    "colunas = list(df.columns)\n",
    "\n",
    "# localizar TAU_DENSA para o bloco físico\n",
    "if \"TAU_DENSA\" in colunas:\n",
    "    idx_tau = colunas.index(\"TAU_DENSA\")\n",
    "else:\n",
    "    c,_ = best_match(\"tau_densa\", colunas); idx_tau = colunas.index(c) if c else len(colunas)\n",
    "\n",
    "# ---------- 2) GOLD: filtrar 10005 válidos (Wr/Wm finitos) e salvar ----------\n",
    "for c in [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]:\n",
    "    if c not in df.columns: raise RuntimeError(f\"Coluna faltando no v4: {c}\")\n",
    "\n",
    "mask_gold = np.isfinite(pd.to_numeric(df[\"wr_kg_m2_h\"], errors=\"coerce\")) & \\\n",
    "            np.isfinite(pd.to_numeric(df[\"wm_kg_m2_h\"], errors=\"coerce\"))\n",
    "df_gold = df.loc[mask_gold].copy()\n",
    "\n",
    "# Leak-guard: excluir features que entram no cálculo (inclui bloco físico)\n",
    "ALIASES = {\n",
    "    \"tau_main\":     [\"tau_densa\",\"tau\",\"t_leito\",\"leito_temp_average\",\"bed_temp\",\"temperatura\"],\n",
    "    \"temp_leito\":   [\"leito_temp_average\",\"bed_temp\",\"temperatura_leito\",\"fornalha_leito_temp_average\"],\n",
    "    \"air_total\":    [\"air_total_knm3_h_filled\",\"air_total_knm3_h\",\"air_total_nm3_h\",\"air_total_mn3_h\"],\n",
    "    \"air_primary\":  [\"air_primary_knm3_h\",\"air_primary_nm3_h\",\"air_primary\"],\n",
    "    \"air_secondary\":[\"air_secondary_knm3_h\",\"air_secondary_nm3_h\",\"air_secondary\"],\n",
    "    \"dp_furn\":      [\"dp_fornalha\",\"delta_p_fornalha\",\"fornalha_dp\",\"pressao_fornalha\",\"dp_furnace\"],\n",
    "    \"p_furn_a\":     [\"pressao_fornalha_a_inf\",\"pressao_fornalha_a\",\"furnace_a_pressure\",\"fornalha_a_press\"],\n",
    "    \"p_furn_b\":     [\"pressao_fornalha_b_inf\",\"pressao_fornalha_b\",\"furnace_b_pressure\",\"fornalha_b_press\"],\n",
    "    \"p_abs\":        [\"pressao_fornalha\",\"furnace_pressure\",\"pressao_plenum\",\"pressao_leito\",\"pressao_tambor\"],\n",
    "    \"rho_co2\":      [\"densidade_co2\",\"co2_density\",\"rho_co2\"],\n",
    "    \"o2\":           [\"o2_medio\",\"o2_excess_pct\",\"oxygen\"]\n",
    "}\n",
    "def choose(qs):\n",
    "    best, bs = None, -1.0\n",
    "    for q in qs:\n",
    "        c, s = best_match(q, colunas)\n",
    "        if c and s > bs:\n",
    "            best, bs = c, s\n",
    "    return best\n",
    "\n",
    "tau_main       = choose(ALIASES[\"tau_main\"])\n",
    "temp_leito_col = choose(ALIASES[\"temp_leito\"])\n",
    "air_total_col  = choose(ALIASES[\"air_total\"])\n",
    "air_pri_col    = choose(ALIASES[\"air_primary\"])\n",
    "air_sec_col    = choose(ALIASES[\"air_secondary\"])\n",
    "dp_furn_col    = choose(ALIASES[\"dp_furn\"])\n",
    "p_furn_a_col   = choose(ALIASES[\"p_furn_a\"])\n",
    "p_furn_b_col   = choose(ALIASES[\"p_furn_b\"])\n",
    "p_abs_col      = choose(ALIASES[\"p_abs\"])\n",
    "rho_co2_col    = choose(ALIASES[\"rho_co2\"])\n",
    "o2_col         = choose(ALIASES[\"o2\"])\n",
    "\n",
    "used_cols = set([tau_main, temp_leito_col, air_total_col, air_pri_col, air_sec_col,\n",
    "                 dp_furn_col, p_furn_a_col, p_furn_b_col, p_abs_col, rho_co2_col, o2_col,\n",
    "                 \"air_total_knm3_h_filled\"])\n",
    "used_cols = {c for c in used_cols if c}\n",
    "used_cols.update(colunas[idx_tau:])\n",
    "\n",
    "# Features GOLD = tudo menos used_cols + alvos (mesmas linhas GOLD)\n",
    "orig_cols = [c for c in df.columns if c not in [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]]\n",
    "cols_keep_gold = [c for c in orig_cols if c not in used_cols] + [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]\n",
    "df_gold_feat = df_gold[cols_keep_gold].copy()\n",
    "\n",
    "# salvar GOLD\n",
    "write_multiheader_csv(df_gold, dim_map, PATH_GOLD_L)\n",
    "write_multiheader_csv(df_gold_feat, dim_map, PATH_GOLD_F)\n",
    "\n",
    "print(f\"GOLD: linhas={len(df_gold)} | salvo:\")\n",
    "print(\"  Rotulado:\", PATH_GOLD_L)\n",
    "print(\"  Features:\", PATH_GOLD_F)\n",
    "\n",
    "# ---------- 3) SILVER: refazer ΔP (interp curto + k·ρg·ν²) e recalc Wr/Wm ----------\n",
    "# mapeamentos e séries base\n",
    "def series_TK(df_loc, name):\n",
    "    return TK_from_series(df_loc[name], dim_text(dim_map, name), fallback_name=name) if name else pd.Series(np.nan, index=df_loc.index)\n",
    "\n",
    "TK = series_TK(df, tau_main) if tau_main else (series_TK(df, temp_leito_col) if temp_leito_col else pd.Series(np.nan, index=df.index))\n",
    "\n",
    "if p_abs_col:\n",
    "    Pabs = to_pa(df[p_abs_col], dim_text(dim_map, p_abs_col))\n",
    "    if Pabs.median(skipna=True) < 2e5: Pabs = Pabs + P_N\n",
    "else:\n",
    "    Pabs = pd.Series(P_N, index=df.index)\n",
    "\n",
    "DP_from_col = to_pa(df[dp_furn_col], dim_text(dim_map, dp_furn_col)) if dp_furn_col else pd.Series(np.nan, index=df.index)\n",
    "DP_from_ab  = None\n",
    "if p_furn_a_col and p_furn_b_col:\n",
    "    Pa = to_pa(df[p_furn_a_col], dim_text(dim_map, p_furn_a_col))\n",
    "    Pb = to_pa(df[p_furn_b_col], dim_text(dim_map, p_furn_b_col))\n",
    "    DP_from_ab = (Pa - Pb).abs()\n",
    "\n",
    "def looks_like_absolute(dp_name: str) -> bool:\n",
    "    if not dp_name: return False\n",
    "    n = dp_name.lower()\n",
    "    return (\"delta\" not in n) and (\"dp\" not in n)\n",
    "\n",
    "use_ab_all = False\n",
    "if dp_furn_col and p_abs_col and (dp_furn_col == p_abs_col):\n",
    "    use_ab_all = True\n",
    "elif dp_furn_col and looks_like_absolute(dp_furn_col) and DP_from_ab is not None:\n",
    "    use_ab_all = True\n",
    "\n",
    "if use_ab_all and DP_from_ab is not None:\n",
    "    DP0 = DP_from_ab.copy()\n",
    "else:\n",
    "    DP0 = DP_from_col.copy()\n",
    "    if DP_from_ab is not None:\n",
    "        need = ~np.isfinite(DP0) & np.isfinite(DP_from_ab)\n",
    "        DP0.loc[need] = DP_from_ab.loc[need]\n",
    "\n",
    "# Vazão real (usa a coluna preenchida se existir)\n",
    "air_filled_col = \"air_total_knm3_h_filled\" if \"air_total_knm3_h_filled\" in df.columns else air_total_col\n",
    "QN_total_filled = pd.to_numeric(df[air_filled_col], errors=\"coerce\") if air_filled_col else pd.Series(np.nan, index=df.index)\n",
    "Q_real = (QN_total_filled * 1000.0 / 3600.0) * (TK / T_N) * (P_N / Pabs)\n",
    "nu_base = Q_real / A_EF\n",
    "DP_ref  = np.nanmedian(DP0) if np.isfinite(DP0).any() else 1.0\n",
    "nu_proxy = nu_base * np.power(np.maximum(DP0, 1.0) / max(DP_ref, 1.0), BETA_DP)\n",
    "\n",
    "# fração CO2 robusta (não usa O2)\n",
    "gamma = pd.Series(GAMMA_DEFAULT, index=df.index)\n",
    "\n",
    "# ρg, μgas (robusto com TK_mu)\n",
    "M_mix = (1 - gamma) * M_AIR + gamma * M_CO2\n",
    "rho_g = (Pabs * M_mix) / (R * TK)\n",
    "\n",
    "TK_mu = TK.where(np.isfinite(TK), 1100.0)\n",
    "mu_gas = (1 - gamma) * mu_air_suth(TK_mu) + gamma * mu_co2_suth(TK_mu)\n",
    "mu_gas = pd.to_numeric(mu_gas, errors=\"coerce\").fillna(mu_air_suth(pd.Series(np.full(len(df), 1100.0))))\n",
    "\n",
    "# ====== ΔP SILVER: interp curto e físico para lacunas longas ======\n",
    "DP_interp = DP0.interpolate(method=\"linear\", limit=6, limit_direction=\"both\")\n",
    "mask_nan  = DP_interp.isna() & np.isfinite(rho_g) & np.isfinite(nu_proxy)\n",
    "k = np.nanmedian(DP0 / (rho_g * (nu_proxy**2)))\n",
    "DP_hat = k * rho_g * (nu_proxy**2)\n",
    "\n",
    "# clipping físico\n",
    "p5, p95 = np.nanpercentile(DP0, [5, 95]) if np.isfinite(DP0).any() else (1.0, 1.0)\n",
    "DP_hat = DP_hat.clip(lower=p5*0.5, upper=p95*1.5)\n",
    "\n",
    "DP_silver = DP_interp.copy()\n",
    "DP_silver.loc[mask_nan] = DP_hat.loc[mask_nan]\n",
    "\n",
    "# δ, Wr, Wm — SILVER\n",
    "epsP = 1.0\n",
    "delta_proxy = (rho_g * (nu_proxy ** 2) * L_REF) / np.maximum(DP_silver, epsP)\n",
    "delta_m = C_DELTA * delta_proxy\n",
    "\n",
    "sigma_mat   = pd.Series(SIGMA_MAT_BASE,   index=df.index)\n",
    "rho_fuel    = pd.Series(RHO_FUEL_BASE,    index=df.index)\n",
    "sigma_steel = pd.Series(SIGMA_STEEL_BASE, index=df.index)\n",
    "g = sigma_mat / np.maximum(rho_fuel, 1e-12)\n",
    "h = sigma_steel / np.maximum(mu_gas,  1e-12)\n",
    "\n",
    "nu_pos    = np.maximum(nu_proxy.fillna(0.0), 0.0)\n",
    "delta_pos = np.maximum(delta_m.fillna(np.nan), 1e-9)\n",
    "TK_pos    = np.maximum(TK.fillna(np.nan), 1.0)\n",
    "\n",
    "wr_s = (ALPHA_BASE * (nu_pos**2.0) * (delta_pos**1.0) * np.exp(-TK_pos / TCRIT_BASE) * g).astype(float)\n",
    "wm_s = (BETA_BASE  * (nu_pos**3.0) * np.sqrt(delta_pos)    * np.exp(-TK_pos / TOPT_BASE)  * h).astype(float)\n",
    "\n",
    "# ---------- filtrar válidos e salvar SILVER ----------\n",
    "mask_silver = np.isfinite(wr_s) & np.isfinite(wm_s)\n",
    "df_silver = df.loc[mask_silver].copy()\n",
    "df_silver[\"wr_kg_m2_h\"] = wr_s.loc[mask_silver]\n",
    "df_silver[\"wm_kg_m2_h\"] = wm_s.loc[mask_silver]\n",
    "\n",
    "# Features SILVER (mesma lista de exclusão)\n",
    "orig_cols = [c for c in df.columns if c not in [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]]\n",
    "cols_keep_silver = [c for c in orig_cols if c not in used_cols] + [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]\n",
    "df_silver_feat = df_silver[cols_keep_silver].copy()\n",
    "\n",
    "# salvar SILVER\n",
    "write_multiheader_csv(df_silver, dim_map, PATH_SILV_L)\n",
    "write_multiheader_csv(df_silver_feat, dim_map, PATH_SILV_F)\n",
    "\n",
    "print(f\"SILVER: linhas={len(df_silver)} | salvo:\")\n",
    "print(\"  Rotulado:\", PATH_SILV_L)\n",
    "print(\"  Features:\", PATH_SILV_F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d39195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOLD: linhas=10005 | salvo:\n",
      "  Rotulado: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_rotulado_v4_gold.csv\n",
      "  Features: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_features_v4_gold.csv\n",
      "SILVER2: linhas=11749 | salvo:\n",
      "  Rotulado: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_rotulado_v4_silver2.csv\n",
      "  Features: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\\A1_ML_DL_features_v4_silver2.csv\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# GOLD & SILVER2 (com imputação de Q_N e ΔP) a partir de v4\n",
    "# ==============================================================\n",
    "\n",
    "import os, re, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# ---------- caminhos ----------\n",
    "CURATED_DIR = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\"\n",
    "PATH_ROT_V4 = os.path.join(CURATED_DIR, \"A1_ML_DL_rotulado_v4.csv\")\n",
    "\n",
    "PATH_GOLD_L = os.path.join(CURATED_DIR, \"A1_ML_DL_rotulado_v4_gold.csv\")\n",
    "PATH_GOLD_F = os.path.join(CURATED_DIR, \"A1_ML_DL_features_v4_gold.csv\")\n",
    "\n",
    "PATH_SILV2_L = os.path.join(CURATED_DIR, \"A1_ML_DL_rotulado_v4_silver2.csv\")\n",
    "PATH_SILV2_F = os.path.join(CURATED_DIR, \"A1_ML_DL_features_v4_silver2.csv\")\n",
    "\n",
    "# ---------- constantes físicas e parâmetros ----------\n",
    "T_N, P_N = 273.15, 1.01325e5\n",
    "R = 8.314462618\n",
    "M_AIR, M_CO2 = 0.029, 0.044\n",
    "\n",
    "A_EF   = 15.0     # m²\n",
    "L_REF  = 3.0      # m\n",
    "BETA_DP = 0.25\n",
    "C_DELTA = 1e-3\n",
    "GAMMA_DEFAULT = 0.25\n",
    "\n",
    "ALPHA_BASE = 4.2e-3\n",
    "BETA_BASE  = 1.2e-4\n",
    "TCRIT_BASE = 1200.0\n",
    "TOPT_BASE  = 1250.0\n",
    "SIGMA_MAT_BASE   = 6.0e9\n",
    "RHO_FUEL_BASE    = 700.0\n",
    "SIGMA_STEEL_BASE = 2.5e9\n",
    "\n",
    "# ---------- utilidades ----------\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFKD\", str(s)) if not unicodedata.combining(c))\n",
    "def normalize_token(s: str) -> str:\n",
    "    s = strip_accents(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9_\\s]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "def split_tokens(s: str): return set(t for t in re.split(r\"[ _]+\", normalize_token(s)) if t)\n",
    "def hybrid_similarity(a: str, b: str) -> float:\n",
    "    ta, tb = split_tokens(a), split_tokens(b)\n",
    "    jac = len(ta & tb) / len(ta | tb) if (ta and tb) else 0.0\n",
    "    seq = SequenceMatcher(None, normalize_token(a), normalize_token(b)).ratio()\n",
    "    return 0.6*jac + 0.4*seq\n",
    "def best_match(q, cands):\n",
    "    sc = [(c, hybrid_similarity(q, c)) for c in cands]\n",
    "    sc.sort(key=lambda x: x[1], reverse=True)\n",
    "    return sc[0] if sc else (None, 0.0)\n",
    "\n",
    "def read_multiheader_csv(path):\n",
    "    dfr = pd.read_csv(path, header=[0,1], engine=\"python\")\n",
    "    dim = {c: d for (c,d) in dfr.columns}\n",
    "    df  = dfr.copy()\n",
    "    df.columns = [c for (c,d) in dfr.columns]\n",
    "    return df, dim\n",
    "\n",
    "def write_multiheader_csv(df, dim, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as f:\n",
    "        f.write(\",\".join(df.columns) + \"\\n\")\n",
    "        f.write(\",\".join([str(dim.get(c,\"\")) for c in df.columns]) + \"\\n\")\n",
    "    df.to_csv(path, mode=\"a\", index=False, header=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "def dim_text(dim_map, col): return str(dim_map.get(col,\"\") or \"\").lower()\n",
    "def to_pa(x, dimtxt):\n",
    "    s = (dimtxt or \"\").lower()\n",
    "    v = pd.to_numeric(x, errors=\"coerce\")\n",
    "    if \"mpa\" in s:   return v * 1e6\n",
    "    if \"kpa\" in s:   return v * 1e3\n",
    "    if \"bar\" in s:   return v * 1e5\n",
    "    if \"mbar\" in s:  return v * 1e2\n",
    "    return v\n",
    "\n",
    "def TK_from_series(series, dimtxt, fallback_name=\"\"):\n",
    "    is_c = (\"°c\" in (dimtxt or \"\").lower()) or ((\"c\" in (dimtxt or \"\").lower() and \"k\" not in (dimtxt or \"\").lower()))\n",
    "    if is_c or (\"tau\" in str(fallback_name).lower() and not dimtxt):\n",
    "        return pd.to_numeric(series, errors=\"coerce\") + 273.15\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "def mu_air_suth(TK):\n",
    "    mu0, T0, S = 1.716e-5, 273.15, 111.0\n",
    "    return mu0 * ((T0 + S) / (TK + S)) * (TK / T0) ** 1.5\n",
    "def mu_co2_suth(TK):\n",
    "    mu0, T0, S = 1.37e-5, 273.15, 240.0\n",
    "    return mu0 * ((T0 + S) / (TK + S)) * (TK / T0) ** 1.5\n",
    "\n",
    "def fill_blocks_avg(series, limit_interp=24, roll_win=48):\n",
    "    \"\"\"Interpola até 'limit_interp' pontos; preenche remanescentes com mediana móvel centrada (roll_win);\n",
    "       e o que sobrar com mediana global.\"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    s1 = s.interpolate(method=\"linear\", limit=limit_interp, limit_direction=\"both\")\n",
    "    s2 = s1.copy()\n",
    "    # completa NaNs com mediana móvel centrada\n",
    "    roll_med = s2.rolling(roll_win, min_periods=1, center=True).median()\n",
    "    s2 = s2.fillna(roll_med)\n",
    "    # fallback global\n",
    "    s3 = s2.fillna(np.nanmedian(s2))\n",
    "    return s3\n",
    "\n",
    "# ---------- 1) Ler o rotulado v4 ----------\n",
    "df, dim_map = read_multiheader_csv(PATH_ROT_V4)\n",
    "colunas = list(df.columns)\n",
    "\n",
    "# localizar TAU_DENSA para o bloco físico\n",
    "if \"TAU_DENSA\" in colunas:\n",
    "    idx_tau = colunas.index(\"TAU_DENSA\")\n",
    "else:\n",
    "    c,_ = best_match(\"tau_densa\", colunas); idx_tau = colunas.index(c) if c else len(colunas)\n",
    "\n",
    "# ---------- (A) GOLD ----------\n",
    "mask_gold = np.isfinite(pd.to_numeric(df[\"wr_kg_m2_h\"], errors=\"coerce\")) & \\\n",
    "            np.isfinite(pd.to_numeric(df[\"wm_kg_m2_h\"], errors=\"coerce\"))\n",
    "df_gold = df.loc[mask_gold].copy()\n",
    "\n",
    "# leak-guard set\n",
    "ALIASES = {\n",
    "    \"tau_main\":     [\"tau_densa\",\"tau\",\"t_leito\",\"leito_temp_average\",\"bed_temp\",\"temperatura\"],\n",
    "    \"temp_leito\":   [\"leito_temp_average\",\"bed_temp\",\"temperatura_leito\",\"fornalha_leito_temp_average\"],\n",
    "    \"air_total\":    [\"air_total_knm3_h_filled\",\"air_total_knm3_h\",\"air_total_nm3_h\",\"air_total_mn3_h\"],\n",
    "    \"air_primary\":  [\"air_primary_knm3_h\",\"air_primary_nm3_h\",\"air_primary\"],\n",
    "    \"air_secondary\":[\"air_secondary_knm3_h\",\"air_secondary_nm3_h\",\"air_secondary\"],\n",
    "    \"dp_furn\":      [\"dp_fornalha\",\"delta_p_fornalha\",\"fornalha_dp\",\"pressao_fornalha\",\"dp_furnace\"],\n",
    "    \"p_furn_a\":     [\"pressao_fornalha_a_inf\",\"pressao_fornalha_a\",\"furnace_a_pressure\",\"fornalha_a_press\"],\n",
    "    \"p_furn_b\":     [\"pressao_fornalha_b_inf\",\"pressao_fornalha_b\",\"furnace_b_pressure\",\"fornalha_b_press\"],\n",
    "    \"p_abs\":        [\"pressao_fornalha\",\"furnace_pressure\",\"pressao_plenum\",\"pressao_leito\",\"pressao_tambor\"],\n",
    "    \"rho_co2\":      [\"densidade_co2\",\"co2_density\",\"rho_co2\"],\n",
    "    \"o2\":           [\"o2_medio\",\"o2_excess_pct\",\"oxygen\"]\n",
    "}\n",
    "def choose(qs):\n",
    "    best, bs = None, -1.0\n",
    "    for q in qs:\n",
    "        c, s = best_match(q, colunas)\n",
    "        if c and s > bs:\n",
    "            best, bs = c, s\n",
    "    return best\n",
    "\n",
    "tau_main       = choose(ALIASES[\"tau_main\"])\n",
    "temp_leito_col = choose(ALIASES[\"temp_leito\"])\n",
    "air_total_col  = choose(ALIASES[\"air_total\"])\n",
    "air_pri_col    = choose(ALIASES[\"air_primary\"])\n",
    "air_sec_col    = choose(ALIASES[\"air_secondary\"])\n",
    "dp_furn_col    = choose(ALIASES[\"dp_furn\"])\n",
    "p_furn_a_col   = choose(ALIASES[\"p_furn_a\"])\n",
    "p_furn_b_col   = choose(ALIASES[\"p_furn_b\"])\n",
    "p_abs_col      = choose(ALIASES[\"p_abs\"])\n",
    "rho_co2_col    = choose(ALIASES[\"rho_co2\"])\n",
    "o2_col         = choose(ALIASES[\"o2\"])\n",
    "\n",
    "used_cols = set([tau_main, temp_leito_col, air_total_col, air_pri_col, air_sec_col,\n",
    "                 dp_furn_col, p_furn_a_col, p_furn_b_col, p_abs_col, rho_co2_col, o2_col,\n",
    "                 \"air_total_knm3_h_filled\"])\n",
    "used_cols = {c for c in used_cols if c}\n",
    "used_cols.update(colunas[idx_tau:])\n",
    "\n",
    "# features GOLD\n",
    "orig_cols = [c for c in df.columns if c not in [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]]\n",
    "cols_keep_gold = [c for c in orig_cols if c not in used_cols] + [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]\n",
    "df_gold_feat = df_gold[cols_keep_gold].copy()\n",
    "\n",
    "write_multiheader_csv(df_gold, dim_map, PATH_GOLD_L)\n",
    "write_multiheader_csv(df_gold_feat, dim_map, PATH_GOLD_F)\n",
    "print(f\"GOLD: linhas={len(df_gold)} | salvo:\\n  Rotulado: {PATH_GOLD_L}\\n  Features: {PATH_GOLD_F}\")\n",
    "\n",
    "# ---------- (B) SILVER2: imputar Q_N e ΔP e recalc Wr/Wm ----------\n",
    "# séries base\n",
    "def series_TK(df_loc, name):\n",
    "    return TK_from_series(df_loc[name], dim_text(dim_map, name), fallback_name=name) if name else pd.Series(np.nan, index=df_loc.index)\n",
    "\n",
    "TK = series_TK(df, tau_main) if tau_main else (series_TK(df, temp_leito_col) if temp_leito_col else pd.Series(np.nan, index=df.index))\n",
    "\n",
    "if p_abs_col:\n",
    "    Pabs = to_pa(df[p_abs_col], dim_text(dim_map, p_abs_col))\n",
    "    if Pabs.median(skipna=True) < 2e5: Pabs = Pabs + P_N\n",
    "else:\n",
    "    Pabs = pd.Series(P_N, index=df.index)\n",
    "\n",
    "DP_from_col = to_pa(df[dp_furn_col], dim_text(dim_map, dp_furn_col)) if dp_furn_col else pd.Series(np.nan, index=df.index)\n",
    "DP_from_ab  = None\n",
    "if p_furn_a_col and p_furn_b_col:\n",
    "    Pa = to_pa(df[p_furn_a_col], dim_text(dim_map, p_furn_a_col))\n",
    "    Pb = to_pa(df[p_furn_b_col], dim_text(dim_map, p_furn_b_col))\n",
    "    DP_from_ab = (Pa - Pb).abs()\n",
    "\n",
    "def looks_like_absolute(dp_name: str) -> bool:\n",
    "    if not dp_name: return False\n",
    "    n = dp_name.lower()\n",
    "    return (\"delta\" not in n) and (\"dp\" not in n)\n",
    "\n",
    "use_ab_all = False\n",
    "if dp_furn_col and p_abs_col and (dp_furn_col == p_abs_col):\n",
    "    use_ab_all = True\n",
    "elif dp_furn_col and looks_like_absolute(dp_furn_col) and DP_from_ab is not None:\n",
    "    use_ab_all = True\n",
    "\n",
    "if use_ab_all and DP_from_ab is not None:\n",
    "    DP0 = DP_from_ab.copy()\n",
    "else:\n",
    "    DP0 = DP_from_col.copy()\n",
    "    if DP_from_ab is not None:\n",
    "        need = ~np.isfinite(DP0) & np.isfinite(DP_from_ab)\n",
    "        DP0.loc[need] = DP_from_ab.loc[need]\n",
    "\n",
    "# --------- imputação de Q_N (Silver2) ----------\n",
    "air_filled_col = \"air_total_knm3_h_filled\" if \"air_total_knm3_h_filled\" in df.columns else air_total_col\n",
    "QN0 = pd.to_numeric(df[air_filled_col], errors=\"coerce\") if air_filled_col else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "QN_s2 = fill_blocks_avg(QN0, limit_interp=24, roll_win=48)\n",
    "\n",
    "# --------- construir Q_real, ν ----------\n",
    "Q_real = (QN_s2 * 1000.0 / 3600.0) * (TK / T_N) * (P_N / Pabs)\n",
    "nu_base = Q_real / A_EF\n",
    "DP_ref  = np.nanmedian(DP0) if np.isfinite(DP0).any() else 1.0\n",
    "nu_proxy = nu_base * np.power(np.maximum(DP0, 1.0) / max(DP_ref, 1.0), BETA_DP)\n",
    "\n",
    "# --------- fração CO2 robusta (default) + ρg e μgas ---------\n",
    "gamma = pd.Series(GAMMA_DEFAULT, index=df.index)\n",
    "M_mix = (1 - gamma) * M_AIR + gamma * M_CO2\n",
    "rho_g = (Pabs * M_mix) / (R * TK)\n",
    "\n",
    "TK_mu = TK.where(np.isfinite(TK), 1100.0)\n",
    "mu_gas = (1 - gamma) * mu_air_suth(TK_mu) + gamma * mu_co2_suth(TK_mu)\n",
    "mu_gas = pd.to_numeric(mu_gas, errors=\"coerce\").fillna(mu_air_suth(pd.Series(np.full(len(df), 1100.0))))\n",
    "\n",
    "# --------- ΔP SILVER2: interp curto + k·ρg·ν² com clipping ----------\n",
    "DP_interp = DP0.interpolate(method=\"linear\", limit=6, limit_direction=\"both\")\n",
    "mask_nan  = DP_interp.isna() & np.isfinite(rho_g) & np.isfinite(nu_proxy)\n",
    "k = np.nanmedian(DP0 / (rho_g * (nu_proxy**2)))\n",
    "DP_hat = k * rho_g * (nu_proxy**2)\n",
    "\n",
    "p5, p95 = np.nanpercentile(DP0, [5, 95]) if np.isfinite(DP0).any() else (1.0, 1.0)\n",
    "DP_hat = DP_hat.clip(lower=p5*0.5, upper=p95*1.5)\n",
    "\n",
    "DP_s2 = DP_interp.copy()\n",
    "DP_s2.loc[mask_nan] = DP_hat.loc[mask_nan]\n",
    "\n",
    "# --------- δ, Wr, Wm — SILVER2 ----------\n",
    "epsP = 1.0\n",
    "delta_proxy = (rho_g * (nu_proxy ** 2) * L_REF) / np.maximum(DP_s2, epsP)\n",
    "delta_m = C_DELTA * delta_proxy\n",
    "\n",
    "sigma_mat   = pd.Series(SIGMA_MAT_BASE,   index=df.index)\n",
    "rho_fuel    = pd.Series(RHO_FUEL_BASE,    index=df.index)\n",
    "sigma_steel = pd.Series(SIGMA_STEEL_BASE, index=df.index)\n",
    "g = sigma_mat / np.maximum(rho_fuel, 1e-12)\n",
    "h = sigma_steel / np.maximum(mu_gas,  1e-12)\n",
    "\n",
    "nu_pos    = np.maximum(nu_proxy.fillna(0.0), 0.0)\n",
    "delta_pos = np.maximum(delta_m.fillna(np.nan), 1e-9)\n",
    "TK_pos    = np.maximum(TK.fillna(np.nan), 1.0)\n",
    "\n",
    "wr_s2 = (ALPHA_BASE * (nu_pos**2.0) * (delta_pos**1.0) * np.exp(-TK_pos / TCRIT_BASE) * g).astype(float)\n",
    "wm_s2 = (BETA_BASE  * (nu_pos**3.0) * np.sqrt(delta_pos)    * np.exp(-TK_pos / TOPT_BASE)  * h).astype(float)\n",
    "\n",
    "# --------- filtrar válidos e salvar SILVER2 ----------\n",
    "mask_s2 = np.isfinite(wr_s2) & np.isfinite(wm_s2)\n",
    "df_s2 = df.loc[mask_s2].copy()\n",
    "df_s2[\"wr_kg_m2_h\"] = wr_s2.loc[mask_s2]\n",
    "df_s2[\"wm_kg_m2_h\"] = wm_s2.loc[mask_s2]\n",
    "\n",
    "# features (mesma lista de exclusão do GOLD)\n",
    "cols_keep_s2 = [c for c in orig_cols if c not in used_cols] + [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]\n",
    "df_s2_feat = df_s2[cols_keep_s2].copy()\n",
    "\n",
    "write_multiheader_csv(df_s2, dim_map, PATH_SILV2_L)\n",
    "write_multiheader_csv(df_s2_feat, dim_map, PATH_SILV2_F)\n",
    "\n",
    "print(f\"SILVER2: linhas={len(df_s2)} | salvo:\\n  Rotulado: {PATH_SILV2_L}\\n  Features: {PATH_SILV2_F}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1160f405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== GOLD ====\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\gold_missingness.csv\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\gold_constants.csv\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\gold_near_constants.csv\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\gold_high_corr_pairs.csv\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\gold_leakage_flags.csv\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\gold_relatorio.md\n",
      "\n",
      "==== SILVER2 ====\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\silver2_missingness.csv\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\silver2_constants.csv\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\silver2_near_constants.csv\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\silver2_high_corr_pairs.csv\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\silver2_leakage_flags.csv\n",
      "  - salvo: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\\silver2_relatorio.md\n",
      "\n",
      "==== RESUMO FINAL ====\n",
      "GOLD: linhas=10005, feats_num=144, miss_mediana=0.00%\n",
      "  Superv (VAL) R2: wr_kg_m2_h=0.930, wm_kg_m2_h=0.921\n",
      "  Superv (TST) R2: wr_kg_m2_h=0.754, wm_kg_m2_h=0.682\n",
      "  Silhouette: k=2:0.58, k=3:0.53, k=4:0.34, k=5:0.31, k=6:0.30\n",
      "  TS ok: passo≈1h 99.59% | gaps>1h 41\n",
      "SILVER2: linhas=11749, feats_num=144, miss_mediana=0.00%\n",
      "  Superv (VAL) R2: wr_kg_m2_h=0.825, wm_kg_m2_h=0.861\n",
      "  Superv (TST) R2: wr_kg_m2_h=0.506, wm_kg_m2_h=0.627\n",
      "  Silhouette: k=2:0.58, k=3:0.49, k=4:0.34, k=5:0.32, k=6:0.30\n",
      "  TS ok: passo≈1h 99.6% | gaps>1h 47\n",
      "\n",
      "Saídas gravadas em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# Verificação de ADEQUAÇÃO (Supervisionado & Não-Supervisionado)\n",
    "# GOLD e SILVER2 — todas as saídas em um único diretório\n",
    "# ==============================================================\n",
    "\n",
    "import os, re, unicodedata, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------ Caminhos ------------------\n",
    "BASE_IN = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\data\\curated\"\n",
    "PATHS = {\n",
    "    \"GOLD\":    os.path.join(BASE_IN, \"A1_ML_DL_features_v4_gold.csv\"),\n",
    "    \"SILVER2\": os.path.join(BASE_IN, \"A1_ML_DL_features_v4_silver2.csv\"),\n",
    "}\n",
    "\n",
    "# >>> ÚNICO diretório de saída (será criado se não existir)\n",
    "OUT_DIR = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\diagnosticos\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ------------------ Utils de leitura/escrita ------------------\n",
    "def read_features_multiheader(path):\n",
    "    # Lê 2 linhas de cabeçalho: (nome | dimensão)\n",
    "    dfr = pd.read_csv(path, header=[0,1], engine=\"python\")\n",
    "    dim_map = {c:d for (c,d) in dfr.columns}\n",
    "    df = dfr.copy()\n",
    "    df.columns = [c for (c,d) in dfr.columns]\n",
    "    return df, dim_map\n",
    "\n",
    "def write_md(version, filename, text):\n",
    "    path = os.path.join(OUT_DIR, f\"{version.lower()}_{filename}\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(\"  - salvo:\", path)\n",
    "\n",
    "def write_csv(version, filename, df):\n",
    "    path = os.path.join(OUT_DIR, f\"{version.lower()}_{filename}\")\n",
    "    df.to_csv(path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"  - salvo:\", path)\n",
    "\n",
    "# ------------------ Funções auxiliares ------------------\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFKD\", str(s)) if not unicodedata.combining(c))\n",
    "def normalize_token(s: str) -> str:\n",
    "    s = strip_accents(s).lower()\n",
    "    s = re.sub(r\"[^a-z0-9_\\s]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "def split_tokens(s: str): return set(t for t in re.split(r\"[ _]+\", normalize_token(s)) if t)\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "def hybrid_similarity(a: str, b: str) -> float:\n",
    "    ta, tb = split_tokens(a), split_tokens(b)\n",
    "    jac = len(ta & tb) / len(ta | tb) if (ta and tb) else 0.0\n",
    "    seq = SequenceMatcher(None, normalize_token(a), normalize_token(b)).ratio()\n",
    "    return 0.6*jac + 0.4*seq\n",
    "\n",
    "LEAKY_PATTERNS = [\n",
    "    r\"^tau\", r\"leito_temp\", r\"bed_temp\", r\"temperatura\", r\"o2\", r\"co2\", r\"air_\",\n",
    "    r\"pressao_fornalha\", r\"dp_fornalha\", r\"delta_p\", r\"fornalha_dp\",\n",
    "    r\"pressao_plenum\", r\"pressao_leito\", r\"pressao_tambor\",\n",
    "    r\"tau_densa\", r\"tau_diluida\", r\"tau_global\", r\"tau_backpass\",\n",
    "    r\"air_total_knm3_h_filled\",\n",
    "]\n",
    "TARGETS = [\"wr_kg_m2_h\",\"wm_kg_m2_h\"]\n",
    "\n",
    "def leakage_scan(columns):\n",
    "    flags = []\n",
    "    for c in columns:\n",
    "        if c in TARGETS:\n",
    "            continue\n",
    "        for pat in LEAKY_PATTERNS:\n",
    "            if re.search(pat, c, re.I):\n",
    "                flags.append(c); break\n",
    "    return sorted(set(flags))\n",
    "\n",
    "def missingness(dfX):\n",
    "    return dfX.isna().mean().sort_values(ascending=False)\n",
    "\n",
    "def constant_columns(dfX, near_threshold=0.01):\n",
    "    const, near = [], []\n",
    "    for c in dfX.columns:\n",
    "        x = dfX[c]\n",
    "        if x.nunique(dropna=True) <= 1:\n",
    "            const.append(c)\n",
    "        else:\n",
    "            ratio = x.nunique(dropna=True) / max(len(x),1)\n",
    "            if ratio <= near_threshold:\n",
    "                near.append((c, ratio))\n",
    "    return const, near\n",
    "\n",
    "def high_corr_pairs(dfX, thr=0.97, max_cols=200):\n",
    "    X = dfX.select_dtypes(include=[np.number])\n",
    "    if X.shape[1] > max_cols:\n",
    "        X = X.iloc[:, :max_cols]\n",
    "    corr = X.corr().abs()\n",
    "    pairs = []\n",
    "    cols = corr.columns\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            if corr.iloc[i,j] >= thr:\n",
    "                pairs.append((cols[i], cols[j], corr.iloc[i,j]))\n",
    "    return sorted(pairs, key=lambda t: t[2], reverse=True)\n",
    "\n",
    "def to_datetime_guess(df):\n",
    "    cand_names = [c for c in df.columns if re.search(r\"(time|hora|data|date|timestamp|datetime|ts)\", c, re.I)]\n",
    "    ts = None; col = None\n",
    "    for c in cand_names:\n",
    "        s = pd.to_datetime(df[c], errors=\"coerce\", dayfirst=False, utc=False)\n",
    "        if s.notna().mean() > 0.9:\n",
    "            ts, col = s, c\n",
    "            break\n",
    "    return ts, col\n",
    "\n",
    "def supervised_quick_eval(df, ts, ycols=TARGETS):\n",
    "    y = df[ycols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X = df.drop(columns=ycols).select_dtypes(include=[np.number])\n",
    "\n",
    "    n = len(df)\n",
    "    if n < 200 or X.shape[1] < 3:\n",
    "        return None\n",
    "\n",
    "    # split temporal 70/10/20\n",
    "    order = np.arange(n)\n",
    "    train_end = int(0.7*n); val_end = int(0.8*n)\n",
    "    idx_train = order[:train_end]; idx_val = order[train_end:val_end]; idx_test = order[val_end:]\n",
    "\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    X_train = imp.fit_transform(X.iloc[idx_train])\n",
    "    X_val   = imp.transform(X.iloc[idx_val])\n",
    "    X_test  = imp.transform(X.iloc[idx_test])\n",
    "\n",
    "    y_train = y.iloc[idx_train].values\n",
    "    y_val   = y.iloc[idx_val].values\n",
    "    y_test  = y.iloc[idx_test].values\n",
    "\n",
    "    model = MultiOutputRegressor(RandomForestRegressor(\n",
    "        n_estimators=200, random_state=42, n_jobs=-1\n",
    "    ))\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    y_pred_tst = model.predict(X_test)\n",
    "\n",
    "    def _m(y_true, y_pred):\n",
    "        out = {}\n",
    "        for i, col in enumerate(ycols):\n",
    "            out[col] = {\n",
    "                \"R2\": float(r2_score(y_true[:,i], y_pred[:,i])),\n",
    "                \"MAE\": float(mean_absolute_error(y_true[:,i], y_pred[:,i]))\n",
    "            }\n",
    "        return out\n",
    "\n",
    "    return {\n",
    "        \"n_train\": int(len(idx_train)), \"n_val\": int(len(idx_val)), \"n_test\": int(len(idx_test)),\n",
    "        \"val\": _m(y_val, y_pred_val),\n",
    "        \"test\": _m(y_test, y_pred_tst),\n",
    "        \"num_features\": int(X.shape[1])\n",
    "    }\n",
    "\n",
    "def unsupervised_quick_eval(df, ycols=TARGETS, sample_max=4000):\n",
    "    X = df.drop(columns=ycols, errors=\"ignore\").select_dtypes(include=[np.number])\n",
    "\n",
    "    imp = SimpleImputer(strategy=\"median\")\n",
    "    X_imp = imp.fit_transform(X)\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X_imp)\n",
    "\n",
    "    n = X_std.shape[0]\n",
    "    idx = np.arange(n)\n",
    "    if n > sample_max:\n",
    "        rng = np.random.RandomState(42)\n",
    "        idx = rng.choice(n, size=sample_max, replace=False)\n",
    "    Xs = X_std[idx]\n",
    "\n",
    "    pca = PCA(n_components=min(30, Xs.shape[1]))\n",
    "    Xp = pca.fit_transform(Xs)\n",
    "    cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "    comps95 = int(np.searchsorted(cumvar, 0.95) + 1)\n",
    "\n",
    "    Xp10 = Xp[:, :min(10, Xp.shape[1])]\n",
    "    sil = []\n",
    "    for k in range(2,7):\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "        labels = km.fit_predict(Xp10)\n",
    "        s = silhouette_score(Xp10, labels) if len(set(labels)) > 1 else np.nan\n",
    "        sil.append((k, float(s)))\n",
    "    return {\n",
    "        \"pca_cumvar_first10\": [float(v) for v in cumvar[:10]],\n",
    "        \"n_components_95pct\": comps95,\n",
    "        \"silhouette\": sil\n",
    "    }\n",
    "\n",
    "# ------------------ Avaliação por versão ------------------\n",
    "def assess_version(name, path_in):\n",
    "    print(f\"\\n==== {name} ====\")\n",
    "    df, dim = read_features_multiheader(path_in)\n",
    "\n",
    "    # Targets\n",
    "    for t in TARGETS:\n",
    "        if t not in df.columns:\n",
    "            raise RuntimeError(f\"[{name}] Alvo ausente: {t}\")\n",
    "\n",
    "    # Vazamento\n",
    "    leak = leakage_scan(df.columns)\n",
    "\n",
    "    # X/Y\n",
    "    y = df[TARGETS].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X = df.drop(columns=TARGETS)\n",
    "\n",
    "    # Faltas / Constantes / Colinearidade\n",
    "    miss = missingness(X)\n",
    "    const, near = constant_columns(X)\n",
    "    pairs = high_corr_pairs(X, thr=0.97)\n",
    "\n",
    "    # Salvar CSVs (com prefixo da versão)\n",
    "    write_csv(name, \"missingness.csv\", miss.reset_index().rename(columns={\"index\":\"col\",0:\"pct_missing\"}))\n",
    "    write_csv(name, \"constants.csv\", pd.DataFrame({\"constant_cols\": const}))\n",
    "    write_csv(name, \"near_constants.csv\", pd.DataFrame(near, columns=[\"col\",\"unique_ratio\"]))\n",
    "    write_csv(name, \"high_corr_pairs.csv\", pd.DataFrame(pairs, columns=[\"col_i\",\"col_j\",\"abs_corr\"]))\n",
    "    write_csv(name, \"leakage_flags.csv\", pd.DataFrame({\"leaky_cols\": leak}))\n",
    "\n",
    "    # Tempo\n",
    "    ts, ts_name = to_datetime_guess(df)\n",
    "    if ts is not None:\n",
    "        dif = ts.diff().dropna()\n",
    "        hours = dif.dt.total_seconds()/3600.0\n",
    "        pct_1h = (np.isclose(hours, 1.0)).mean()*100 if len(hours)>0 else np.nan\n",
    "        gaps = int((hours>1.0).sum())\n",
    "        tdiag = {\"has_ts\": True, \"rows\": int(len(ts)), \"start\": str(ts.iloc[0]),\n",
    "                 \"end\": str(ts.iloc[-1]), \"pct_step_1h\": round(pct_1h,2), \"num_gaps_gt_1h\": gaps,\n",
    "                 \"ts_name\": ts_name}\n",
    "    else:\n",
    "        tdiag = {\"has_ts\": False}\n",
    "\n",
    "    # Supervisionado\n",
    "    sup = supervised_quick_eval(df, ts, TARGETS)\n",
    "\n",
    "    # Não-supervisionado\n",
    "    unsup = unsupervised_quick_eval(df, TARGETS)\n",
    "\n",
    "    # Relatório MD\n",
    "    nrows, nfeat = df.shape[0], X.select_dtypes(include=[np.number]).shape[1]\n",
    "    miss_med = float(miss.median()) if len(miss)>0 else 0.0\n",
    "    ok_super = (sup is not None) and (sup[\"n_train\"]>=1000) and (nfeat>=10)\n",
    "    ok_unsup = True if unsup[\"n_components_95pct\"] <= max(30, nfeat) else False\n",
    "\n",
    "    md = []\n",
    "    md.append(f\"# Verificação de Adequação – {name}\\n\")\n",
    "    md.append(f\"- Linhas: **{nrows}**  |  Features numéricas: **{nfeat}**\")\n",
    "    md.append(f\"- Alvos presentes: `{', '.join(TARGETS)}`\")\n",
    "    md.append(f\"- Mediana de faltas nas features: **{100*miss_med:.2f}%**\")\n",
    "    md.append(f\"- Colunas constantes: **{len(const)}**  | quase-constantes: **{len(near)}**\")\n",
    "    md.append(f\"- Pares com |corr| ≥ 0.97: **{len(pairs)}**\")\n",
    "    if tdiag[\"has_ts\"]:\n",
    "        md.append(f\"- Timestamp: **{tdiag['ts_name']}** | início: **{tdiag['start']}** | fim: **{tdiag['end']}**\")\n",
    "        md.append(f\"- Passo de 1h (≈): **{tdiag['pct_step_1h']}%** | gaps >1h: **{tdiag['num_gaps_gt_1h']}**\")\n",
    "    else:\n",
    "        md.append(f\"- Timestamp: **não encontrado**\")\n",
    "    md.append(\"\\n## Vazamento (features proibidas detectadas)\")\n",
    "    md.append(f\"- Encontradas: **{len(leak)}**\" + (\"\" if len(leak)==0 else f\"\\n  - \" + \"\\n  - \".join(leak)))\n",
    "    md.append(\"\\n## Baseline – Supervisionado (RandomForest multi-alvo)\")\n",
    "    if sup is None:\n",
    "        md.append(\"- **Dataset pequeno/insuficiente** para métrica robusta.\")\n",
    "    else:\n",
    "        md.append(f\"- Split: train={sup['n_train']}, val={sup['n_val']}, test={sup['n_test']}\")\n",
    "        for split in [\"val\",\"test\"]:\n",
    "            md.append(f\"- {split.upper()}:\")\n",
    "            for tgt, m in sup[split].items():\n",
    "                md.append(f\"  - `{tgt}` → R²={m['R2']:.3f} | MAE={m['MAE']:.3g}\")\n",
    "    md.append(\"\\n## Não-supervisionado (PCA + Silhouette)\")\n",
    "    md.append(f\"- Componentes p/ 95% variância: **{unsup['n_components_95pct']}**\")\n",
    "    md.append(f\"- Variância acumulada (10 comps): \" + \", \".join([f\"{v:.2f}\" for v in unsup[\"pca_cumvar_first10\"]]))\n",
    "    md.append(f\"- Silhouette k=2..6: \" + \", \".join([f\"k={k}:{s:.3f}\" for k,s in unsup[\"silhouette\"]]))\n",
    "    md.append(\"\\n## Juízo de adequação (heurístico)\")\n",
    "    md.append(f\"- **Supervisionado**: {'OK' if ok_super else 'ATENÇÃO'} (n_train≥1000 e ≥10 features num.)\")\n",
    "    md.append(f\"- **Não-supervisionado**: {'OK' if ok_unsup else 'ATENÇÃO'} (PCA ≤ 30 comps p/ 95% var.)\")\n",
    "\n",
    "    write_md(name, \"relatorio.md\", \"\\n\".join(md))\n",
    "\n",
    "    return {\n",
    "        \"name\": name, \"nrows\": nrows, \"nfeat_num\": nfeat,\n",
    "        \"miss_median\": miss_med, \"sup\": sup, \"unsup\": unsup, \"temporal\": tdiag\n",
    "    }\n",
    "\n",
    "# ------------------ Executa para GOLD e SILVER2 ------------------\n",
    "summaries = {}\n",
    "for ver, path in PATHS.items():\n",
    "    summaries[ver] = assess_version(ver, path)\n",
    "\n",
    "print(\"\\n==== RESUMO FINAL ====\")\n",
    "for ver, s in summaries.items():\n",
    "    print(f\"{ver}: linhas={s['nrows']}, feats_num={s['nfeat_num']}, miss_mediana={100*s['miss_median']:.2f}%\")\n",
    "    if s[\"sup\"] is not None:\n",
    "        print(f\"  Superv (VAL) R2: \" + \", \".join([f\"{k}={v['R2']:.3f}\" for k,v in s['sup']['val'].items()]))\n",
    "        print(f\"  Superv (TST) R2: \" + \", \".join([f\"{k}={v['R2']:.3f}\" for k,v in s['sup']['test'].items()]))\n",
    "    sils = \", \".join([f\"k={k}:{sc:.2f}\" for k,sc in s[\"unsup\"][\"silhouette\"]])\n",
    "    print(f\"  Silhouette: {sils}\")\n",
    "    if s[\"temporal\"].get(\"has_ts\", False):\n",
    "        print(f\"  TS ok: passo≈1h {s['temporal']['pct_step_1h']}% | gaps>1h {s['temporal']['num_gaps_gt_1h']}\")\n",
    "print(\"\\nSaídas gravadas em:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349cff37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Freeze concluído em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\\outputs\\freeze\\v1_20250818_0635\n"
     ]
    }
   ],
   "source": [
    "# ===================== FREEZE DOS DADOS PARA MODELAGEM =====================\n",
    "import os, shutil, json, hashlib, pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "BASE = r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL_REFAZIMENTO\"\n",
    "CUR  = os.path.join(BASE, \"data\", \"curated\")\n",
    "OUTS = os.path.join(BASE, \"outputs\", \"diagnosticos\")\n",
    "\n",
    "# Tag de versão (ex.: v1_YYYYmmdd_HHMM)\n",
    "TAG = \"v1_\" + datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "FREEZE_DIR = os.path.join(BASE, \"outputs\", \"freeze\", TAG)\n",
    "os.makedirs(FREEZE_DIR, exist_ok=True)\n",
    "\n",
    "# Arquivos que vamos congelar (adicione se quiser mais)\n",
    "FILES = [\n",
    "    os.path.join(CUR, \"A1_ML_DL_rotulado_v4_gold.csv\"),\n",
    "    os.path.join(CUR, \"A1_ML_DL_features_v4_gold.csv\"),\n",
    "    os.path.join(CUR, \"A1_ML_DL_rotulado_v4_silver2.csv\"),\n",
    "    os.path.join(CUR, \"A1_ML_DL_features_v4_silver2.csv\"),\n",
    "    os.path.join(CUR, \"A1_ML_DL_rotulado_v4.csv\"),\n",
    "    os.path.join(CUR, \"A1_ML_DL_features_v4.csv\"),\n",
    "]\n",
    "\n",
    "# Copiar diagnósticos principais (MDs)\n",
    "DIAG_MD = [\n",
    "    os.path.join(OUTS, \"gold_relatorio.md\"),\n",
    "    os.path.join(OUTS, \"silver2_relatorio.md\"),\n",
    "]\n",
    "\n",
    "def sha256_of(path, chunk=1<<20):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk)\n",
    "            if not b: break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# 1) Copia arquivos\n",
    "copied = []\n",
    "for p in FILES + DIAG_MD:\n",
    "    if os.path.exists(p):\n",
    "        dst = os.path.join(FREEZE_DIR, os.path.basename(p))\n",
    "        shutil.copy2(p, dst)\n",
    "        copied.append(dst)\n",
    "\n",
    "# 2) Gera checksums\n",
    "ck_path = os.path.join(FREEZE_DIR, \"checksums_sha256.txt\")\n",
    "with open(ck_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for p in copied:\n",
    "        f.write(f\"{sha256_of(p)}  {os.path.basename(p)}\\n\")\n",
    "\n",
    "# 3) Manifesto com contagem de linhas/colunas dos CSV principais\n",
    "manifest = {\"tag\": TAG, \"generated_at\": datetime.now().isoformat(), \"files\": []}\n",
    "for p in copied:\n",
    "    item = {\"file\": os.path.basename(p), \"path\": p}\n",
    "    if p.lower().endswith(\".csv\"):\n",
    "        try:\n",
    "            df = pd.read_csv(p, header=[0,1], engine=\"python\")\n",
    "            item[\"rows\"] = int(df.shape[0])\n",
    "            item[\"cols\"] = int(len(df.columns))\n",
    "            # registra nomes das colunas e dimensões (primeiras 10 colunas como amostra)\n",
    "            cols = [str(c[0]) for c in df.columns]\n",
    "            dims = [str(c[1]) for c in df.columns]\n",
    "            item[\"sample_columns\"] = cols[:10]\n",
    "            item[\"sample_dims\"]    = dims[:10]\n",
    "        except Exception:\n",
    "            pass\n",
    "    manifest[\"files\"].append(item)\n",
    "\n",
    "with open(os.path.join(FREEZE_DIR, \"manifest.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Freeze concluído em:\", FREEZE_DIR)\n",
    "\n",
    "# 4) (Opcional) Criar um ZIP da pasta\n",
    "# import shutil\n",
    "# shutil.make_archive(FREEZE_DIR, \"zip\", FREEZE_DIR)\n",
    "# print(\"ZIP gerado em:\", FREEZE_DIR + \".zip\")\n",
    "# ========================================================================== \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
